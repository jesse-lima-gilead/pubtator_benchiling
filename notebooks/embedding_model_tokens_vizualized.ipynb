{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2> Model's Overview </h2>",
   "id": "a86098c88343ada6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of models with their specific token limits\n",
    "models = [\n",
    "    (\"dmis-lab/biobert-v1.1\", 512),\n",
    "    (\"microsoft/biogpt\", 1024),\n",
    "    (\"allenai/longformer-base-4096\", 4096),\n",
    "    (\"allenai/scibert_scivocab_uncased\", 512)\n",
    "]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>BERT: </h3>\n",
    "Utilizes a bidirectional Transformer architecture, meaning it processes the input text in both directions simultaneously. This allows BERT to capture the context around each word, considering all the words in the sentence.\n",
    "\n",
    "<h3>GPT: </h3>\n",
    "Employs a unidirectional Transformer architecture, processing the text from left to right. This design enables GPT to predict the next word in a sequence but limits its understanding of the context to the left of a given word."
   ],
   "id": "3312878e8424b8c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4>Attention Heads and Layers:</h4> \n",
    "Both models use multi-head attention, but they differ in the number of layers and heads. BERT has two versions with different configurations, while GPT-1 has a 12-level, 12-headed structure.\n",
    "\n",
    "<h4>Output Layer:</h4> \n",
    "BERT is fine-tuned with task-specific layers, while GPT-1 uses a linear-softmax layer for word prediction."
   ],
   "id": "ae762a282f9f4049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4>Use Case:</h4>\n",
    "-> BERT is very good at solving sentence and token-level classification tasks. Extensions of BERT (e.g., sBERT) can be used for semantic search, making BERT applicable to retrieval tasks as well. Finetuning BERT to solve classification tasks is oftentimes preferable to performing few-shot prompting via an LLM.\n",
    "\n",
    "->Encoder-only models such as BERT cannot generate text. This is where we need decoder-only models such as GPT. They are suitable for tasks like text generation, translation, etc.\n"
   ],
   "id": "f3066162d13682d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparison of Biomedical and Scientific NLP Model Architectures\n",
    "\n",
    "| Model | Token Limit | Multi-Head Attention | No. of Attention Heads | No. of Layers | Vector Dimensionality |\n",
    "|-------|-------------|----------------------|------------------------|----------------|------------------------|\n",
    "| dmis-lab/biobert-v1.1 | 512 | Yes | 12 | 12 | 768 |\n",
    "| microsoft/biogpt | 1024 | Yes | 24 | 24 | 2048 |\n",
    "| allenai/longformer-base-4096 | 4096 | Yes | 12 | 12 | 768 |\n",
    "| allenai/scibert_scivocab_uncased | 512 | Yes | 12 | 12 | 768 |\n",
    "\n",
    "## Additional Notes:\n",
    "\n",
    "1. **dmis-lab/biobert-v1.1**:\n",
    "   - Based on BERT architecture\n",
    "   - Specialized for biomedical text\n",
    "   - Uses WordPiece tokenization\n",
    "\n",
    "2. **microsoft/biogpt**:\n",
    "   - Based on GPT architecture\n",
    "   - Larger model compared to others in the list\n",
    "   - Uses byte-level BPE tokenization\n",
    "\n",
    "3. **allenai/longformer-base-4096**:\n",
    "   - Modified attention mechanism for long sequences\n",
    "   - Uses a combination of sliding window and global attention\n",
    "   - Based on RoBERTa architecture\n",
    "\n",
    "4. **allenai/scibert_scivocab_uncased**:\n",
    "   - Based on BERT architecture\n",
    "   - Uses its own vocabulary (scivocab) tailored for scientific text\n",
    "   - Trained on a large corpus of scientific publications\n",
    "\n",
    "Key Observations:\n",
    "- All models use multi-head attention, which is a fundamental component of transformer-based architectures.\n",
    "- BioBERT and SciBERT have the smallest token limits (512), while Longformer and BigBird can handle much longer sequences (4096 tokens).\n",
    "- BioGPT stands out with a larger number of attention heads, layers, and vector dimensionality, indicating a more complex model.\n",
    "- Despite differences in specialization, Longformer, BigBird, BioBERT, and SciBERT share similar architectural properties (12 attention heads, 12 layers, 768-dimensional vectors)."
   ],
   "id": "c4446bf3ed4a8693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> WordPiece Tokenization </h3>",
   "id": "614376dead31ff9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:05:44.148280Z",
     "start_time": "2024-10-17T12:05:43.833429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Breaks words into smaller subwords.\n",
    "\n",
    "#Whitespace Tokenization: The sentence is split by whitespace into individual tokens.\n",
    "\n",
    "#Subword Splitting: Each token is checked against the vocabulary. If the token is not in the vocabulary, it is split further into smaller subwords.\n",
    "\n",
    "from transformers import AutoTokenizer, BertTokenizerFast\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "# Example sentence\n",
    "text = \"Gut microbiota influences enzyme metabolism.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Add special tokens (CLS and SEP)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"Input IDs:\", inputs[\"input_ids\"])"
   ],
   "id": "896b514694e0f418",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['G', '##ut', 'micro', '##bio', '##ta', 'influences', 'enzyme', 'metabolism', '.']\n",
      "Token IDs: [144, 3818, 17599, 25959, 1777, 7751, 8744, 20443, 119]\n",
      "Input IDs: tensor([[  101,   144,  3818, 17599, 25959,  1777,  7751,  8744, 20443,   119,\n",
      "           102]])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:17:24.488663Z",
     "start_time": "2024-10-17T14:17:23.370389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#WordPiece algorithm.\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "\n",
    "# Load SciBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Example input\n",
    "text = \"Gut microbiota influences enzyme metabolism.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Prepare input with padding and truncation\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "print(\"Input IDs:\", inputs[\"input_ids\"])\n"
   ],
   "id": "ddff97a2226dbdd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['gut', 'microbiota', 'influences', 'enzyme', 'metabolism', '.']\n",
      "Token IDs: [7881, 17697, 7794, 3823, 5009, 205]\n",
      "Input IDs: tensor([[  102,  7881, 17697,  7794,  3823,  5009,   205,   103]])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Byte Pair Encoding Tokenisation</h3>",
   "id": "91c4083469c6986d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T14:19:26.165344Z",
     "start_time": "2024-10-17T14:19:25.020766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#BPE starts with individual characters and iteratively merges the most frequent pairs of tokens.\n",
    "\n",
    "#The BPE algorithm used in this tokenizer splits rare or out-of-vocabulary words into subword units.\n",
    "#If a word is present in the vocabulary as a whole, it will remain intact with the </w> suffix.\n",
    "#Byte Pair Encoding\n",
    "\n",
    "from transformers import BioGptTokenizer, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "# Example sentence\n",
    "text = \"Gut microbiota influences enzyme metabolism.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Add special tokens if needed (e.g., <|endoftext|>)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Input IDs:\", inputs[\"input_ids\"])"
   ],
   "id": "f58864ff034f0a5a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Gut</w>', 'microbiota</w>', 'influences</w>', 'enzyme</w>', 'metabolism</w>', '.</w>']\n",
      "Token IDs: [22560, 4940, 2991, 439, 795, 4]\n",
      "Input IDs: tensor([[    2, 22560,  4940,  2991,   439,   795,     4]])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:06:09.576013Z",
     "start_time": "2024-10-17T12:06:08.288582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#It uses BPE tokenization and handles both common words and subword splits effectively.\n",
    "#The Ġ prefix indicates that a word follows a space in the original text. This helps the model maintain the context of word boundaries.\n",
    "from transformers import LongformerTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "# Example input text\n",
    "text = \"Gut microbiota influences enzyme metabolism.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Prepare inputs with padding and truncation\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Input IDs:\", inputs[\"input_ids\"])\n"
   ],
   "id": "4a77a94ef4291689",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['G', 'ut', 'Ġmicrobiota', 'Ġinfluences', 'Ġenzyme', 'Ġmetabolism', '.']\n",
      "Token IDs: [534, 1182, 46009, 16882, 32834, 30147, 4]\n",
      "Input IDs: tensor([[    0,   534,  1182, 46009, 16882, 32834, 30147,     4,     2]])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T12:02:04.503642Z",
     "start_time": "2024-10-17T12:02:03.375601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# \n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "# \n",
    "# # Print the tokenizer type\n",
    "# print(type(tokenizer))\n"
   ],
   "id": "f34e2b355abfa924",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Model Info:\n",
    "#token limit, parameters, multi headed attentio?, no. of attention head, no. of layers, vector dimensional space"
   ],
   "id": "b08498dcc70a30ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a00b4ba8d12b0af1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
