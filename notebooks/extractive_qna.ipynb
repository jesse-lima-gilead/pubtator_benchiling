{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T07:38:52.617002Z",
     "start_time": "2024-10-29T07:38:51.390465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch"
   ],
   "id": "17be1671f5de4dc2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T07:39:12.634706Z",
     "start_time": "2024-10-29T07:39:12.630848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExtractiveQA:\n",
    "    def __init__(self, model_name=\"bert-large-uncased-whole-word-masking-finetuned-squad\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "    def get_answer(self, question, context):\n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            answer_start = torch.argmax(outputs.start_logits)\n",
    "            answer_end = torch.argmax(outputs.end_logits)\n",
    "            \n",
    "        # Convert token positions to character positions\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        answer = tokens[answer_start:answer_end + 1]\n",
    "        \n",
    "        # Clean up answer by removing special tokens and combining wordpieces\n",
    "        cleaned_answer = self.tokenizer.convert_tokens_to_string(answer)\n",
    "        \n",
    "        # Get confidence scores\n",
    "        start_confidence = torch.max(torch.softmax(outputs.start_logits, dim=1)).item()\n",
    "        end_confidence = torch.max(torch.softmax(outputs.end_logits, dim=1)).item()\n",
    "        confidence = (start_confidence + end_confidence) / 2\n",
    "        \n",
    "        return {\n",
    "            \"answer\": cleaned_answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"start_position\": answer_start.item(),\n",
    "            \"end_position\": answer_end.item()\n",
    "        }"
   ],
   "id": "d8799f4550e0d44b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T07:44:25.259657Z",
     "start_time": "2024-10-29T07:44:24.388862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = \"\"\"\n",
    "The Python programming language was created by Guido van Rossum and was released in 1991. \n",
    "Python is known for its simple syntax and readability. It has become one of the most popular \n",
    "programming languages for data science, machine learning, and web development.\n",
    "\"\"\"\n",
    "\n",
    "question1 = \"Who created Python?\"\n",
    "question2 = \"What is Python best used for?\"\n",
    "\n",
    "# Initialize QA system\n",
    "qa_system = ExtractiveQA()\n",
    "\n",
    "# Get answer 1\n",
    "result1 = qa_system.get_answer(question1, context)\n",
    "print(f\"Question: {question1}\")\n",
    "print(f\"Answer: {result1['answer']}\")\n",
    "print(f\"Confidence: {result1['confidence']:.2%}\")\n",
    "\n",
    "# Get answer 2\n",
    "result2 = qa_system.get_answer(question2, context)\n",
    "print(f\"Question: {question2}\")\n",
    "print(f\"Answer: {result2['answer']}\")\n",
    "print(f\"Confidence: {result2['confidence']:.2%}\")\n",
    "\n"
   ],
   "id": "1a6dae6b91f08561",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who created Python?\n",
      "Answer: guido van rossum\n",
      "Confidence: 99.91%\n",
      "Question: What is Python best used for?\n",
      "Answer: data science, machine learning, and web development\n",
      "Confidence: 91.54%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on Chunks",
   "id": "eb0748125cf2b902"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T04:48:31.893374Z",
     "start_time": "2024-10-30T04:48:30.659257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summary = \"\"\"\n",
    "Summary:\n",
    "Air pollution promotes lung cancer by inducing inflammation and expanding pre-existing oncogenic mutations. Particulate matter (PM2.5) exposure correlates with increased EGFR-driven lung cancer incidence across countries. PM2.5 triggers macrophage-derived interleukin-1β release, promoting a progenitor-like state in EGFR-mutant lung cells and accelerating tumor formation. Oncogenic EGFR and KRAS mutations were found in 18% and 53% of healthy lung samples, respectively, suggesting air pollutants may promote expansion of existing mutant clones rather than directly causing mutations.\n",
    "\"\"\"\n",
    "\n",
    "chunk_caps = \"\"\"\n",
    "Summary:\n",
    "Air pollution promotes lung cancer by inducing inflammation and expanding pre-existing oncogenic mutations. Particulate matter (PM2.5) exposure correlates with increased EGFR-driven lung cancer incidence across countries. PM2.5 triggers macrophage-derived interleukin-1β release, promoting a progenitor-like state in EGFR-mutant lung cells and accelerating tumor formation. Oncogenic EGFR and KRAS mutations were found in 18% and 53% of healthy lung samples, respectively, suggesting air pollutants may promote expansion of existing mutant clones rather than directly causing mutations.\n",
    "\n",
    "Annotations:\n",
    "Text - GFR \n",
    "Type - Gene\n",
    "NCBI Gene - 13649\n",
    "Text - RAS \n",
    "Type - Gene\n",
    "NCBI Gene - 16653\n",
    "Text - mouse\n",
    "Type - Species\n",
    "NCBI Taxonomy - 10090\n",
    "Text - EGFR\n",
    "Type - Gene\n",
    "NCBI Gene - 13649\n",
    "Text - KRAS\n",
    "Type - Gene\n",
    "NCBI Gene - 16653\n",
    "\n",
    "Text:\n",
    "A complete understanding of how exposure to environmental substances promotes cancer formation is lacking. More than 70 years ago, tumorigenesis was proposed to occur in a two-step process: an initiating step that induces mutations in healthy cells, followed by a promoter step that triggers cancer development1. Here we propose that environmental particulate matter measuring ≤2.5 μm (PM2.5), known to be associated with lung cancer risk, promotes lung cancer by acting on cells that harbour pre-existing oncogenic mutations in healthy lung tissue. Focusing on EGFR-driven lung cancer, which is more common in never-smokers or light smokers, we found a significant association between PM2.5 levels and the incidence of lung cancer for 32,957 EGFR driven lung cancer cases in four within-country cohorts. Functional mouse models revealed that air pollutants cause an influx of macrophages into the lung and release of interleukin-1β. This process results in a progenitor-like cell state within EGFR mutant lung alveolar type II epithelial cells that fuels tumorigenesis. Ultradeep mutational profiling of histologically normal lung tissue from 295 individuals across 3 clinical cohorts revealed oncogenic EGFR and KRAS driver mutations in 18% and 53% of healthy tissue samples, respectively. These findings collectively support a tumour promoting role for PM2.5 air pollutants and provide impetus for public health policy initiatives to address air pollution to reduce disease burden.      \n",
    "\"\"\"\n",
    "\n",
    "question1 = \"lung cancer and air pollution\"\n",
    "question2 = \"EGFR mutation frequency healthy lung tissue\"\n",
    "\n",
    "# Initialize QA system\n",
    "qa_system = ExtractiveQA()\n",
    "\n",
    "# Get answer 1\n",
    "result1 = qa_system.get_answer(question1, chunk_caps)\n",
    "print(f\"Question: {question1}\")\n",
    "print(f\"Answer: {result1['answer']}\")\n",
    "print(f\"Confidence: {result1['confidence']:.2%}\")\n",
    "\n",
    "# Get answer 2\n",
    "result2 = qa_system.get_answer(question2, chunk_caps)\n",
    "print(f\"Question: {question2}\")\n",
    "print(f\"Answer: {result2['answer']}\")\n",
    "print(f\"Confidence: {result2['confidence']:.2%}\")\n",
    "\n"
   ],
   "id": "4bb249a003a6dd90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (525) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 38\u001B[0m\n\u001B[1;32m     35\u001B[0m qa_system \u001B[38;5;241m=\u001B[39m ExtractiveQA()\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Get answer 1\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m result1 \u001B[38;5;241m=\u001B[39m \u001B[43mqa_system\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_answer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_caps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuestion: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquestion1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult1[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124manswer\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 17\u001B[0m, in \u001B[0;36mExtractiveQA.get_answer\u001B[0;34m(self, question, context)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Get model predictions\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 17\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     answer_start \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(outputs\u001B[38;5;241m.\u001B[39mstart_logits)\n\u001B[1;32m     19\u001B[0m     answer_end \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(outputs\u001B[38;5;241m.\u001B[39mend_logits)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1978\u001B[0m, in \u001B[0;36mBertForQuestionAnswering.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1966\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1967\u001B[0m \u001B[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1968\u001B[0m \u001B[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1974\u001B[0m \u001B[38;5;124;03m    are not taken into account for computing the loss.\u001B[39;00m\n\u001B[1;32m   1975\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1976\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1978\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1979\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1982\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1983\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1984\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1985\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1986\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1988\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1990\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1992\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqa_outputs(sequence_output)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1078\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1075\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1076\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m-> 1078\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1081\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1084\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1086\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1087\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones((batch_size, seq_length \u001B[38;5;241m+\u001B[39m past_key_values_length), device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/gileadpubtator-XugtBIpc-py3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:217\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    216\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embeddings(position_ids)\n\u001B[0;32m--> 217\u001B[0m     \u001B[43membeddings\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m\n\u001B[1;32m    218\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(embeddings)\n\u001B[1;32m    219\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(embeddings)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (525) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict, Generator, Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class BioCXMLParser:\n",
    "    \"\"\"Parser for BioC XML files with chunking capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512):\n",
    "        self.chunk_size = chunk_size\n",
    "    \n",
    "    def parse_bioc_file(self, file_path: str) -> Generator[Dict, None, None]:\n",
    "        \"\"\"Parse BioC XML file and yield documents with their annotations\"\"\"\n",
    "        for event, elem in ET.iterparse(file_path, events=(\"start\", \"end\")):\n",
    "            if event == \"end\" and elem.tag == \"document\":\n",
    "                doc = self._process_document(elem)\n",
    "                yield doc\n",
    "                elem.clear()  # Clear memory\n",
    "    \n",
    "    def _process_document(self, doc_elem) -> Dict:\n",
    "        \"\"\"Process a single document element\"\"\"\n",
    "        passages = []\n",
    "        annotations = []\n",
    "        \n",
    "        # Extract passages and their annotations\n",
    "        for passage in doc_elem.findall(\".//passage\"):\n",
    "            text = passage.find(\"text\").text\n",
    "            passage_annotations = []\n",
    "            \n",
    "            for annotation in passage.findall(\".//annotation\"):\n",
    "                anno_text = annotation.find(\"text\").text\n",
    "                locations = [(loc.get(\"offset\"), loc.get(\"length\")) \n",
    "                           for loc in annotation.findall(\"location\")]\n",
    "                passage_annotations.append({\n",
    "                    \"text\": anno_text,\n",
    "                    \"locations\": locations\n",
    "                })\n",
    "            \n",
    "            passages.append(text)\n",
    "            annotations.extend(passage_annotations)\n",
    "        \n",
    "        return {\n",
    "            \"id\": doc_elem.find(\"id\").text,\n",
    "            \"passages\": passages,\n",
    "            \"annotations\": annotations\n",
    "        }\n",
    "    \n",
    "    def chunk_text(self, text: str, overlap: int = 100) -> List[str]:\n",
    "        \"\"\"Create overlapping chunks of text\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            \n",
    "            # Try to end at a sentence boundary\n",
    "            if end < len(text):\n",
    "                # Look for sentence endings (.!?) followed by space\n",
    "                for i in range(end-1, max(start, end-50), -1):\n",
    "                    if text[i] in \".!?\" and (i+1 >= len(text) or text[i+1].isspace()):\n",
    "                        end = i + 1\n",
    "                        break\n",
    "            \n",
    "            chunks.append(text[start:end])\n",
    "            start = end - overlap\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "class BiomedicalQA:\n",
    "    \"\"\"Biomedical Question Answering System with semantic search\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 qa_model_name: str = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                 embedding_model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\"):\n",
    "        # QA model for answer extraction\n",
    "        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "        \n",
    "        # Embedding model for semantic search\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        \n",
    "        # Initialize parser\n",
    "        self.parser = BioCXMLParser()\n",
    "    \n",
    "    def get_relevant_chunks(self, \n",
    "                          question: str, \n",
    "                          chunks: List[str], \n",
    "                          top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find most relevant chunks using semantic search\"\"\"\n",
    "        # Get embeddings\n",
    "        question_embedding = self.embedding_model.encode([question])[0]\n",
    "        chunk_embeddings = self.embedding_model.encode(chunks)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([question_embedding], chunk_embeddings)[0]\n",
    "        \n",
    "        # Get top-k chunks\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(chunks[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    def get_answer(self, question: str, context: str) -> Dict:\n",
    "        \"\"\"Extract answer from context\"\"\"\n",
    "        inputs = self.qa_tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.qa_model(**inputs)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            \n",
    "            start_idx = torch.argmax(start_scores)\n",
    "            end_idx = torch.argmax(end_scores)\n",
    "            \n",
    "            tokens = self.qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "            answer_tokens = tokens[start_idx:end_idx + 1]\n",
    "            answer = self.qa_tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "            \n",
    "            confidence = (torch.max(torch.softmax(start_scores, dim=1)) + \n",
    "                        torch.max(torch.softmax(end_scores, dim=1))).item() / 2\n",
    "            \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"context\": context\n",
    "        }\n",
    "    \n",
    "    def process_file(self, file_path: str, question: str) -> List[Dict]:\n",
    "        \"\"\"Process entire BioC XML file and find answers\"\"\"\n",
    "        answers = []\n",
    "        \n",
    "        for doc in self.parser.parse_bioc_file(file_path):\n",
    "            # Combine passages into single text\n",
    "            full_text = \" \".join(doc[\"passages\"])\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self.parser.chunk_text(full_text)\n",
    "            \n",
    "            # Get relevant chunks\n",
    "            relevant_chunks = self.get_relevant_chunks(question, chunks)\n",
    "            \n",
    "            # Get answers from relevant chunks\n",
    "            for chunk, similarity in relevant_chunks:\n",
    "                answer = self.get_answer(question, chunk)\n",
    "                answer[\"document_id\"] = doc[\"id\"]\n",
    "                answer[\"semantic_similarity\"] = similarity\n",
    "                answers.append(answer)\n",
    "        \n",
    "        # Sort by combined confidence and semantic similarity\n",
    "        answers.sort(key=lambda x: (x[\"confidence\"] + x[\"semantic_similarity\"]) / 2, reverse=True)\n",
    "        return answers\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    qa_system = BiomedicalQA()\n",
    "    \n",
    "    # Process a BioC XML file\n",
    "    file_path = \"path/to/your/bioc_file.xml\"\n",
    "    question = \"What are the side effects of aspirin?\"\n",
    "    \n",
    "    answers = qa_system.process_file(file_path, question)\n",
    "    \n",
    "    # Print top answer\n",
    "    if answers:\n",
    "        top_answer = answers[0]\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {top_answer['answer']}\")\n",
    "        print(f\"Confidence: {top_answer['confidence']:.2%}\")\n",
    "        print(f\"Semantic Similarity: {top_answer['semantic_similarity']:.2%}\")\n",
    "        print(f\"Document ID: {top_answer['document_id']}\")\n",
    "    else:\n",
    "        print(\"No answers found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "5361f76d5badb6a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b5042b492c2367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ff62422097e8233"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T04:31:45.672610Z",
     "start_time": "2024-10-30T04:31:42.889780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict, Tuple, List\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "\n",
    "class EnhancedBiomedicalQA:\n",
    "    def __init__(self, model_name: str = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "        # Get all special tokens\n",
    "        self.special_token_ids = set(self.tokenizer.all_special_ids)\n",
    "        \n",
    "        # Dictionary of special tokens and their meanings\n",
    "        self.special_tokens = {\n",
    "            '[CLS]': 'Classification token (start of sequence)',\n",
    "            '[SEP]': 'Separator token (end of sequence/between sequences)',\n",
    "            '[PAD]': 'Padding token',\n",
    "            '[UNK]': 'Unknown token',\n",
    "            '[MASK]': 'Masked token for MLM tasks'\n",
    "        }\n",
    "    \n",
    "    def is_special_token_id(self, token_id: int) -> bool:\n",
    "        \"\"\"Check if a token ID is a special token\"\"\"\n",
    "        return token_id in self.special_token_ids\n",
    "    \n",
    "    def clean_answer(self, answer: str) -> str:\n",
    "        \"\"\"Remove special tokens and clean up the answer\"\"\"\n",
    "        # Remove all special tokens\n",
    "        for special_token in self.special_tokens.keys():\n",
    "            answer = answer.replace(special_token, '')\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        answer = re.sub(r'\\s+', ' ', answer).strip()\n",
    "        return answer\n",
    "    \n",
    "    def get_answer_with_constraints(self, \n",
    "                                  question: str, \n",
    "                                  context: str,\n",
    "                                  min_length: int = 10,\n",
    "                                  max_length: int = 100,\n",
    "                                  min_confidence: float = 0.2) -> Dict:\n",
    "        \"\"\"Get answer with length constraints and confidence threshold\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True,\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Get token offsets for mapping back to original text\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get probability distributions\n",
    "            start_probs = torch.softmax(outputs.start_logits, dim=1)[0]\n",
    "            end_probs = torch.softmax(outputs.end_logits, dim=1)[0]\n",
    "            \n",
    "            # Find valid start-end pairs\n",
    "            valid_answers = []\n",
    "            for start_idx in range(len(start_probs)):\n",
    "                for end_idx in range(start_idx, min(len(end_probs), start_idx + max_length)):\n",
    "                    # Skip special tokens\n",
    "                    if self.is_special_token_id(inputs['input_ids'][0][start_idx].item()) or \\\n",
    "                       self.is_special_token_id(inputs['input_ids'][0][end_idx].item()):\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate score for this span\n",
    "                    score = start_probs[start_idx] * end_probs[end_idx]\n",
    "                    \n",
    "                    # Get answer text\n",
    "                    answer_tokens = inputs['input_ids'][0][start_idx:end_idx + 1]\n",
    "                    answer_text = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "                    cleaned_answer = self.clean_answer(answer_text)\n",
    "                    \n",
    "                    # Check constraints\n",
    "                    if len(cleaned_answer.split()) >= min_length / 5 and \\\n",
    "                       score >= min_confidence and \\\n",
    "                       cleaned_answer:\n",
    "                        # Get character positions in original text\n",
    "                        char_start = offset_mapping[start_idx][0].item()\n",
    "                        char_end = offset_mapping[end_idx][1].item()\n",
    "                        \n",
    "                        valid_answers.append({\n",
    "                            'answer': cleaned_answer,\n",
    "                            'confidence': score.item(),\n",
    "                            'char_start': char_start,\n",
    "                            'char_end': char_end\n",
    "                        })\n",
    "            \n",
    "            # Sort by confidence and get best answer\n",
    "            valid_answers.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "            \n",
    "            if not valid_answers:\n",
    "                return {\n",
    "                    'answer': '',\n",
    "                    'confidence': 0.0,\n",
    "                    'char_start': -1,\n",
    "                    'char_end': -1,\n",
    "                    'error': 'No valid answer found meeting constraints'\n",
    "                }\n",
    "            \n",
    "            return valid_answers[0]\n",
    "    \n",
    "    def highlight_answer(self, context: str, char_start: int, char_end: int) -> str:\n",
    "        \"\"\"Create HTML with highlighted answer in context\"\"\"\n",
    "        if char_start < 0 or char_end < 0:\n",
    "            return context\n",
    "        \n",
    "        highlighted = (\n",
    "            context[:char_start] +\n",
    "            f'<span style=\"background-color: yellow\">{context[char_start:char_end]}</span>' +\n",
    "            context[char_end:]\n",
    "        )\n",
    "        return highlighted\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    qa = EnhancedBiomedicalQA()\n",
    "    \n",
    "    # Example context and question\n",
    "    context = \"\"\"\n",
    "Annotations:\n",
    "Text - GFR \n",
    "Type - Gene\n",
    "NCBI Gene - 13649\n",
    "Text - RAS \n",
    "Type - Gene\n",
    "NCBI Gene - 16653\n",
    "Text - mouse\n",
    "Type - Species\n",
    "NCBI Taxonomy - 10090\n",
    "Text - EGFR\n",
    "Type - Gene\n",
    "NCBI Gene - 13649\n",
    "Text - KRAS\n",
    "Type - Gene\n",
    "NCBI Gene - 16653\n",
    "\n",
    "Text:\n",
    "A complete understanding of how exposure to environmental substances promotes cancer formation is lacking. More than 70 years ago, tumorigenesis was proposed to occur in a two-step process: an initiating step that induces mutations in healthy cells, followed by a promoter step that triggers cancer development1. Here we propose that environmental particulate matter measuring ≤2.5 μm (PM2.5), known to be associated with lung cancer risk, promotes lung cancer by acting on cells that harbour pre-existing oncogenic mutations in healthy lung tissue. Focusing on EGFR-driven lung cancer, which is more common in never-smokers or light smokers, we found a significant association between PM2.5 levels and the incidence of lung cancer for 32,957 EGFR driven lung cancer cases in four within-country cohorts. Functional mouse models revealed that air pollutants cause an influx of macrophages into the lung and release of interleukin-1β. This process results in a progenitor-like cell state within EGFR mutant lung alveolar type II epithelial cells that fuels tumorigenesis. Ultradeep mutational profiling of histologically normal lung tissue from 295 individuals across 3 clinical cohorts revealed oncogenic EGFR and KRAS driver mutations in 18% and 53% of healthy tissue samples, respectively. These findings collectively support a tumour promoting role for PM2.5 air pollutants and provide impetus for public health policy initiatives to address air pollution to reduce disease burden.      \n",
    "    \"\"\"\n",
    "    \n",
    "    question = \"EGFR mutation frequency healthy lung tissue\"\n",
    "    \n",
    "    # Get answer with constraints\n",
    "    result = qa.get_answer_with_constraints(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        min_length=10,  # Minimum 10 characters\n",
    "        max_length=100,  # Maximum 100 characters\n",
    "        min_confidence=0.2  # Minimum 20% confidence\n",
    "    )\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    \n",
    "    if result['char_start'] >= 0:\n",
    "        print(\"\\nHighlighted context:\")\n",
    "        print(result['char_start'], result['char_end'])\n",
    "        highlighted = qa.highlight_answer(\n",
    "            context=context,\n",
    "            char_start=result['char_start'],\n",
    "            char_end=result['char_end']\n",
    "        )\n",
    "        print(highlighted)\n",
    "\n",
    "main()"
   ],
   "id": "1642ab1f8a5596d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: EGFR mutation frequency healthy lung tissue\n",
      "Answer: \n",
      "Confidence: 0.00%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d4adb7675aa12f3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
