{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-23T11:56:07.760507Z",
     "start_time": "2024-10-23T11:56:05.732898Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from src.data_processing.indexing.embeddings_handler import (\n",
    "    get_embeddings,\n",
    "    get_model_info,\n",
    "    save_embeddings_details_to_json,\n",
    "    load_embeddings_details_from_json,\n",
    "    find_most_similar,\n",
    "    save_to_csv\n",
    ")\n",
    "import numpy as np\n",
    "from typing import List, Union, Dict, Any"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_embedding_details(\n",
    "        chunk_dir: str, \n",
    "        annotations_models: List[str], \n",
    "        embedding_models: List[str],\n",
    "        output_file_path: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    annotations_models = [\n",
    "        \"bioformer\",\n",
    "        \"pubmedbert\"\n",
    "    ]\n",
    "\n",
    "    embedding_models = [\n",
    "        \"bio_bert\",\n",
    "        \"bio_gpt\",\n",
    "        \"longformer\",\n",
    "        \"sci_bert\"\n",
    "    ]\n",
    "\n",
    "    all_embedding_detials = []\n",
    "\n",
    "    chunk_dir = \"../../data/PMC_7614604_chunks\"\n",
    "    #chunk_dir = \"../../data/test/test\"\n",
    "\n",
    "    # Read the chunks and create embeddings:\n",
    "    for annotation_model in annotations_models:\n",
    "        print(\"Processing for Annotation Model: \", annotation_model)\n",
    "        for cur_file in os.listdir(chunk_dir):\n",
    "            if cur_file.endswith(\".json\") and annotation_model in cur_file:\n",
    "                input_file_path = f\"{chunk_dir}/{cur_file}\"\n",
    "\n",
    "                with open(f\"{input_file_path}\", \"r\") as f:\n",
    "                    print(f\"Processing {input_file_path}\")\n",
    "                    chunks = json.load(f)\n",
    "                    merged_texts_with_sum = [\n",
    "                        f\"Summary:\\n{article_summary}\\nText:\\n{chunk[\"merged_text\"]}\"\n",
    "                        for chunk in chunks\n",
    "                    ]\n",
    "                    #print(merged_texts_with_sum)\n",
    "                    merged_texts_without_sum = [chunk[\"merged_text\"] for chunk in chunks]\n",
    "\n",
    "                    # Creating Embeddings with Summary\n",
    "                    for embedding_model in embedding_models:\n",
    "                        print(f\"Processing for Embedding Model: {embedding_model} with article summary\")\n",
    "                        model_info = get_model_info(embedding_model)\n",
    "                        embeddings = get_embeddings(\n",
    "                            model_name=model_info[0],\n",
    "                            token_limit=model_info[1],\n",
    "                            texts=merged_texts_with_sum\n",
    "                        )\n",
    "\n",
    "                        embeddings_details = {\n",
    "                            \"file\": cur_file,\n",
    "                            \"chunks_count\": len(merged_texts_with_sum),\n",
    "                            \"annotation_model\": annotation_model,\n",
    "                            \"embeddings_model\": embedding_model,\n",
    "                            \"embeddings_model_token_limit\": model_info[1],\n",
    "                            \"contains_summary\": True,\n",
    "                            \"embeddings\": embeddings\n",
    "                        }\n",
    "\n",
    "                        all_embedding_detials.append(embeddings_details)\n",
    "\n",
    "                    # Creating Embeddings without Summary\n",
    "                    for embedding_model in embedding_models:\n",
    "                        print(f\"Processing for Embedding Model: {embedding_model} without article summary\")\n",
    "                        model_info = get_model_info(embedding_model)\n",
    "                        embeddings = get_embeddings(\n",
    "                            model_name=model_info[0],\n",
    "                            token_limit=model_info[1],\n",
    "                            texts=merged_texts_without_sum\n",
    "                        )\n",
    "\n",
    "                        embeddings_details = {\n",
    "                            \"file\": cur_file,\n",
    "                            \"chunks_count\": len(merged_texts_without_sum),\n",
    "                            \"annotation_model\": annotation_model,\n",
    "                            \"embeddings_model\": embedding_model,\n",
    "                            \"embeddings_model_token_limit\": model_info[1],\n",
    "                            \"contains_summary\": False,\n",
    "                            \"embeddings\": embeddings\n",
    "                        }\n",
    "\n",
    "                        all_embedding_detials.append(embeddings_details)\n",
    "\n",
    "    # Write the Embeddings to a file:\n",
    "    file_path = \"../../data/PMC_7614604_chunks/embeddings/PMC_7614604_embeddings.json\"\n",
    "    save_embeddings_details_to_json(all_embedding_detials, file_path)\n"
   ],
   "id": "bac684613028e3ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
