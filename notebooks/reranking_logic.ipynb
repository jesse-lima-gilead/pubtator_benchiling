{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retrieval and Reranking Logic:\n",
    "\n",
    "## Retrieval Logic - Iteration 1:\n",
    "\n",
    "- Lets say we have embeddings of articles stored in a vector db\n",
    "- There is a given user query, we create an embedding of the query\n",
    "- We extract certain keywords (mainly NOUNS) from user queries using basic NER (Stanza)\n",
    "- We perform initial keyword based filtering on chunks using the chunk's metadata. eg.\n",
    "    - Filtering on the Author's names\n",
    "    - Filtering on the Journal's name\n",
    "    - Filtering on the Year of Publication (if applicable)\n",
    "- We then calculate the cosine similarity between the query embedding and the chunks embedding\n",
    "- We fetch Top k (=5) ARTICLES with there top N (=5) matching chunks. \n",
    "- Then we perform the RERANKING to reduce the False Positives.\n",
    "\n",
    "## Reranking Logic - Iteration 1:\n",
    "\n",
    "Lets say, we have Top 5 Articles with top 5 similar chunks\n",
    "\n",
    "Logic:\n",
    "- Calculate simillarity score between those top k (=5) articles (full articles).\n",
    "- This would give us k*(k-1)/2 (for k=5, =10) similarity scores.\n",
    "- Classify those Article pairs in 1 of 4 clusters in a k X k Matrix (here, 5X5):\n",
    "    - High Similarity\n",
    "    - Moderate Similarity\n",
    "    - Low Similarity\n",
    "    - Unrelated  \n",
    "- Example of a k X k matrix:\n",
    "5 X 5 Matrix of Simillarity\n",
    "    a1  a2  a3  a4  a5\n",
    "a1  HS  LS  HS  LS  LS\n",
    "a2  LS  HS  MS  HS  HS\n",
    "a3  HS  MS  HS  LS  LS\n",
    "a4  LS  HS  LS  HS  HS\n",
    "a5  LS  HS  LS  HS  HS\n",
    "\n",
    "Hence, \n",
    "a1 and a3 are HS\n",
    "a2, a4, a5 are HS\n",
    "(a2, a4, a5) are LS to a1\n",
    "(a2, a4, a5) are LS to a3\n",
    "\n",
    "Question:\n",
    "- Which one out of (a1,a3) or (a2,a4,a5) is the actual and which one is Decoy?\n",
    "- How to assign weightage to Similarity Score with User Query VS the Similarity Score with other articles\n",
    "\n",
    "Possible Solution: Weighted Score\n",
    "eg. Let's assume:\n",
    "Similarity Score with User Query - 0.87\n",
    "Similarity Score with Other Articles - 0.69 0.23 0.45 0.56 0.78\n",
    "Avg Simillarity Score with other articles - 0.542\n",
    "Number of Citations - 10 -> Norminalise it to a score between 0 and 1 - 0.2\n",
    "Other Factors - 0.4 (If any??)\n",
    "\n",
    "Weights:\n",
    "SS with User Query - 0.7\n",
    "SS with Other articles - 0.2\n",
    "Other Factors - 0.1\n",
    "\n",
    "Overall Simillarity Score = 0.87 * 0.7 + 0.67 * 0.2 + (0.2 + 0.4) * 0.1 = 0.803\n",
    "\n",
    "- Finally, we can rank the Articles based on the Overall Simillarity Score\n",
    "\n"
   ],
   "id": "4a58e3467bcff279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c755f48cd0c054ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7c8da69a967db660"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
