{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ed8eed-e698-4031-a8e8-c8d85dbdb47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PDF Local Ingestor\n",
    "\n",
    "This notebook ingests a PDF file locally using functions from the original project.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses **PyMuPDF** for proper PDF text extraction (not Pandoc)\n",
    "- Leverages `extract_pages_block_level_simple` and BioC converters from the project\n",
    "- Automatically detects and removes headers/footers\n",
    "- Detects section headings (Abstract, Methods, Results, etc.)\n",
    "- Mocks config to bypass HPC/S3/database dependencies\n",
    "- **No PostgreSQL** - **No S3** - **Fully local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a39633d-8a9a-409f-9a1c-3c908161ec62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b74a31-6487-43c5-b925-7ebcbf48597e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# This allows us to use the project code without HPC/S3/Database dependencies\n",
    "# ============================================================================\n",
    "\n",
    "# Mock config that simulates \"test\" storage type with local paths\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # AWS config needed by file_handler_factory at import time\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a mock YAMLConfigLoader\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "# Apply the mock BEFORE importing project modules\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK db.py TO PREVENT DATABASE CONNECTION AT IMPORT TIME\n",
    "# db.py has module-level code: db_url = get_db_url() and engine = create_engine(db_url)\n",
    "# ============================================================================\n",
    "import sys\n",
    "from types import ModuleType\n",
    "\n",
    "# Create a mock db module\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "\n",
    "# Register the mock in sys.modules BEFORE any imports that might reference it\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Also mock the FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    # Include _handlers dict that original factory has\n",
    "    _handlers = {\n",
    "        \"local\": LocalFileHandler,\n",
    "        \"test\": LocalFileHandler,\n",
    "        \"s3\": LocalFileHandler,  # Mock S3 to also use local\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e029d5-2faf-430e-ba23-db0caeb00a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES (now safe after mocking config)\n",
    "# ============================================================================\n",
    "\n",
    "# Logger\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# PDF extraction using PyMuPDF (the correct approach for PDFs!)\n",
    "from src.data_ingestion.ingest_preprints_rxivs.preprint_pdf_to_bioc_converter import (\n",
    "    extract_pages_block_level_simple,  # Extracts text blocks from PDF using PyMuPDF\n",
    "    make_document_from_blocks,          # Converts blocks to passages with merging\n",
    "    build_bioc_collection_lib,          # Creates BioC collection\n",
    "    find_running_headers_footers,       # Detects repeating headers/footers\n",
    "    detect_heading_and_strip_regex,     # Detects section headings\n",
    "    clean_xml_text,                     # Cleans text for XML output\n",
    ")\n",
    "\n",
    "import bioc\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize logger and file handler\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  Using project modules (PyMuPDF-based):\")\n",
    "print(\"  - extract_pages_block_level_simple (PDF â†’ text blocks)\")\n",
    "print(\"  - make_document_from_blocks (blocks â†’ passages)\")\n",
    "print(\"  - build_bioc_collection_lib (passages â†’ BioC)\")\n",
    "print(\"  - find_running_headers_footers (header/footer detection)\")\n",
    "print(\"  - detect_heading_and_strip_regex (section heading detection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e01051-63f3-4bb9-8161-d1229e2e7c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input PDF and output directories. All processing happens locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e2a73a-4c10-484e-b0c6-93f110e3b38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input PDF file path\n",
    "PDF_INPUT_PATH = \"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/attention.pdf\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "INTERIM_PATH = OUTPUT_BASE_DIR / \"interim\"\n",
    "BIOC_PATH = OUTPUT_BASE_DIR / \"bioc_xml\"\n",
    "FAILED_PATH = OUTPUT_BASE_DIR / \"failed\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "\n",
    "# Get PDF name without extension\n",
    "pdf_name = Path(PDF_INPUT_PATH).stem  # e.g., \"attention\"\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, INTERIM_PATH, BIOC_PATH, FAILED_PATH, METADATA_PATH, EMBEDDINGS_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ PDF to process: {pdf_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958f5d77-6870-4ee1-a254-234eab7a1926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read and Prepare PDF\n",
    "\n",
    "Copy the PDF to the ingestion directory using `LocalFileHandler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91efabe3-0ce5-4a24-9327-9fa59feea95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read PDF and copy to ingestion directory\n",
    "pdf_source_path = Path(PDF_INPUT_PATH).resolve()\n",
    "\n",
    "if not file_handler.exists(str(pdf_source_path)):\n",
    "    raise FileNotFoundError(f\"PDF not found: {pdf_source_path}\")\n",
    "\n",
    "pdf_content = file_handler.read_file_bytes(str(pdf_source_path))\n",
    "pdf_dest_path = INGESTION_PATH / f\"{pdf_name}.pdf\"\n",
    "file_handler.write_file(str(pdf_dest_path), pdf_content)\n",
    "\n",
    "print(f\"âœ“ PDF: {pdf_source_path}\")\n",
    "print(f\"âœ“ Size: {len(pdf_content):,} bytes\")\n",
    "print(f\"âœ“ Copied to: {pdf_dest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef6de71a-189a-481f-8079-8da01699572a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Extract Text from PDF using PyMuPDF\n",
    "\n",
    "Using `extract_pages_block_level_simple` from `preprint_pdf_to_bioc_converter.py` (PyMuPDF-based extraction).\n",
    "\n",
    "This function:\n",
    "- Opens the PDF with PyMuPDF\n",
    "- Detects and removes repeating headers/footers\n",
    "- Identifies table regions (to avoid duplicating table text)\n",
    "- Extracts text blocks with position information\n",
    "- Detects section headings (Abstract, Methods, Results, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecca5f36-cd05-4a5d-ae76-bfb71efbfb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract text blocks from PDF using PyMuPDF\n",
    "# This properly reads the PDF (unlike Pandoc which doesn't support PDF input)\n",
    "\n",
    "pdf_file_path = str(pdf_dest_path)\n",
    "\n",
    "# Extract blocks from all pages\n",
    "# Returns: List[List[Tuple[heading, body_text]]] - one list per page\n",
    "kept_blocks_per_page = extract_pages_block_level_simple(\n",
    "    pdf_path=pdf_file_path,\n",
    "    table_thresh=0.2,  # Drop text blocks that overlap tables by >= 20%\n",
    ")\n",
    "\n",
    "# Count total blocks extracted\n",
    "total_blocks = sum(len(page_blocks) for page_blocks in kept_blocks_per_page)\n",
    "total_pages = len(kept_blocks_per_page)\n",
    "\n",
    "print(f\"âœ“ Extracted {total_blocks} text blocks from {total_pages} pages\")\n",
    "print(f\"âœ“ Source: {pdf_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200d40fb-c765-40d9-b97b-36d96196a730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Preview Extracted Blocks\n",
    "\n",
    "Show the extracted text blocks with their detected section headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6e29a4-0b3d-4efe-bacb-57f86529890b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview extracted blocks (first few from each page)\n",
    "print(\"Extracted blocks preview:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for page_idx, page_blocks in enumerate(kept_blocks_per_page[:3]):  # First 3 pages\n",
    "    print(f\"\\nðŸ“„ Page {page_idx + 1} ({len(page_blocks)} blocks)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for block_idx, (heading, body_text) in enumerate(page_blocks[:3]):  # First 3 blocks per page\n",
    "        preview = body_text[:100] + \"...\" if len(body_text) > 100 else body_text\n",
    "        word_count = len(body_text.split())\n",
    "        print(f\"  [{block_idx + 1}] {heading} ({word_count} words)\")\n",
    "        print(f\"      {preview}\")\n",
    "    \n",
    "    if len(page_blocks) > 3:\n",
    "        print(f\"      ... and {len(page_blocks) - 3} more blocks on this page\")\n",
    "\n",
    "if len(kept_blocks_per_page) > 3:\n",
    "    print(f\"\\n... and {len(kept_blocks_per_page) - 3} more pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0903e3d-3968-49fb-8800-8406d1526a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show section heading distribution\n",
    "from collections import Counter\n",
    "\n",
    "all_headings = [heading for page_blocks in kept_blocks_per_page \n",
    "                for heading, _ in page_blocks]\n",
    "heading_counts = Counter(all_headings)\n",
    "\n",
    "print(\"Section heading distribution:\")\n",
    "print(\"-\" * 40)\n",
    "for heading, count in heading_counts.most_common():\n",
    "    print(f\"  {heading}: {count} block(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b322ea9-3526-44f9-a7e0-69ca0cfc151d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Convert to BioC XML\n",
    "\n",
    "Using `make_document_from_blocks` and `build_bioc_collection_lib` from `preprint_pdf_to_bioc_converter.py`.\n",
    "\n",
    "This step:\n",
    "- Merges small consecutive blocks (minimum 100 words per passage)\n",
    "- Creates properly structured BioC passages with section types\n",
    "- Builds a complete BioC collection with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cd5e55-be66-419c-a4f5-8be40a1fcac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert extracted blocks â†’ BioC using the preprint converter functions\n",
    "\n",
    "# Create metadata for the document\n",
    "metadata_infons = {\n",
    "    \"source\": \"local_pdf\",\n",
    "    \"filename\": pdf_name,\n",
    "    \"title\": pdf_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(pdf_source_path),\n",
    "}\n",
    "\n",
    "# Step 1: Convert blocks to a document dict with merged passages\n",
    "# This merges consecutive blocks until each passage has at least 100 words\n",
    "doc_dict = make_document_from_blocks(\n",
    "    doc_id=pdf_name,\n",
    "    kept_blocks_per_page=kept_blocks_per_page,\n",
    "    infons=metadata_infons,\n",
    "    min_words=100,  # Minimum words per passage before merging stops\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Document created: {doc_dict['id']}\")\n",
    "print(f\"âœ“ Passages after merging: {len(doc_dict['passages'])}\")\n",
    "\n",
    "# Step 2: Build BioC collection\n",
    "bioc_collection = build_bioc_collection_lib(\n",
    "    source=\"Local PDF\",\n",
    "    date_str=datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    documents=[doc_dict],\n",
    ")\n",
    "\n",
    "print(f\"âœ“ BioC collection created: {len(bioc_collection.documents)} document(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c16e65-9b5e-4d18-9dbb-ea03134e2981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The PyMuPDF extraction already handles:\n",
    "# - Header/footer removal (via find_running_headers_footers)\n",
    "# - Section heading detection (via detect_heading_and_strip_regex)  \n",
    "# - Small passage merging (via make_document_from_blocks with min_words)\n",
    "# - XML-safe text cleaning (via clean_xml_text)\n",
    "\n",
    "# Show passage statistics\n",
    "total_passages = sum(len(doc.passages) for doc in bioc_collection.documents)\n",
    "total_words = sum(\n",
    "    len(passage.text.split()) \n",
    "    for doc in bioc_collection.documents \n",
    "    for passage in doc.passages\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Total passages: {total_passages}\")\n",
    "print(f\"âœ“ Total words: {total_words:,}\")\n",
    "print(f\"âœ“ Average words per passage: {total_words // max(total_passages, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833653e2-cfda-4fae-b05a-afb58ca3b6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save BioC XML\n",
    "bioc_xml_path = BIOC_PATH / f\"{pdf_name}.xml\"\n",
    "\n",
    "with open(bioc_xml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    bioc.dump(bioc_collection, f)\n",
    "\n",
    "bioc_size = bioc_xml_path.stat().st_size\n",
    "print(f\"âœ“ Saved: {bioc_xml_path} ({bioc_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90649ac6-2922-4dd3-89fe-2b8a6aa6752c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Inspect Results\n",
    "\n",
    "View the BioC XML structure and passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d42b45-f7d2-4021-892a-eb1d5c183526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect BioC document\n",
    "for doc in bioc_collection.documents:\n",
    "    print(f\"Document: {doc.id}\")\n",
    "    print(f\"Passages: {len(doc.passages)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Show first 5 passages\n",
    "    for i, passage in enumerate(doc.passages[:5]):\n",
    "        # The preprint converter uses \"type\" for section headings\n",
    "        section_type = passage.infons.get(\"type\", \"body_text\")\n",
    "        word_count = len(passage.text.split()) if passage.text else 0\n",
    "        preview = (passage.text[:80] + \"...\") if len(passage.text) > 80 else passage.text\n",
    "        print(f\"[{i+1}] {section_type} ({word_count} words)\")\n",
    "        print(f\"    {preview}\")\n",
    "    \n",
    "    if len(doc.passages) > 5:\n",
    "        print(f\"\\n... and {len(doc.passages) - 5} more passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f4ff2d-7057-451b-97a3-5192b6a34aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List generated output files\n",
    "print(\"Generated files:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_BASE_DIR):\n",
    "    level = root.replace(str(OUTPUT_BASE_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder = os.path.basename(root)\n",
    "    if files:  # Only show folders with files\n",
    "        print(f\"{indent}{folder}/\")\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size = file_path.stat().st_size\n",
    "            print(f\"{indent}  {file} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb17b454-b204-497b-b6c5-b7d05b3ba26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook runs the **complete PDF ingestion pipeline** using **PyMuPDF** for proper PDF text extraction.\n",
    "\n",
    "### Why PyMuPDF instead of Pandoc?\n",
    "\n",
    "**Pandoc does NOT support PDF as an input format.** It can only output to PDF (via LaTeX), but cannot read PDFs. The previous `convert_apollo_to_html()` function was broken because it tried to use Pandoc for PDF input.\n",
    "\n",
    "**PyMuPDF** (`pymupdf`) properly reads PDF files and extracts text blocks with position information.\n",
    "\n",
    "### Functions Used from Project:\n",
    "\n",
    "| Function | Module | Purpose |\n",
    "|----------|--------|---------|\n",
    "| `LocalFileHandler` | `file_handler.local_handler` | File I/O |\n",
    "| `SingletonLogger` | `logs_handler.logger` | Logging |\n",
    "| `extract_pages_block_level_simple` | `preprint_pdf_to_bioc_converter` | PDF â†’ text blocks (PyMuPDF) |\n",
    "| `make_document_from_blocks` | `preprint_pdf_to_bioc_converter` | Blocks â†’ merged passages |\n",
    "| `build_bioc_collection_lib` | `preprint_pdf_to_bioc_converter` | Passages â†’ BioC collection |\n",
    "| `find_running_headers_footers` | `preprint_pdf_to_bioc_converter` | Header/footer detection |\n",
    "| `detect_heading_and_strip_regex` | `preprint_pdf_to_bioc_converter` | Section heading detection |\n",
    "\n",
    "### Built-in Processing:\n",
    "\n",
    "The PyMuPDF-based functions automatically handle:\n",
    "- âœ… **Header/footer removal** - detects repeating text at top/bottom of pages\n",
    "- âœ… **Section heading detection** - Abstract, Methods, Results, Discussion, etc.\n",
    "- âœ… **Table region avoidance** - prevents duplicating text from tables\n",
    "- âœ… **Small passage merging** - combines blocks until min_words threshold\n",
    "- âœ… **XML-safe text cleaning** - removes illegal XML characters\n",
    "\n",
    "### Output:\n",
    "```\n",
    "./output/\n",
    "â”œâ”€â”€ ingestion/    # PDF files\n",
    "â””â”€â”€ bioc_xml/     # BioC XML output\n",
    "```\n",
    "\n",
    "**Requirement:** PyMuPDF must be installed (`pip install pymupdf`) - included in requirements.txt."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pdf_local_ingestor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
