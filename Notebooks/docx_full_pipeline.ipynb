{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "075df7a1-8f43-432f-96b0-3a57702dde83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DOCX Full Pipeline - Ingestion to RAG Chunks\n",
    "\n",
    "This notebook runs the **complete pipeline** for processing a DOCX document locally, **using existing Apollo project functions**.\n",
    "\n",
    "**Key Functions Used (same as Apollo production pipeline):**\n",
    "- `PandocProcessor` - DOCX â†’ HTML conversion (from `pandoc_processor.py`)\n",
    "- `process_tables()` - Table extraction (from `apollo_tables_processor.py`)\n",
    "- `html_to_bioc_collection()` - HTML â†’ BioC XML (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `clean_bioc_collection()` - BioC cleaning (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `remove_toc_passages()` - TOC removal (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `merge_small_passages_in_collection()` - Passage merging (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `chunk_annotated_articles()` - Chunking (from `chunks_handler.py`)\n",
    "- `merge_annotations()` - Annotation merging (from `merge_handler.py`)\n",
    "- `load_embeddings_model()` - Model loading (from `embeddings_generator.py`)\n",
    "- `get_embeddings()` - Embedding generation (from `embeddings_generator.py`)\n",
    "- `calculate_similarity()` - Cosine similarity (from `embeddings_generator.py`)\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Convert** - DOCX â†’ HTML using Pandoc\n",
    "2. **Extract Tables** - Process and extract table data\n",
    "3. **BioC** - Convert HTML to BioC XML format\n",
    "4. **Chunk** - Split into chunks using `chunk_annotated_articles()`\n",
    "5. **Embed** - Generate vector embeddings using `get_embeddings()`\n",
    "\n",
    "**No PostgreSQL** - **No S3** - **Fully local**\n",
    "\n",
    "**Note:** This skips the annotation step (which requires external tools like PubTator). The chunks will not have biomedical entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79e4ca2-06cd-4de1-a851-41883642c38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fe2b74-6bc1-441f-af64-c994e543baaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "                \"chunks_path\": \"./output/chunks\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                },\n",
    "                \"embeddings_model\": {\n",
    "                    \"pubmedbert\": {\n",
    "                        \"model_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                        \"token_limit\": 512\n",
    "                    },\n",
    "                    \"chemberta\": {\n",
    "                        \"model_path\": \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "                        \"token_limit\": 512\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# Mock db.py to prevent database connection\n",
    "from types import ModuleType\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Mock FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    _handlers = {\"local\": LocalFileHandler, \"test\": LocalFileHandler, \"s3\": LocalFileHandler}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bfd454-5559-41b7-ba45-6ddd4249ad12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES - Apollo DOCX Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# Apollo DOCX processing functions\n",
    "from src.data_ingestion.ingestion_utils.pandoc_processor import PandocProcessor\n",
    "from src.data_ingestion.ingest_apollo.ingest_docx.apollo_docx_to_bioc_converter import (\n",
    "    html_to_bioc_collection,\n",
    "    clean_bioc_collection,\n",
    "    remove_toc_passages,\n",
    "    merge_small_passages_in_collection,\n",
    ")\n",
    "from src.data_ingestion.ingest_apollo.ingest_docx.apollo_tables_processor import (\n",
    "    process_tables,\n",
    ")\n",
    "\n",
    "import bioc\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  Apollo DOCX pipeline modules loaded:\")\n",
    "print(\"  - PandocProcessor (DOCX â†’ HTML)\")\n",
    "print(\"  - html_to_bioc_collection (HTML â†’ BioC)\")\n",
    "print(\"  - clean_bioc_collection (BioC cleaning)\")\n",
    "print(\"  - remove_toc_passages (TOC removal)\")\n",
    "print(\"  - merge_small_passages_in_collection (passage merging)\")\n",
    "print(\"  - process_tables (table extraction)\")\n",
    "print(\"  - Ready for chunking and embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "608224e4-21d3-4146-9ff6-b1dc8f96cec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input DOCX and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe981417-f242-4169-a2d6-9984d9e02f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input DOCX file path - CHANGE THIS to your DOCX file\n",
    "DOCX_INPUT_PATH = \"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/file-sample_1MB.docx\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "INTERIM_PATH = OUTPUT_BASE_DIR / \"interim\"\n",
    "FAILED_PATH = OUTPUT_BASE_DIR / \"failed\"\n",
    "BIOC_PATH = OUTPUT_BASE_DIR / \"bioc_xml\"\n",
    "CHUNKS_PATH = OUTPUT_BASE_DIR / \"chunks\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "\n",
    "# Get DOCX name without extension\n",
    "docx_name = Path(DOCX_INPUT_PATH).stem\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, INTERIM_PATH, FAILED_PATH, BIOC_PATH, CHUNKS_PATH, EMBEDDINGS_PATH, METADATA_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create document-specific interim directory (same structure as Apollo pipeline)\n",
    "DOCX_INTERIM_DIR = INTERIM_PATH / docx_name\n",
    "DOCX_INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ DOCX to process: {docx_name}\")\n",
    "print(f\"âœ“ Interim directory: {DOCX_INTERIM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31593551-6901-4476-beb2-09285b6f7735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Convert DOCX to HTML using Pandoc\n",
    "\n",
    "Using **PandocProcessor** from the Apollo pipeline to convert DOCX to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2e87bd-dbd7-4902-a8e8-967e0dec86da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify DOCX file exists and copy to ingestion directory\n",
    "docx_source_path = Path(DOCX_INPUT_PATH).resolve()\n",
    "\n",
    "if not docx_source_path.exists():\n",
    "    raise FileNotFoundError(f\"DOCX not found: {docx_source_path}\")\n",
    "\n",
    "docx_content = file_handler.read_file_bytes(str(docx_source_path))\n",
    "docx_dest_path = INGESTION_PATH / f\"{docx_name}.docx\"\n",
    "file_handler.write_file(str(docx_dest_path), docx_content)\n",
    "\n",
    "print(f\"âœ“ DOCX: {docx_source_path}\")\n",
    "print(f\"âœ“ Size: {len(docx_content):,} bytes\")\n",
    "print(f\"âœ“ Copied to: {docx_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d03f650-18b7-4d15-a68a-5d7de043c300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd50471d-a6f3-4169-b90a-66f65e17a2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert DOCX to HTML using PandocProcessor (same as Apollo pipeline)\n",
    "# This is the core conversion step that Apollo's convert_apollo_to_html() uses internally\n",
    "\n",
    "pandoc_processor = PandocProcessor(pandoc_executable=\"pandoc\")\n",
    "\n",
    "# Define output HTML path (Apollo structure: interim/{docx_name}/{docx_name}.html)\n",
    "html_output_path = DOCX_INTERIM_DIR / f\"{docx_name}.html\"\n",
    "\n",
    "print(f\"Converting DOCX to HTML using PandocProcessor...\")\n",
    "print(f\"  Input: {docx_dest_path}\")\n",
    "print(f\"  Output: {html_output_path}\")\n",
    "\n",
    "pandoc_processor.convert(\n",
    "    input_path=docx_dest_path,\n",
    "    output_path=html_output_path,\n",
    "    input_format=\"docx\",\n",
    "    output_format=\"html\",\n",
    "    failed_ingestion_path=str(FAILED_PATH),\n",
    "    extract_media_dir=DOCX_INTERIM_DIR,  # Extract embedded images to interim dir\n",
    ")\n",
    "\n",
    "if html_output_path.exists():\n",
    "    html_size = html_output_path.stat().st_size\n",
    "    print(f\"âœ“ Conversion successful!\")\n",
    "    print(f\"âœ“ HTML size: {html_size:,} bytes\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Conversion failed - HTML not created: {html_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef7b6b0-2906-4741-afd5-6e6f6e49bb3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Extract Tables from HTML (Optional)\n",
    "\n",
    "Using **process_tables()** from the Apollo pipeline to extract table data.\n",
    "\n",
    "This step:\n",
    "- Extracts each table as an Excel file\n",
    "- Generates clean and context representations for each table\n",
    "- Removes or replaces tables in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a7d678-e40f-4724-b0ef-9eccea04a541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract tables from HTML using Apollo's process_tables()\n",
    "# This is optional but matches the production pipeline behavior\n",
    "\n",
    "EXTRACT_TABLES = True  # Set to False to skip table extraction\n",
    "TABLE_STATE = \"remove\"  # \"remove\" removes tables from HTML, \"replace\" keeps flattened text\n",
    "\n",
    "if EXTRACT_TABLES:\n",
    "    print(f\"Extracting tables from HTML using process_tables()...\")\n",
    "    print(f\"  Table state: {TABLE_STATE}\")\n",
    "    \n",
    "    # Read HTML content\n",
    "    html_content = file_handler.read_file(str(html_output_path))\n",
    "    \n",
    "    # Create empty metadata file for process_tables (it expects this)\n",
    "    metadata_file = METADATA_PATH / f\"{docx_name}.json\"\n",
    "    if not metadata_file.exists():\n",
    "        file_handler.write_file(str(metadata_file), json.dumps({\"title\": docx_name, \"source\": \"local\"}))\n",
    "    \n",
    "    # Process tables\n",
    "    html_with_tables_processed, table_details = process_tables(\n",
    "        html_str=html_content,\n",
    "        source_filename=f\"{docx_name}.html\",\n",
    "        output_tables_path=str(DOCX_INTERIM_DIR),\n",
    "        article_metadata_path=str(METADATA_PATH),\n",
    "        table_state=TABLE_STATE,\n",
    "    )\n",
    "    \n",
    "    # Write back modified HTML\n",
    "    file_handler.write_file(str(html_output_path), html_with_tables_processed)\n",
    "    \n",
    "    # Save table details\n",
    "    if table_details:\n",
    "        tables_json_path = EMBEDDINGS_PATH / f\"{docx_name}_tables.json\"\n",
    "        file_handler.write_file_as_json(str(tables_json_path), table_details)\n",
    "        print(f\"âœ“ Extracted {len(table_details)} tables\")\n",
    "        print(f\"âœ“ Table details saved to: {tables_json_path}\")\n",
    "    else:\n",
    "        print(f\"âœ“ No tables found in document\")\n",
    "else:\n",
    "    print(\"â­ Table extraction skipped (EXTRACT_TABLES=False)\")\n",
    "    html_content = file_handler.read_file(str(html_output_path))\n",
    "    html_with_tables_processed = html_content\n",
    "    table_details = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43be0f9-5c90-4127-ad05-2fd4d35ea82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Convert HTML to BioC XML\n",
    "\n",
    "Using Apollo's BioC conversion functions:\n",
    "- `html_to_bioc_collection()` - Convert HTML to BioC\n",
    "- `clean_bioc_collection()` - Clean and normalize text\n",
    "- `remove_toc_passages()` - Remove table of contents\n",
    "- `merge_small_passages_in_collection()` - Merge small passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a05350-2832-4cfd-8bec-021cb36cff5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert HTML to BioC using Apollo's functions\n",
    "print(f\"Converting HTML to BioC XML using Apollo functions...\")\n",
    "\n",
    "# Document metadata\n",
    "metadata_fields = {\n",
    "    \"source\": \"local_docx\",\n",
    "    \"filename\": docx_name,\n",
    "    \"title\": docx_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(docx_source_path),\n",
    "}\n",
    "\n",
    "# Step 1: HTML to BioC collection\n",
    "print(\"  1. html_to_bioc_collection()...\")\n",
    "bioc_collection = html_to_bioc_collection(\n",
    "    html_content=html_with_tables_processed,\n",
    "    doc_id=docx_name,\n",
    "    source=\"Internal Documents\",\n",
    "    metadata_fields=metadata_fields,\n",
    "    debug_verify=True,\n",
    ")\n",
    "\n",
    "# Count passages before processing\n",
    "initial_passage_count = sum(len(doc.passages) for doc in bioc_collection.documents)\n",
    "print(f\"     Initial passages: {initial_passage_count}\")\n",
    "\n",
    "# Step 2: Clean BioC collection\n",
    "print(\"  2. clean_bioc_collection()...\")\n",
    "bioc_collection = clean_bioc_collection(\n",
    "    collection=bioc_collection,\n",
    "    preserve_original=False,\n",
    "    clean_infons=True,\n",
    ")\n",
    "\n",
    "# Step 3: Remove TOC-like passages\n",
    "print(\"  3. remove_toc_passages()...\")\n",
    "bioc_collection, removed_toc_passages = remove_toc_passages(bioc_collection)\n",
    "if removed_toc_passages:\n",
    "    print(f\"     Removed {len(removed_toc_passages)} TOC passages\")\n",
    "\n",
    "# Step 4: Merge small passages\n",
    "print(\"  4. merge_small_passages_in_collection()...\")\n",
    "bioc_collection = merge_small_passages_in_collection(\n",
    "    collection=bioc_collection,\n",
    "    threshold_words=100,\n",
    "    max_iterations=5,\n",
    "    prefer_merge_with_next=True,\n",
    ")\n",
    "\n",
    "# Count final passages\n",
    "final_passage_count = sum(len(doc.passages) for doc in bioc_collection.documents)\n",
    "print(f\"     Final passages: {final_passage_count}\")\n",
    "\n",
    "print(\"âœ“ BioC conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3f448b-0bca-44eb-a78a-2233a89153ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save BioC XML\n",
    "bioc_xml_path = BIOC_PATH / f\"{docx_name}.xml\"\n",
    "\n",
    "with open(bioc_xml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    bioc.dump(bioc_collection, f)\n",
    "\n",
    "bioc_size = bioc_xml_path.stat().st_size\n",
    "print(f\"âœ“ Saved BioC XML: {bioc_xml_path}\")\n",
    "print(f\"âœ“ Size: {bioc_size:,} bytes\")\n",
    "\n",
    "# Preview passages\n",
    "print(f\"\\nðŸ“„ Passage preview:\")\n",
    "for doc in bioc_collection.documents:\n",
    "    for i, passage in enumerate(doc.passages[:3]):\n",
    "        section = passage.infons.get('section_title', 'N/A')\n",
    "        text_preview = passage.text[:150] + \"...\" if len(passage.text) > 150 else passage.text\n",
    "        print(f\"   [{i+1}] Section: {section}\")\n",
    "        print(f\"       Text: {text_preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e112b859-c3d0-432d-a05f-2688d19ff5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Chunk the Document\n",
    "\n",
    "Using `chunk_annotated_articles()` from `chunks_handler.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148f0d77-fe27-4078-9edb-f2088bc222ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHUNKING - Using project's chunk_annotated_articles()\n",
    "# ============================================================================\n",
    "\n",
    "from src.data_processing.chunking.chunks_handler import chunk_annotated_articles\n",
    "from src.data_processing.merging.merge_handler import merge_annotations\n",
    "\n",
    "print(\"âœ“ Imported chunk_annotated_articles from chunks_handler.py\")\n",
    "print(\"âœ“ Imported merge_annotations from merge_handler.py\")\n",
    "print(\"  Supported chunker types: sliding_window, passage, annotation_aware, grouped_annotation_aware_sliding_window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0160dc8e-c605-4bd2-bf9a-630b2385298e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chunk the BioC XML using project's chunk_annotated_articles()\n",
    "# Chunker types: 'sliding_window', 'passage', 'annotation_aware', 'grouped_annotation_aware_sliding_window'\n",
    "\n",
    "CHUNKER_TYPE = \"sliding_window\"  # Same default as production pipeline\n",
    "WINDOW_SIZE = 512  # Window size in words\n",
    "\n",
    "print(f\"Chunking {bioc_xml_path} using chunk_annotated_articles()...\")\n",
    "print(f\"  Chunker type: {CHUNKER_TYPE}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE}\")\n",
    "\n",
    "chunks = chunk_annotated_articles(\n",
    "    file_handler=file_handler,\n",
    "    input_file_path=str(bioc_xml_path),\n",
    "    chunker_type=CHUNKER_TYPE,\n",
    "    window_size=WINDOW_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created {len(chunks)} chunks\")\n",
    "\n",
    "# Save chunks\n",
    "chunks_output_path = CHUNKS_PATH / f\"{docx_name}_chunks.json\"\n",
    "with open(chunks_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ“ Saved chunks to: {chunks_output_path}\")\n",
    "\n",
    "# Show statistics\n",
    "if chunks:\n",
    "    word_counts = [len(c['text'].split()) for c in chunks]\n",
    "    print(f\"\\nðŸ“Š Chunk Statistics:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print(f\"   Avg words/chunk: {sum(word_counts) // len(word_counts)}\")\n",
    "    print(f\"   Min/Max words: {min(word_counts)} / {max(word_counts)}\")\n",
    "    \n",
    "    # Preview first chunk\n",
    "    print(f\"\\nðŸ“„ First chunk preview:\")\n",
    "    print(f\"   Text: {chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c71c86e9-2830-4b3b-b368-493c8a9033f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Generate Embeddings\n",
    "\n",
    "Using `get_embeddings()` from `embeddings_generator.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d410957e-4eec-4db4-84dd-02557aa67976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION - Download model from S3 and load locally\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from src.pubtator_utils.embeddings_handler.embeddings_generator import (\n",
    "    get_embeddings,\n",
    "    calculate_similarity,\n",
    ")\n",
    "\n",
    "local_dir = \"src/models/pubmedbert-base-embeddings\"\n",
    "\n",
    "def download_model_from_s3_uri(\n",
    "    s3_uri: str = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\",\n",
    "    local_dir: str = \"src/models/pubmedbert-base-embeddings\",\n",
    ") -> None:\n",
    "    parsed = urlparse(s3_uri)\n",
    "    if parsed.scheme != \"s3\":\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Ensure base folder exists\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading model from {s3_uri} â†’ {local_dir}\")\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    found_objects = False\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            found_objects = True\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            # Skip directory markers\n",
    "            if key.endswith(\"/\") or key == prefix:\n",
    "                continue\n",
    "\n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "            print(f\"Downloaded {key}\")\n",
    "\n",
    "    if not found_objects:\n",
    "        print(f\"No objects found at {s3_uri}\")\n",
    "\n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "download_model_from_s3_uri()\n",
    "\n",
    "def load_embeddings_model():\n",
    "    # Must match the download location exactly\n",
    "    model_dir = os.path.abspath(\"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "    print(f\"Loading embeddings model from {model_dir}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"Embeddings model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_embeddings_model()\n",
    "MODEL_NAME = \"pubmedbert\"  # Used by get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd5cc38-3694-430b-b0df-e3bcd18b1a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks using project's get_embeddings()\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks using get_embeddings()...\")\n",
    "\n",
    "# Get texts from chunks (merge_annotations if annotations exist)\n",
    "chunk_texts = []\n",
    "for chunk in chunks:\n",
    "    text = chunk['text']\n",
    "    annotations = chunk.get('annotations', [])\n",
    "    if annotations:\n",
    "        # Use project's merge_annotations to combine text with annotations\n",
    "        merged_text = merge_annotations(text=text, annotations=annotations, merger_type=\"prepend\")\n",
    "        chunk_texts.append(merged_text)\n",
    "    else:\n",
    "        chunk_texts.append(text)\n",
    "\n",
    "# Generate embeddings using project's function\n",
    "embeddings = get_embeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    texts=chunk_texts,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated embeddings for {len(chunk_texts)} chunks\")\n",
    "print(f\"âœ“ Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5064d137-6c0b-4a07-8ed3-4f66d2dae1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create chunks with embeddings in the same format as production pipeline\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    chunk_sequence = i + 1\n",
    "    \n",
    "    chunks_with_embeddings.append({\n",
    "        'chunk_sequence': str(chunk_sequence),\n",
    "        'merged_text': chunk_texts[i],  # Text used for embedding (may include annotations)\n",
    "        'payload': {\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_processing_date': datetime.now().date().isoformat(),\n",
    "            'chunk_name': f\"{docx_name}_chunk_{chunk_sequence}\",\n",
    "            'chunk_text': chunk['text'],\n",
    "            'chunk_length': len(chunk['text']),\n",
    "            'token_count': len(chunk['text'].split()),\n",
    "            'chunk_annotations_count': len(chunk.get('annotations', [])),\n",
    "            'article_id': docx_name,\n",
    "            'source': 'local_docx',\n",
    "            'chunk_type': 'article_chunk',\n",
    "            'processing_ts': datetime.now().isoformat(),\n",
    "            'section_title': chunk.get('section_title', []),\n",
    "            'offset': chunk.get('offset', 0),\n",
    "        },\n",
    "        'embeddings': embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else list(embeddings[i]),\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{docx_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"âœ“ Format matches production pipeline output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487894ff-4342-460e-a74d-62b48c55fcc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Test Semantic Search (RAG Demo)\n",
    "\n",
    "Demonstrate semantic search using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbdb23d-5ec5-4bd1-9e47-ea43b18acb1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def semantic_search(query: str, chunks_with_embeddings: list, model_name: str, model, tokenizer, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using project's get_embeddings() and calculate_similarity().\n",
    "    \"\"\"\n",
    "    # Get query embedding using project's function\n",
    "    query_embedding = get_embeddings(\n",
    "        model_name=model_name,\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Get chunk embeddings\n",
    "    chunk_embeddings = [c['embeddings'] for c in chunks_with_embeddings]\n",
    "    \n",
    "    # Calculate similarities using project's function\n",
    "    similarities = calculate_similarity(query_embedding[0].numpy(), chunk_embeddings)\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks_with_embeddings[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'rank': len(results) + 1,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Semantic search function defined (using project's calculate_similarity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938106ac-583a-4af3-8434-5f1f8d0ed53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Semantic search\n",
    "# Modify this query based on your DOCX content\n",
    "QUERY = \"clinical trial results\"  # Change this to match your document content\n",
    "\n",
    "print(f\"ðŸ” Query: '{QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = semantic_search(\n",
    "    query=QUERY, \n",
    "    chunks_with_embeddings=chunks_with_embeddings, \n",
    "    model_name=MODEL_NAME,\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    chunk = result['chunk']\n",
    "    similarity = result['similarity']\n",
    "    rank = result['rank']\n",
    "    chunk_text = chunk['payload']['chunk_text']\n",
    "    preview = chunk_text[:300] + \"...\" if len(chunk_text) > 300 else chunk_text\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Result #{rank} (similarity: {similarity:.4f})\")\n",
    "    print(f\"   Chunk: {chunk['payload']['chunk_name']}\")\n",
    "    print(f\"   Words: {chunk['payload']['token_count']}\")\n",
    "    print(f\"   Text: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff3b9066-9cba-4637-8359-3cddb51e5867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d58ee4-06a9-4172-9a32-deb9684dac11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"Generated files:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_BASE_DIR):\n",
    "    level = root.replace(str(OUTPUT_BASE_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder = os.path.basename(root)\n",
    "    if files:\n",
    "        print(f\"{indent}{folder}/\")\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size = file_path.stat().st_size\n",
    "            size_str = f\"{size:,} bytes\" if size < 1024*1024 else f\"{size/1024/1024:.2f} MB\"\n",
    "            print(f\"{indent}  {file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9cb5ba0-e0fc-47fb-aecf-3e639a02f83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "This notebook executed the **complete DOCX-to-RAG pipeline** using **Apollo project functions**:\n",
    "\n",
    "| Step | Project Function | Source File |\n",
    "|------|------------------|-------------|\n",
    "| 1. Convert | `PandocProcessor.convert()` | `pandoc_processor.py` |\n",
    "| 2. Tables | `process_tables()` | `apollo_tables_processor.py` |\n",
    "| 3. BioC | `html_to_bioc_collection()` | `apollo_docx_to_bioc_converter.py` |\n",
    "| 4. Clean | `clean_bioc_collection()` | `apollo_docx_to_bioc_converter.py` |\n",
    "| 5. TOC | `remove_toc_passages()` | `apollo_docx_to_bioc_converter.py` |\n",
    "| 6. Merge | `merge_small_passages_in_collection()` | `apollo_docx_to_bioc_converter.py` |\n",
    "| 7. Chunk | `chunk_annotated_articles()` | `chunks_handler.py` |\n",
    "| 8. Embed | `get_embeddings()` | `embeddings_generator.py` |\n",
    "| 9. Search | `calculate_similarity()` | `embeddings_generator.py` |\n",
    "\n",
    "### Output Structure\n",
    "```\n",
    "./output/\n",
    "â”œâ”€â”€ ingestion/         # Original DOCX\n",
    "â”œâ”€â”€ interim/           # HTML + extracted media (Pandoc output)\n",
    "â”œâ”€â”€ bioc_xml/          # BioC XML (structured passages)\n",
    "â”œâ”€â”€ chunks/            # Chunk JSON (same format as production)\n",
    "â”œâ”€â”€ embeddings/        # Chunks + 768-dim vectors + extracted tables\n",
    "â””â”€â”€ metadata/          # Document metadata\n",
    "```\n",
    "\n",
    "### Output Format (matches production pipeline):\n",
    "The `embeddings/{docx_name}_embeddings.json` file uses the **same format as `orchestrator.py`**:\n",
    "- `chunk_sequence` - Chunk order\n",
    "- `merged_text` - Text used for embedding (may include annotations)\n",
    "- `payload.chunk_id` - Unique identifier\n",
    "- `payload.chunk_text` - The raw chunk content\n",
    "- `embeddings` - 768-dimensional PubMedBERT vector\n",
    "\n",
    "Load this into a vector database (Pinecone, Weaviate, OpenSearch, etc.) for semantic search!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8075789507507163,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "docx_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
