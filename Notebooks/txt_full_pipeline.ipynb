{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4d15bb-c692-4f00-b9e4-b724a593584f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# TXT Full Pipeline - Ingestion to RAG Chunks\n",
    "\n",
    "This notebook runs the **complete pipeline** for processing a TXT (plain text) document locally, **using existing project functions**.\n",
    "\n",
    "**Key Functions Used (same as other pipelines):**\n",
    "- `html_to_bioc_collection()` - Text â†’ BioC XML (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `clean_bioc_collection()` - BioC cleaning (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `merge_small_passages_in_collection()` - Passage merging (from `apollo_docx_to_bioc_converter.py`)\n",
    "- `chunk_annotated_articles()` - Chunking (from `chunks_handler.py`)\n",
    "- `merge_annotations()` - Annotation merging (from `merge_handler.py`)\n",
    "- `get_embeddings()` - Embedding generation (from `embeddings_generator.py`)\n",
    "- `calculate_similarity()` - Cosine similarity (from `embeddings_generator.py`)\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Read** - Load TXT file directly (no conversion needed)\n",
    "2. **Parse** - Split into paragraphs and wrap in minimal HTML\n",
    "3. **BioC** - Convert to BioC XML format for consistency\n",
    "4. **Chunk** - Split into chunks using `chunk_annotated_articles()`\n",
    "5. **Embed** - Generate vector embeddings using `get_embeddings()`\n",
    "\n",
    "**No PostgreSQL** - **No S3** - **Fully local**\n",
    "\n",
    "**Note:** TXT files are the simplest format - just read, parse paragraphs, and process. No Pandoc or table extraction needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9255518c-302f-4db5-a17a-f45e05d0446f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "298f80fe-66ef-42df-9872-16f6b282a438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "                \"chunks_path\": \"./output/chunks\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                },\n",
    "                \"embeddings_model\": {\n",
    "                    \"pubmedbert\": {\n",
    "                        \"model_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                        \"token_limit\": 512\n",
    "                    },\n",
    "                    \"chemberta\": {\n",
    "                        \"model_path\": \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "                        \"token_limit\": 512\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# Mock db.py to prevent database connection\n",
    "from types import ModuleType\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Mock FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    _handlers = {\"local\": LocalFileHandler, \"test\": LocalFileHandler, \"s3\": LocalFileHandler}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7c63c5-99e9-4b82-9d05-a6fdcf455ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES - TXT Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# Reuse DOCX BioC conversion functions (they work on HTML, which we'll generate from TXT)\n",
    "from src.data_ingestion.ingest_apollo.ingest_docx.apollo_docx_to_bioc_converter import (\n",
    "    html_to_bioc_collection,\n",
    "    clean_bioc_collection,\n",
    "    merge_small_passages_in_collection,\n",
    ")\n",
    "\n",
    "import bioc\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  TXT pipeline modules loaded:\")\n",
    "print(\"  - html_to_bioc_collection (Text â†’ BioC)\")\n",
    "print(\"  - clean_bioc_collection (BioC cleaning)\")\n",
    "print(\"  - merge_small_passages_in_collection (passage merging)\")\n",
    "print(\"  - Ready for chunking and embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe15529b-6bbe-477a-a0c8-529339f09b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input TXT file and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db091770-4204-4a45-9419-48faed968923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input TXT file path - CHANGE THIS to your TXT file\n",
    "TXT_INPUT_PATH = \"./sample_data/sample_document.txt\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"./output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "INTERIM_PATH = OUTPUT_BASE_DIR / \"interim\"\n",
    "FAILED_PATH = OUTPUT_BASE_DIR / \"failed\"\n",
    "BIOC_PATH = OUTPUT_BASE_DIR / \"bioc_xml\"\n",
    "CHUNKS_PATH = OUTPUT_BASE_DIR / \"chunks\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "\n",
    "# Get TXT name without extension\n",
    "txt_name = Path(TXT_INPUT_PATH).stem\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, INTERIM_PATH, FAILED_PATH, BIOC_PATH, CHUNKS_PATH, EMBEDDINGS_PATH, METADATA_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create document-specific interim directory\n",
    "TXT_INTERIM_DIR = INTERIM_PATH / txt_name\n",
    "TXT_INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ TXT to process: {txt_name}\")\n",
    "print(f\"âœ“ Interim directory: {TXT_INTERIM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17c2194b-2710-4b20-9731-75e8fbc3cb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read TXT File\n",
    "\n",
    "TXT files are the simplest - just read the content directly. No conversion needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26356a1a-dbf5-4e70-a9b3-230c4a82b851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify TXT file exists and copy to ingestion directory\n",
    "txt_source_path = Path(TXT_INPUT_PATH).resolve()\n",
    "\n",
    "if not txt_source_path.exists():\n",
    "    raise FileNotFoundError(f\"TXT not found: {txt_source_path}\")\n",
    "\n",
    "txt_content = file_handler.read_file(str(txt_source_path))\n",
    "txt_dest_path = INGESTION_PATH / f\"{txt_name}.txt\"\n",
    "file_handler.write_file(str(txt_dest_path), txt_content)\n",
    "\n",
    "print(f\"âœ“ TXT: {txt_source_path}\")\n",
    "print(f\"âœ“ Size: {len(txt_content):,} characters\")\n",
    "print(f\"âœ“ Copied to: {txt_dest_path}\")\n",
    "\n",
    "# Preview content\n",
    "preview = txt_content[:500] + \"...\" if len(txt_content) > 500 else txt_content\n",
    "print(f\"\\nðŸ“„ Content preview:\")\n",
    "print(f\"   {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddad9799-a75c-4663-bf2e-393850e57d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Parse TXT into Paragraphs\n",
    "\n",
    "Split the text into paragraphs based on blank lines, then wrap in minimal HTML for BioC conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d62b7818-fdb9-4e6e-9e64-93a5d7e8c91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARSE TXT INTO PARAGRAPHS AND GENERATE HTML\n",
    "# ============================================================================\n",
    "\n",
    "def parse_txt_to_paragraphs(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split TXT content into paragraphs based on blank lines.\n",
    "    Returns list of non-empty paragraph strings.\n",
    "    \"\"\"\n",
    "    # Split on one or more blank lines\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    # Clean and filter empty paragraphs\n",
    "    cleaned = []\n",
    "    for p in paragraphs:\n",
    "        # Normalize whitespace within paragraph\n",
    "        p = ' '.join(p.split())\n",
    "        if p.strip():\n",
    "            cleaned.append(p.strip())\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def paragraphs_to_html(paragraphs: list, title: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Convert list of paragraphs to minimal HTML for BioC conversion.\n",
    "    \"\"\"\n",
    "    html_parts = [\"<!DOCTYPE html>\", \"<html>\", \"<head>\"]\n",
    "    if title:\n",
    "        html_parts.append(f\"<title>{title}</title>\")\n",
    "    html_parts.append(\"</head>\")\n",
    "    html_parts.append(\"<body>\")\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        # Escape HTML special characters\n",
    "        p_escaped = p.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
    "        html_parts.append(f\"<p>{p_escaped}</p>\")\n",
    "    \n",
    "    html_parts.append(\"</body>\")\n",
    "    html_parts.append(\"</html>\")\n",
    "    \n",
    "    return \"\\n\".join(html_parts)\n",
    "\n",
    "# Parse TXT into paragraphs\n",
    "paragraphs = parse_txt_to_paragraphs(txt_content)\n",
    "print(f\"âœ“ Parsed {len(paragraphs)} paragraphs from TXT\")\n",
    "\n",
    "# Show paragraph statistics\n",
    "word_counts = [len(p.split()) for p in paragraphs]\n",
    "print(f\"\\nðŸ“Š Paragraph Statistics:\")\n",
    "print(f\"   Total paragraphs: {len(paragraphs)}\")\n",
    "print(f\"   Total words: {sum(word_counts):,}\")\n",
    "print(f\"   Avg words/paragraph: {sum(word_counts) // max(len(word_counts), 1)}\")\n",
    "print(f\"   Min/Max words: {min(word_counts)} / {max(word_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e460f6b8-c4f3-4827-8007-52f1cf2b4c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert paragraphs to HTML for BioC processing\n",
    "html_content = paragraphs_to_html(paragraphs, title=txt_name.replace(\"_\", \" \").title())\n",
    "\n",
    "# Save HTML to interim directory\n",
    "html_output_path = TXT_INTERIM_DIR / f\"{txt_name}.html\"\n",
    "file_handler.write_file(str(html_output_path), html_content)\n",
    "\n",
    "print(f\"âœ“ Generated HTML from paragraphs\")\n",
    "print(f\"âœ“ HTML size: {len(html_content):,} characters\")\n",
    "print(f\"âœ“ Saved to: {html_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a140d9bc-64fc-44a3-8d44-735102db5f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Convert to BioC XML\n",
    "\n",
    "Using Apollo's BioC conversion functions for consistency with other pipelines:\n",
    "- `html_to_bioc_collection()` - Convert HTML to BioC\n",
    "- `clean_bioc_collection()` - Clean and normalize text\n",
    "- `merge_small_passages_in_collection()` - Merge small passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b13acbb-06a6-4ba0-bccb-b15bd8cb05e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert HTML to BioC using Apollo's functions\n",
    "print(f\"Converting HTML to BioC XML using Apollo functions...\")\n",
    "\n",
    "# Document metadata\n",
    "metadata_fields = {\n",
    "    \"source\": \"local_txt\",\n",
    "    \"filename\": txt_name,\n",
    "    \"title\": txt_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(txt_source_path),\n",
    "}\n",
    "\n",
    "# Step 1: HTML to BioC collection\n",
    "print(\"  1. html_to_bioc_collection()...\")\n",
    "bioc_collection = html_to_bioc_collection(\n",
    "    html_content=html_content,\n",
    "    doc_id=txt_name,\n",
    "    source=\"Plain Text Documents\",\n",
    "    metadata_fields=metadata_fields,\n",
    "    debug_verify=True,\n",
    ")\n",
    "\n",
    "# Count passages before processing\n",
    "initial_passage_count = sum(len(doc.passages) for doc in bioc_collection.documents)\n",
    "print(f\"     Initial passages: {initial_passage_count}\")\n",
    "\n",
    "# Step 2: Clean BioC collection\n",
    "print(\"  2. clean_bioc_collection()...\")\n",
    "bioc_collection = clean_bioc_collection(\n",
    "    collection=bioc_collection,\n",
    "    preserve_original=False,\n",
    "    clean_infons=True,\n",
    ")\n",
    "\n",
    "# Step 3: Merge small passages (TXT may have short paragraphs)\n",
    "print(\"  3. merge_small_passages_in_collection()...\")\n",
    "bioc_collection = merge_small_passages_in_collection(\n",
    "    collection=bioc_collection,\n",
    "    threshold_words=100,\n",
    "    max_iterations=5,\n",
    "    prefer_merge_with_next=True,\n",
    ")\n",
    "\n",
    "# Count final passages\n",
    "final_passage_count = sum(len(doc.passages) for doc in bioc_collection.documents)\n",
    "print(f\"     Final passages: {final_passage_count}\")\n",
    "\n",
    "print(\"âœ“ BioC conversion complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7087b43-1e6a-4f45-b21f-1ff8d1b307c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save BioC XML\n",
    "bioc_xml_path = BIOC_PATH / f\"{txt_name}.xml\"\n",
    "\n",
    "with open(bioc_xml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    bioc.dump(bioc_collection, f)\n",
    "\n",
    "bioc_size = bioc_xml_path.stat().st_size\n",
    "print(f\"âœ“ Saved BioC XML: {bioc_xml_path}\")\n",
    "print(f\"âœ“ Size: {bioc_size:,} bytes\")\n",
    "\n",
    "# Preview passages\n",
    "print(f\"\\nðŸ“„ Passage preview:\")\n",
    "for doc in bioc_collection.documents:\n",
    "    for i, passage in enumerate(doc.passages[:3]):\n",
    "        section = passage.infons.get('section_title', 'N/A')\n",
    "        text_preview = passage.text[:150] + \"...\" if len(passage.text) > 150 else passage.text\n",
    "        print(f\"   [{i+1}] Section: {section}\")\n",
    "        print(f\"       Text: {text_preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cf33d1c-d86a-47a4-9cd4-58e046328d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Chunk the Document\n",
    "\n",
    "Using `chunk_annotated_articles()` from `chunks_handler.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04f4c602-0b03-4463-9552-ad97c6351a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHUNKING - Using project's chunk_annotated_articles()\n",
    "# ============================================================================\n",
    "\n",
    "from src.data_processing.chunking.chunks_handler import chunk_annotated_articles\n",
    "from src.data_processing.merging.merge_handler import merge_annotations\n",
    "\n",
    "print(\"âœ“ Imported chunk_annotated_articles from chunks_handler.py\")\n",
    "print(\"âœ“ Imported merge_annotations from merge_handler.py\")\n",
    "print(\"  Supported chunker types: sliding_window, passage, annotation_aware, grouped_annotation_aware_sliding_window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2037a02c-70eb-46d0-bc0e-2e29879d0a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chunk the BioC XML using project's chunk_annotated_articles()\n",
    "# Chunker types: 'sliding_window', 'passage', 'annotation_aware', 'grouped_annotation_aware_sliding_window'\n",
    "\n",
    "CHUNKER_TYPE = \"sliding_window\"  # Same default as production pipeline\n",
    "WINDOW_SIZE = 512  # Window size in words\n",
    "\n",
    "print(f\"Chunking {bioc_xml_path} using chunk_annotated_articles()...\")\n",
    "print(f\"  Chunker type: {CHUNKER_TYPE}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE}\")\n",
    "\n",
    "chunks = chunk_annotated_articles(\n",
    "    file_handler=file_handler,\n",
    "    input_file_path=str(bioc_xml_path),\n",
    "    chunker_type=CHUNKER_TYPE,\n",
    "    window_size=WINDOW_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created {len(chunks)} chunks\")\n",
    "\n",
    "# Save chunks\n",
    "chunks_output_path = CHUNKS_PATH / f\"{txt_name}_chunks.json\"\n",
    "with open(chunks_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ“ Saved chunks to: {chunks_output_path}\")\n",
    "\n",
    "# Show statistics\n",
    "if chunks:\n",
    "    word_counts = [len(c['text'].split()) for c in chunks]\n",
    "    print(f\"\\nðŸ“Š Chunk Statistics:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print(f\"   Avg words/chunk: {sum(word_counts) // len(word_counts)}\")\n",
    "    print(f\"   Min/Max words: {min(word_counts)} / {max(word_counts)}\")\n",
    "    \n",
    "    # Preview first chunk\n",
    "    print(f\"\\nðŸ“„ First chunk preview:\")\n",
    "    print(f\"   Text: {chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d7145a-8edf-4c66-b9fc-d99d04a5c09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Generate Embeddings\n",
    "\n",
    "Using `get_embeddings()` from `embeddings_generator.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78c7c280-c975-437b-a5b7-02c2a6a5f28c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION - Download model from S3 and load locally\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from src.pubtator_utils.embeddings_handler.embeddings_generator import (\n",
    "    get_embeddings,\n",
    "    calculate_similarity,\n",
    ")\n",
    "\n",
    "local_dir = \"src/models/pubmedbert-base-embeddings\"\n",
    "\n",
    "def download_model_from_s3_uri(\n",
    "    s3_uri: str = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\",\n",
    "    local_dir: str = \"src/models/pubmedbert-base-embeddings\",\n",
    ") -> None:\n",
    "    parsed = urlparse(s3_uri)\n",
    "    if parsed.scheme != \"s3\":\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Ensure base folder exists\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading model from {s3_uri} â†’ {local_dir}\")\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    found_objects = False\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            found_objects = True\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            # Skip directory markers\n",
    "            if key.endswith(\"/\") or key == prefix:\n",
    "                continue\n",
    "\n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "            print(f\"Downloaded {key}\")\n",
    "\n",
    "    if not found_objects:\n",
    "        print(f\"No objects found at {s3_uri}\")\n",
    "\n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "download_model_from_s3_uri()\n",
    "\n",
    "def load_embeddings_model():\n",
    "    # Must match the download location exactly\n",
    "    model_dir = os.path.abspath(\"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "    print(f\"Loading embeddings model from {model_dir}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"Embeddings model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_embeddings_model()\n",
    "MODEL_NAME = \"pubmedbert\"  # Used by get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f651e9-2cca-45bb-abe0-a112c7df9b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks using project's get_embeddings()\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks using get_embeddings()...\")\n",
    "\n",
    "# Get texts from chunks (merge_annotations if annotations exist)\n",
    "chunk_texts = []\n",
    "for chunk in chunks:\n",
    "    text = chunk['text']\n",
    "    annotations = chunk.get('annotations', [])\n",
    "    if annotations:\n",
    "        # Use project's merge_annotations to combine text with annotations\n",
    "        merged_text = merge_annotations(text=text, annotations=annotations, merger_type=\"prepend\")\n",
    "        chunk_texts.append(merged_text)\n",
    "    else:\n",
    "        chunk_texts.append(text)\n",
    "\n",
    "# Generate embeddings using project's function\n",
    "embeddings = get_embeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    texts=chunk_texts,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated embeddings for {len(chunk_texts)} chunks\")\n",
    "print(f\"âœ“ Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bcbab9b-6f22-4df5-9702-d6e6e286ed70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create chunks with embeddings in the same format as production pipeline\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    chunk_sequence = i + 1\n",
    "    \n",
    "    chunks_with_embeddings.append({\n",
    "        'chunk_sequence': str(chunk_sequence),\n",
    "        'merged_text': chunk_texts[i],  # Text used for embedding (may include annotations)\n",
    "        'payload': {\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_processing_date': datetime.now().date().isoformat(),\n",
    "            'chunk_name': f\"{txt_name}_chunk_{chunk_sequence}\",\n",
    "            'chunk_text': chunk['text'],\n",
    "            'chunk_length': len(chunk['text']),\n",
    "            'token_count': len(chunk['text'].split()),\n",
    "            'chunk_annotations_count': len(chunk.get('annotations', [])),\n",
    "            'article_id': txt_name,\n",
    "            'source': 'local_txt',\n",
    "            'chunk_type': 'article_chunk',\n",
    "            'processing_ts': datetime.now().isoformat(),\n",
    "            'section_title': chunk.get('section_title', []),\n",
    "            'offset': chunk.get('offset', 0),\n",
    "        },\n",
    "        'embeddings': embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else list(embeddings[i]),\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{txt_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"âœ“ Format matches production pipeline output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a907daa9-ffee-4dbd-b26d-12d3519577c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Test Semantic Search (RAG Demo)\n",
    "\n",
    "Demonstrate semantic search using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bff3866b-97b2-4c5e-bbfe-0d2a56df81cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def semantic_search(query: str, chunks_with_embeddings: list, model_name: str, model, tokenizer, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using project's get_embeddings() and calculate_similarity().\n",
    "    \"\"\"\n",
    "    # Get query embedding using project's function\n",
    "    query_embedding = get_embeddings(\n",
    "        model_name=model_name,\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Get chunk embeddings\n",
    "    chunk_embeddings = [c['embeddings'] for c in chunks_with_embeddings]\n",
    "    \n",
    "    # Calculate similarities using project's function\n",
    "    similarities = calculate_similarity(query_embedding[0].numpy(), chunk_embeddings)\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks_with_embeddings[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'rank': len(results) + 1,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Semantic search function defined (using project's calculate_similarity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa83c345-89dc-416f-8f5f-ed9b3d948bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Semantic search\n",
    "# Modify this query based on your TXT content\n",
    "QUERY = \"clinical trial results\"  # Change this to match your document content\n",
    "\n",
    "print(f\"ðŸ” Query: '{QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = semantic_search(\n",
    "    query=QUERY, \n",
    "    chunks_with_embeddings=chunks_with_embeddings, \n",
    "    model_name=MODEL_NAME,\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    chunk = result['chunk']\n",
    "    similarity = result['similarity']\n",
    "    rank = result['rank']\n",
    "    chunk_text = chunk['payload']['chunk_text']\n",
    "    preview = chunk_text[:300] + \"...\" if len(chunk_text) > 300 else chunk_text\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Result #{rank} (similarity: {similarity:.4f})\")\n",
    "    print(f\"   Chunk: {chunk['payload']['chunk_name']}\")\n",
    "    print(f\"   Words: {chunk['payload']['token_count']}\")\n",
    "    print(f\"   Text: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c21916-1c67-42a2-9af3-207ee56ecf98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e46d4058-29d9-410c-9c6e-b4286b266406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create chunks with embeddings in the same format as production pipeline\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    chunk_sequence = i + 1\n",
    "    \n",
    "    chunks_with_embeddings.append({\n",
    "        'chunk_sequence': str(chunk_sequence),\n",
    "        'merged_text': chunk_texts[i],  # Text used for embedding (may include annotations)\n",
    "        'payload': {\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_processing_date': datetime.now().date().isoformat(),\n",
    "            'chunk_name': f\"{txt_name}_chunk_{chunk_sequence}\",\n",
    "            'chunk_text': chunk['text'],\n",
    "            'chunk_length': len(chunk['text']),\n",
    "            'token_count': len(chunk['text'].split()),\n",
    "            'chunk_annotations_count': len(chunk.get('annotations', [])),\n",
    "            'article_id': txt_name,\n",
    "            'source': 'local_txt',\n",
    "            'chunk_type': 'article_chunk',\n",
    "            'processing_ts': datetime.now().isoformat(),\n",
    "            'section_title': chunk.get('section_title', []),\n",
    "            'offset': chunk.get('offset', 0),\n",
    "        },\n",
    "        'embeddings': embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else list(embeddings[i]),\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{txt_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"âœ“ Format matches production pipeline output\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "txt_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
