{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03699ae-1aae-490a-9cfe-55fcfe5de544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CSV Full Pipeline - Ingestion to RAG Chunks\n",
    "\n",
    "This notebook runs the **complete pipeline** for processing a CSV (Comma-Separated Values) file locally, **using existing project functions**.\n",
    "\n",
    "**Key Functions Used (same as other pipelines):**\n",
    "- `pd.read_csv()` - Read CSV file (from pandas)\n",
    "- `PandocProcessor` - CSV â†’ HTML conversion (from `pandoc_processor.py`)\n",
    "- `process_tables()` - Table extraction & processing (from `xlsx_table_processor.py`)\n",
    "- `get_embeddings()` - Embedding generation (from `embeddings_generator.py`)\n",
    "- `calculate_similarity()` - Cosine similarity (from `embeddings_generator.py`)\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Read** - Load CSV file using pandas\n",
    "2. **Convert** - Convert CSV to HTML using Pandoc\n",
    "3. **Extract** - Process table from HTML (structure, metadata, text)\n",
    "4. **Embed** - Generate vector embeddings for table content\n",
    "\n",
    "**No PostgreSQL** - **No S3** - **Fully local**\n",
    "\n",
    "**Note:** CSV files are inherently tabular data (single table), so they don't go through BioC conversion. The table becomes a chunk for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ad00cb-18ea-4cee-a9b7-63e904197a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadda959-508f-4331-9e01-6b8586c4c5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b820e91e-f06b-4dcb-9074-aac97a2eaf17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "                \"chunks_path\": \"./output/chunks\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                },\n",
    "                \"embeddings_model\": {\n",
    "                    \"pubmedbert\": {\n",
    "                        \"model_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                        \"token_limit\": 512\n",
    "                    },\n",
    "                    \"chemberta\": {\n",
    "                        \"model_path\": \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "                        \"token_limit\": 512\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# Mock db.py to prevent database connection\n",
    "from types import ModuleType\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Mock FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    _handlers = {\"local\": LocalFileHandler, \"test\": LocalFileHandler, \"s3\": LocalFileHandler}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2b8ebd-06e5-406c-99f7-6a49906b409a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES - CSV Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# Apollo processing functions (reuse XLSX table processor for CSV)\n",
    "from src.data_ingestion.ingestion_utils.pandoc_processor import PandocProcessor\n",
    "from src.data_ingestion.ingest_apollo.ingest_xlsx.xlsx_table_processor import (\n",
    "    process_tables,\n",
    "    expand_table_to_matrix,\n",
    "    generate_clean_and_context_flat,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  CSV pipeline modules loaded:\")\n",
    "print(\"  - PandocProcessor (CSV â†’ HTML)\")\n",
    "print(\"  - process_tables (table extraction)\")\n",
    "print(\"  - expand_table_to_matrix (table parsing)\")\n",
    "print(\"  - generate_clean_and_context_flat (text generation)\")\n",
    "print(\"  - Ready for embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19bbf30-f7a8-41a8-89ae-1c3c16514462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input CSV file and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c482733-b937-4a84-9c83-2496b83023e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input CSV file path - CHANGE THIS to your CSV file\n",
    "CSV_INPUT_PATH = \"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/username-password-recovery-code.csv\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "INTERIM_PATH = OUTPUT_BASE_DIR / \"interim\"\n",
    "FAILED_PATH = OUTPUT_BASE_DIR / \"failed\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "\n",
    "# Get CSV name without extension\n",
    "csv_name = Path(CSV_INPUT_PATH).stem\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, INTERIM_PATH, FAILED_PATH, EMBEDDINGS_PATH, METADATA_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create document-specific interim directory\n",
    "CSV_INTERIM_DIR = INTERIM_PATH / csv_name\n",
    "CSV_INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ CSV to process: {csv_name}\")\n",
    "print(f\"âœ“ Interim directory: {CSV_INTERIM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e309340-8d4f-4da3-b749-744098afb39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read CSV File\n",
    "\n",
    "Load the CSV file using pandas and copy to ingestion directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1716a3f4-af81-49dd-8f55-8113f2c12119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify CSV file exists and copy to ingestion directory\n",
    "csv_source_path = Path(CSV_INPUT_PATH).resolve()\n",
    "\n",
    "if not csv_source_path.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {csv_source_path}\")\n",
    "\n",
    "# Copy to ingestion directory\n",
    "csv_content = file_handler.read_file(str(csv_source_path))\n",
    "csv_dest_path = INGESTION_PATH / f\"{csv_name}.csv\"\n",
    "file_handler.write_file(str(csv_dest_path), csv_content)\n",
    "\n",
    "print(f\"âœ“ CSV: {csv_source_path}\")\n",
    "print(f\"âœ“ Size: {len(csv_content):,} characters\")\n",
    "print(f\"âœ“ Copied to: {csv_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0bcfde-407f-4590-953d-e47468a1be8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV using pandas\n",
    "print(f\"Reading CSV file: {csv_dest_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_dest_path)\n",
    "    print(f\"âœ“ Successfully read CSV\")\n",
    "    print(f\"   Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Preview data\n",
    "    print(f\"\\nðŸ“Š Data Preview:\")\n",
    "    display(df.head())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to read CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61ab420-f484-40dd-abcc-cecb4282177d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Convert CSV to HTML\n",
    "\n",
    "Using **PandocProcessor** to convert CSV to HTML for table extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57dad4fb-1d1c-46c9-8a0a-57ebef5952dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert CSV to HTML using PandocProcessor\n",
    "pandoc_processor = PandocProcessor(pandoc_executable=\"pandoc\")\n",
    "\n",
    "# Define output HTML path\n",
    "html_path = CSV_INTERIM_DIR / f\"{csv_name}.html\"\n",
    "\n",
    "print(f\"Converting CSV to HTML using PandocProcessor...\")\n",
    "print(f\"  Input: {csv_dest_path}\")\n",
    "print(f\"  Output: {html_path}\")\n",
    "\n",
    "pandoc_processor.convert(\n",
    "    input_path=csv_dest_path,\n",
    "    output_path=html_path,\n",
    "    input_format=\"csv\",\n",
    "    output_format=\"html\",\n",
    "    failed_ingestion_path=str(FAILED_PATH),\n",
    "    extract_media_dir=CSV_INTERIM_DIR,\n",
    ")\n",
    "\n",
    "if html_path.exists():\n",
    "    html_size = html_path.stat().st_size\n",
    "    print(f\"âœ“ Conversion successful!\")\n",
    "    print(f\"âœ“ HTML size: {html_size:,} bytes\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Conversion failed - HTML not created: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cef439-4e7a-4409-8bf5-0afd142615c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Extract Table from HTML\n",
    "\n",
    "Using **process_tables()** from the XLSX processor to extract table structure and generate text representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ae2a05-de11-47b8-8067-303d30f5a774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract table from HTML using process_tables()\n",
    "# This generates clean_flat_text and context_flat_text for the table\n",
    "\n",
    "# Create article metadata\n",
    "article_metadata = {\n",
    "    \"source\": \"local_csv\",\n",
    "    \"filename\": csv_name,\n",
    "    \"title\": csv_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(csv_source_path),\n",
    "}\n",
    "\n",
    "print(f\"Extracting table from HTML using process_tables()...\")\n",
    "\n",
    "# Read HTML content\n",
    "html_content = file_handler.read_file(str(html_path))\n",
    "\n",
    "# Process tables using Apollo's function\n",
    "html_with_tables, table_details = process_tables(\n",
    "    html_str=html_content,\n",
    "    source_filename=f\"{csv_name}.html\",\n",
    "    sheet_idx=1,  # CSV is a single \"sheet\"\n",
    "    output_tables_path=CSV_INTERIM_DIR,\n",
    "    article_metadata=article_metadata,\n",
    "    xlsx_filename=csv_name,\n",
    ")\n",
    "\n",
    "# Add source info to table details\n",
    "for table in table_details:\n",
    "    table['payload']['source_type'] = 'csv'\n",
    "    table['payload']['csv_name'] = csv_name\n",
    "\n",
    "print(f\"âœ“ Extracted {len(table_details)} table(s)\")\n",
    "\n",
    "# Preview table details\n",
    "if table_details:\n",
    "    for i, table in enumerate(table_details):\n",
    "        payload = table['payload']\n",
    "        print(f\"\\nðŸ“Š Table {i+1}:\")\n",
    "        print(f\"   Rows: {payload.get('row_count', 'N/A')}\")\n",
    "        print(f\"   Columns: {payload.get('col_count', 'N/A')}\")\n",
    "        clean_text = payload.get('clean_flat_text', '')[:200]\n",
    "        print(f\"   Clean text preview: {clean_text}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6d2137-3a9d-4fc3-bbed-3a65150d27d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Generate Embeddings\n",
    "\n",
    "Using `get_embeddings()` from `embeddings_generator.py` to generate embeddings for the table content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf028f9d-6ff3-4277-a22c-da3c04510eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION - Download model from S3 and load locally\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from src.pubtator_utils.embeddings_handler.embeddings_generator import (\n",
    "    get_embeddings,\n",
    "    calculate_similarity,\n",
    ")\n",
    "\n",
    "local_dir = \"src/models/pubmedbert-base-embeddings\"\n",
    "\n",
    "def download_model_from_s3_uri(\n",
    "    s3_uri: str = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\",\n",
    "    local_dir: str = \"src/models/pubmedbert-base-embeddings\",\n",
    ") -> None:\n",
    "    parsed = urlparse(s3_uri)\n",
    "    if parsed.scheme != \"s3\":\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Ensure base folder exists\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading model from {s3_uri} â†’ {local_dir}\")\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    found_objects = False\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            found_objects = True\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            # Skip directory markers\n",
    "            if key.endswith(\"/\") or key == prefix:\n",
    "                continue\n",
    "\n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "            print(f\"Downloaded {key}\")\n",
    "\n",
    "    if not found_objects:\n",
    "        print(f\"No objects found at {s3_uri}\")\n",
    "\n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "download_model_from_s3_uri()\n",
    "\n",
    "def load_embeddings_model():\n",
    "    # Must match the download location exactly\n",
    "    model_dir = os.path.abspath(\"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "    print(f\"Loading embeddings model from {model_dir}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"Embeddings model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_embeddings_model()\n",
    "MODEL_NAME = \"pubmedbert\"  # Used by get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f0b13f-0f1d-4dd5-85ac-f60a73143ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for table content using get_embeddings()\n",
    "print(f\"Generating embeddings for {len(table_details)} table(s) using get_embeddings()...\")\n",
    "\n",
    "# Get texts from table details (use clean_flat_text for embedding)\n",
    "table_texts = []\n",
    "for table in table_details:\n",
    "    # Prefer clean_flat_text, fall back to context_flat_text\n",
    "    text = table['payload'].get('clean_flat_text', '')\n",
    "    if not text:\n",
    "        text = table['payload'].get('context_flat_text', '')\n",
    "    table_texts.append(text)\n",
    "\n",
    "# Generate embeddings using project's function\n",
    "if table_texts:\n",
    "    embeddings = get_embeddings(\n",
    "        model_name=MODEL_NAME,\n",
    "        texts=table_texts,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(f\"âœ“ Generated embeddings for {len(table_texts)} table(s)\")\n",
    "    print(f\"âœ“ Embedding shape: {embeddings.shape}\")\n",
    "else:\n",
    "    print(\"âš  No table text found for embedding generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1085897-08be-45d1-8843-48ba9bc37449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create chunks with embeddings in the same format as production pipeline\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i, table in enumerate(table_details):\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    chunk_sequence = i + 1\n",
    "    payload = table['payload']\n",
    "    \n",
    "    chunks_with_embeddings.append({\n",
    "        'chunk_sequence': str(chunk_sequence),\n",
    "        'merged_text': table_texts[i],  # Text used for embedding\n",
    "        'payload': {\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_processing_date': datetime.now().date().isoformat(),\n",
    "            'chunk_name': f\"{csv_name}_table_{chunk_sequence}\",\n",
    "            'chunk_text': table_texts[i],\n",
    "            'chunk_length': len(table_texts[i]),\n",
    "            'token_count': len(table_texts[i].split()),\n",
    "            'chunk_annotations_count': 0,\n",
    "            'article_id': csv_name,\n",
    "            'source': 'local_csv',\n",
    "            'chunk_type': 'table_chunk',\n",
    "            'processing_ts': datetime.now().isoformat(),\n",
    "            # Table-specific metadata\n",
    "            'table_id': payload.get('table_id', f'table_{chunk_sequence}'),\n",
    "            'row_count': payload.get('row_count'),\n",
    "            'col_count': payload.get('col_count'),\n",
    "            'clean_flat_text': payload.get('clean_flat_text', ''),\n",
    "            'context_flat_text': payload.get('context_flat_text', ''),\n",
    "        },\n",
    "        'embeddings': embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else list(embeddings[i]),\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{csv_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"âœ“ Format matches production pipeline output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f22211-42ca-4f3f-8d66-e284666512bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Test Semantic Search (RAG Demo)\n",
    "\n",
    "Demonstrate semantic search using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4940bae5-4732-4fc2-aacd-c0cac3a4cbcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def semantic_search(query: str, chunks_with_embeddings: list, model_name: str, model, tokenizer, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using project's get_embeddings() and calculate_similarity().\n",
    "    \"\"\"\n",
    "    # Get query embedding using project's function\n",
    "    query_embedding = get_embeddings(\n",
    "        model_name=model_name,\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Get chunk embeddings\n",
    "    chunk_embeddings = [c['embeddings'] for c in chunks_with_embeddings]\n",
    "    \n",
    "    # Calculate similarities using project's function\n",
    "    similarities = calculate_similarity(query_embedding[0].numpy(), chunk_embeddings)\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks_with_embeddings[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'rank': len(results) + 1,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Semantic search function defined (using project's calculate_similarity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a326b6c8-6798-4c79-9ebb-a3764fa6a8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Semantic search\n",
    "# Modify this query based on your CSV content\n",
    "QUERY = \"patient data results\"  # Change this to match your data content\n",
    "\n",
    "print(f\"ðŸ” Query: '{QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if chunks_with_embeddings:\n",
    "    results = semantic_search(\n",
    "        query=QUERY, \n",
    "        chunks_with_embeddings=chunks_with_embeddings, \n",
    "        model_name=MODEL_NAME,\n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        top_k=min(3, len(chunks_with_embeddings))\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        chunk = result['chunk']\n",
    "        similarity = result['similarity']\n",
    "        rank = result['rank']\n",
    "        chunk_text = chunk['payload']['chunk_text']\n",
    "        preview = chunk_text[:300] + \"...\" if len(chunk_text) > 300 else chunk_text\n",
    "        \n",
    "        print(f\"\\nðŸ“„ Result #{rank} (similarity: {similarity:.4f})\")\n",
    "        print(f\"   Chunk: {chunk['payload']['chunk_name']}\")\n",
    "        print(f\"   Rows: {chunk['payload'].get('row_count', 'N/A')}\")\n",
    "        print(f\"   Columns: {chunk['payload'].get('col_count', 'N/A')}\")\n",
    "        print(f\"   Text: {preview}\")\n",
    "else:\n",
    "    print(\"âš  No chunks available for search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ea17c3-99ad-4620-b398-cbae659c6757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b3818c-db5e-4ff6-8715-b1059b57c492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"Generated files:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_BASE_DIR):\n",
    "    level = root.replace(str(OUTPUT_BASE_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder = os.path.basename(root)\n",
    "    if files:\n",
    "        print(f\"{indent}{folder}/\")\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size = file_path.stat().st_size\n",
    "            size_str = f\"{size:,} bytes\" if size < 1024*1024 else f\"{size/1024/1024:.2f} MB\"\n",
    "            print(f\"{indent}  {file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9735ddef-6caf-48ce-a611-8394b4dfb98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "This notebook executed the **complete CSV-to-RAG pipeline** using **project functions**:\n",
    "\n",
    "| Step | Function | Source File |\n",
    "|------|----------|-------------|\n",
    "| 1. Read | `pd.read_csv()` | pandas |\n",
    "| 2. Convert | `PandocProcessor.convert()` | `pandoc_processor.py` |\n",
    "| 3. Extract | `process_tables()` | `xlsx_table_processor.py` |\n",
    "| 4. Embed | `get_embeddings()` | `embeddings_generator.py` |\n",
    "| 5. Search | `calculate_similarity()` | `embeddings_generator.py` |\n",
    "\n",
    "### Output Structure\n",
    "```\n",
    "./output/\n",
    "â”œâ”€â”€ ingestion/         # Original CSV\n",
    "â”œâ”€â”€ interim/           # HTML conversion\n",
    "â”œâ”€â”€ embeddings/        # Chunks + 768-dim vectors\n",
    "â””â”€â”€ metadata/          # Document metadata\n",
    "```\n",
    "\n",
    "### Output Format (matches production pipeline):\n",
    "The `embeddings/{csv_name}_embeddings.json` file uses the **same format as other pipelines**:\n",
    "- `chunk_sequence` - Chunk order\n",
    "- `merged_text` - Text used for embedding\n",
    "- `payload.chunk_id` - Unique identifier\n",
    "- `payload.chunk_text` - The table content as text\n",
    "- `payload.row_count` / `col_count` - Table dimensions\n",
    "- `embeddings` - 768-dimensional PubMedBERT vector\n",
    "\n",
    "Load this into a vector database (Pinecone, Weaviate, OpenSearch, etc.) for semantic search!\n",
    "\n",
    "### CSV vs Other Formats\n",
    "CSV is a **simple tabular format**:\n",
    "- âœ… Single table (unlike XLSX which can have multiple sheets)\n",
    "- âœ… No complex formatting\n",
    "- âœ… Direct table extraction\n",
    "- âœ… Reuses XLSX table processor functions\n",
    "- âŒ No BioC conversion (tabular data doesn't need it)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8075789507507237,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "csv_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
