{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a817ba-aa25-48af-b244-558df3526856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PDF Full Pipeline - Ingestion to RAG Chunks\n",
    "\n",
    "This notebook runs the **complete pipeline** for processing a PDF document locally:\n",
    "\n",
    "1. **Extract** - Read PDF using PyMuPDF\n",
    "2. **Convert** - Create BioC XML format\n",
    "3. **Chunk** - Split into chunks using sliding window\n",
    "4. **Embed** - Generate vector embeddings using PubMedBERT\n",
    "\n",
    "- **Key Features:**\n",
    "- Uses **PyMuPDF** for proper PDF text extraction\n",
    "- Creates BioC XML with section detection\n",
    "- Chunks text with sliding window strategy\n",
    "- Generates embeddings ready for RAG\n",
    "- **No PostgreSQL** - **No S3** - **Fully local**\n",
    "\n",
    "**Note:** This skips the annotation step (which requires external tools like PubTator). The chunks will not have biomedical entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec43f69-3546-464f-b6c3-f1f1cb11e5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fadc57e-dd2a-4b58-bb7c-8d2d4420a662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "                \"chunks_path\": \"./output/chunks\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                },\n",
    "                \"embeddings_model\": {\n",
    "                    \"pubmedbert\": {\n",
    "                        \"model_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                        \"token_limit\": 512\n",
    "                    },\n",
    "                    \"chemberta\": {\n",
    "                        \"model_path\": \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "                        \"token_limit\": 512\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# Mock db.py to prevent database connection\n",
    "from types import ModuleType\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Mock FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    _handlers = {\"local\": LocalFileHandler, \"test\": LocalFileHandler, \"s3\": LocalFileHandler}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c86d930-c3e4-47c9-bd4a-3770024726b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES\n",
    "# ============================================================================\n",
    "\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# PDF extraction using PyMuPDF\n",
    "from src.data_ingestion.ingest_preprints_rxivs.preprint_pdf_to_bioc_converter import (\n",
    "    extract_pages_block_level_simple,\n",
    "    make_document_from_blocks,\n",
    "    build_bioc_collection_lib,\n",
    "    clean_xml_text,\n",
    ")\n",
    "\n",
    "import bioc\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  Pipeline modules loaded:\")\n",
    "print(\"  - PyMuPDF extraction (PDF â†’ text blocks)\")\n",
    "print(\"  - BioC conversion (blocks â†’ BioC XML)\")\n",
    "print(\"  - Ready for chunking and embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d734a215-0031-4212-94df-29c1441cd092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input PDF and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a454e65-743d-4b87-8f41-8b50f09fe449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input PDF file path\n",
    "PDF_INPUT_PATH = \"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/attention.pdf\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "BIOC_PATH = OUTPUT_BASE_DIR / \"bioc_xml\"\n",
    "CHUNKS_PATH = OUTPUT_BASE_DIR / \"chunks\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "\n",
    "# Get PDF name without extension\n",
    "pdf_name = Path(PDF_INPUT_PATH).stem\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, BIOC_PATH, CHUNKS_PATH, EMBEDDINGS_PATH, METADATA_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ PDF to process: {pdf_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6434580e-4624-4627-b346-14ad52990040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Extract Text from PDF using PyMuPDF\n",
    "\n",
    "Using **PyMuPDF** to properly read the PDF and extract text blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1deffea-b5e2-4d93-82a2-0fe6690e6a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read PDF and copy to ingestion directory\n",
    "pdf_source_path = Path(PDF_INPUT_PATH).resolve()\n",
    "\n",
    "if not file_handler.exists(str(pdf_source_path)):\n",
    "    raise FileNotFoundError(f\"PDF not found: {pdf_source_path}\")\n",
    "\n",
    "pdf_content = file_handler.read_file_bytes(str(pdf_source_path))\n",
    "pdf_dest_path = INGESTION_PATH / f\"{pdf_name}.pdf\"\n",
    "file_handler.write_file(str(pdf_dest_path), pdf_content)\n",
    "\n",
    "print(f\"âœ“ PDF: {pdf_source_path}\")\n",
    "print(f\"âœ“ Size: {len(pdf_content):,} bytes\")\n",
    "print(f\"âœ“ Copied to: {pdf_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6f4fad-946e-4f74-a9bb-d62f5ea2732d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract text blocks from PDF using PyMuPDF\n",
    "pdf_file_path = str(pdf_dest_path)\n",
    "\n",
    "kept_blocks_per_page = extract_pages_block_level_simple(\n",
    "    pdf_path=pdf_file_path,\n",
    "    table_thresh=0.2,\n",
    ")\n",
    "\n",
    "total_blocks = sum(len(page_blocks) for page_blocks in kept_blocks_per_page)\n",
    "total_pages = len(kept_blocks_per_page)\n",
    "\n",
    "print(f\"âœ“ Extracted {total_blocks} text blocks from {total_pages} pages\")\n",
    "\n",
    "# Show section heading distribution\n",
    "all_headings = [heading for page_blocks in kept_blocks_per_page for heading, _ in page_blocks]\n",
    "heading_counts = Counter(all_headings)\n",
    "\n",
    "print(\"\\nSection heading distribution:\")\n",
    "for heading, count in heading_counts.most_common():\n",
    "    print(f\"  {heading}: {count} block(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab96279e-5ff6-418b-b0ca-18bd66304111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Convert to BioC XML\n",
    "\n",
    "Create a structured BioC XML document with passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbc6302-1d46-4cc6-a346-ac808323b5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert extracted blocks â†’ BioC\n",
    "metadata_infons = {\n",
    "    \"source\": \"local_pdf\",\n",
    "    \"filename\": pdf_name,\n",
    "    \"title\": pdf_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(pdf_source_path),\n",
    "}\n",
    "\n",
    "doc_dict = make_document_from_blocks(\n",
    "    doc_id=pdf_name,\n",
    "    kept_blocks_per_page=kept_blocks_per_page,\n",
    "    infons=metadata_infons,\n",
    "    min_words=100,\n",
    ")\n",
    "\n",
    "bioc_collection = build_bioc_collection_lib(\n",
    "    source=\"Local PDF\",\n",
    "    date_str=datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    documents=[doc_dict],\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Document ID: {doc_dict['id']}\")\n",
    "print(f\"âœ“ Passages: {len(doc_dict['passages'])}\")\n",
    "\n",
    "# Save BioC XML\n",
    "bioc_xml_path = BIOC_PATH / f\"{pdf_name}.xml\"\n",
    "with open(bioc_xml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    bioc.dump(bioc_collection, f)\n",
    "\n",
    "print(f\"âœ“ Saved: {bioc_xml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834998fe-9ab3-4061-bcf4-11ff98f819b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Chunk the Document\n",
    "\n",
    "Split the BioC passages into smaller chunks using a **sliding window** strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17bacb67-e3ff-47ea-97f6-a58381425c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHUNKING - Sliding Window Strategy\n",
    "# ============================================================================\n",
    "\n",
    "def sliding_window_chunk(text: str, window_size: int = 512, stride: int = 256):\n",
    "    \"\"\"Split text into overlapping chunks using sliding window.\"\"\"\n",
    "    words = re.findall(r'\\S+|\\s+', text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i + window_size]\n",
    "        chunk_text = ''.join(chunk_words).strip()\n",
    "        \n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'start_word': i,\n",
    "                'end_word': min(i + window_size, len(words)),\n",
    "            })\n",
    "        \n",
    "        remaining = len(words) - (i + window_size)\n",
    "        if remaining <= stride:\n",
    "            break\n",
    "        \n",
    "        i += stride\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_bioc_collection(collection, window_size: int = 512, stride: int = 256):\n",
    "    \"\"\"Chunk all passages in a BioC collection.\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in collection.documents:\n",
    "        doc_id = doc.id\n",
    "        \n",
    "        for passage_idx, passage in enumerate(doc.passages):\n",
    "            passage_text = passage.text or \"\"\n",
    "            passage_type = passage.infons.get(\"type\", \"body_text\")\n",
    "            \n",
    "            if len(passage_text.split()) < 10:\n",
    "                continue\n",
    "            \n",
    "            passage_chunks = sliding_window_chunk(passage_text, window_size, stride)\n",
    "            \n",
    "            for chunk_idx, chunk in enumerate(passage_chunks):\n",
    "                all_chunks.append({\n",
    "                    'chunk_id': str(uuid.uuid4()),\n",
    "                    'doc_id': doc_id,\n",
    "                    'passage_idx': passage_idx,\n",
    "                    'chunk_idx': chunk_idx,\n",
    "                    'section_type': passage_type,\n",
    "                    'text': chunk['text'],\n",
    "                    'word_count': len(chunk['text'].split()),\n",
    "                    'annotations': [],\n",
    "                })\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"âœ“ Chunking functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eab6d1c-2fb1-48a9-baf8-0ae32869dafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chunk the BioC collection\n",
    "WINDOW_SIZE = 512  # ~256 actual words\n",
    "STRIDE = 256       # 50% overlap\n",
    "\n",
    "chunks = chunk_bioc_collection(bioc_collection, window_size=WINDOW_SIZE, stride=STRIDE)\n",
    "\n",
    "print(f\"âœ“ Created {len(chunks)} chunks\")\n",
    "print(f\"âœ“ Window size: {WINDOW_SIZE} words, Stride: {STRIDE} words (50% overlap)\")\n",
    "\n",
    "# Save chunks\n",
    "chunks_output_path = CHUNKS_PATH / f\"{pdf_name}_chunks.json\"\n",
    "with open(chunks_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved chunks to: {chunks_output_path}\")\n",
    "\n",
    "# Show statistics\n",
    "word_counts = [c['word_count'] for c in chunks]\n",
    "print(f\"\\nðŸ“Š Chunk Statistics:\")\n",
    "print(f\"   Avg words/chunk: {sum(word_counts) // len(word_counts)}\")\n",
    "print(f\"   Min/Max words: {min(word_counts)} / {max(word_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acec809-a748-459f-865e-e8ac9132594c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779fea96-452b-42cc-bf4e-e36780048b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "local_dir = \"src/models/pubmedbert-base-embeddings\"\n",
    "\n",
    "def download_model_from_s3_uri(\n",
    "    s3_uri: str = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\",\n",
    "    local_dir: str = \"src/models/pubmedbert-base-embeddings\",\n",
    ") -> None:\n",
    "    parsed = urlparse(s3_uri)\n",
    "    if parsed.scheme != \"s3\":\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Ensure base folder exists\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading model from {s3_uri} â†’ {local_dir}\")\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    found_objects = False\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            found_objects = True\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            # Skip directory markers\n",
    "            if key.endswith(\"/\") or key == prefix:\n",
    "                continue\n",
    "\n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "            print(f\"Downloaded {key}\")\n",
    "\n",
    "    if not found_objects:\n",
    "        print(f\"No objects found at {s3_uri}\")\n",
    "\n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "download_model_from_s3_uri()\n",
    "\n",
    "def load_embeddings_model():\n",
    "    # Must match the download location exactly\n",
    "    model_dir = os.path.abspath(\"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "    print(f\"Loading embeddings model from {model_dir}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"Embeddings model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_embeddings_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68de06a-f900-47ca-b18b-54a89ee25d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "#MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(f\"Loading PubMedBERT model...\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "#model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ“ Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae30e11e-83ab-4ae5-ab9f-73512a7b81c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding(text: str, tokenizer, model, device, max_length: int = 512):\n",
    "    \"\"\"Generate embedding for a single text using mean pooling.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    attention_mask = inputs['attention_mask']\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    sum_mask = input_mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    embeddings = F.normalize(mean_embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "def batch_get_embeddings(texts: list, tokenizer, model, device, max_length: int = 512, batch_size: int = 8):\n",
    "    \"\"\"Generate embeddings for multiple texts in batches.\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        attention_mask = inputs['attention_mask']\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = input_mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        embeddings = F.normalize(mean_embeddings, p=2, dim=1)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        print(f\"  Processed {min(i + batch_size, len(texts))}/{len(texts)} chunks\", end=\"\\r\")\n",
    "    \n",
    "    print()\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"âœ“ Embedding functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169a04d3-ff07-4a05-8457-9905eedb326d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "\n",
    "chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "embeddings = batch_get_embeddings(chunk_texts, tokenizer, model, device, MAX_LENGTH, batch_size=8)\n",
    "\n",
    "print(f\"âœ“ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"âœ“ Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7037741e-8688-4c19-9a81-8299b88d3125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create chunks with embeddings for RAG\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunks_with_embeddings.append({\n",
    "        'chunk_id': chunk['chunk_id'],\n",
    "        'doc_id': chunk['doc_id'],\n",
    "        'section_type': chunk['section_type'],\n",
    "        'text': chunk['text'],\n",
    "        'word_count': chunk['word_count'],\n",
    "        'embedding': embeddings[i].tolist(),\n",
    "        'metadata': {\n",
    "            'source': 'local_pdf',\n",
    "            'filename': pdf_name,\n",
    "            'passage_idx': chunk['passage_idx'],\n",
    "            'chunk_idx': chunk['chunk_idx'],\n",
    "            'embedding_model': 'pubmedbert',\n",
    "            'processing_date': datetime.now().isoformat(),\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{pdf_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebb6cc7-6c8c-42f2-b531-a9724024dcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Test Semantic Search (RAG Demo)\n",
    "\n",
    "Demonstrate semantic search using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3c6bd0-cb35-4f68-87a6-766ad568d00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query: str, chunks_with_embeddings: list, tokenizer, model, device, top_k: int = 5):\n",
    "    \"\"\"Perform semantic search over chunks.\"\"\"\n",
    "    query_embedding = get_embedding(query, tokenizer, model, device)\n",
    "    chunk_embeddings = np.array([c['embedding'] for c in chunks_with_embeddings])\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks_with_embeddings[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'rank': len(results) + 1,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Semantic search function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787b54d7-5aa8-4024-aeb4-c07c584bd183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Semantic search\n",
    "# Modify this query based on your PDF content\n",
    "QUERY = \"attention mechanism in transformers\"\n",
    "\n",
    "print(f\"ðŸ” Query: '{QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = semantic_search(QUERY, chunks_with_embeddings, tokenizer, model, device, top_k=3)\n",
    "\n",
    "for result in results:\n",
    "    chunk = result['chunk']\n",
    "    similarity = result['similarity']\n",
    "    rank = result['rank']\n",
    "    preview = chunk['text'][:300] + \"...\" if len(chunk['text']) > 300 else chunk['text']\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Result #{rank} (similarity: {similarity:.4f})\")\n",
    "    print(f\"   Section: {chunk['section_type']}\")\n",
    "    print(f\"   Words: {chunk['word_count']}\")\n",
    "    print(f\"   Text: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a55012-58fb-4e53-b897-ff48c9b7761e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c458eb-5cda-4dd1-9227-ad8d70c7450f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"Generated files:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_BASE_DIR):\n",
    "    level = root.replace(str(OUTPUT_BASE_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder = os.path.basename(root)\n",
    "    if files:\n",
    "        print(f\"{indent}{folder}/\")\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size = file_path.stat().st_size\n",
    "            size_str = f\"{size:,} bytes\" if size < 1024*1024 else f\"{size/1024/1024:.2f} MB\"\n",
    "            print(f\"{indent}  {file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60aeb48b-b2e9-49c3-a371-061d4e5f10fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "This notebook executed the **complete PDF-to-RAG pipeline**:\n",
    "\n",
    "| Step | Module | Output |\n",
    "|------|--------|--------|\n",
    "| 1. Extract | PyMuPDF | Text blocks with sections |\n",
    "| 2. Convert | BioC | `bioc_xml/{pdf_name}.xml` |\n",
    "| 3. Chunk | Sliding Window | `chunks/{pdf_name}_chunks.json` |\n",
    "| 4. Embed | PubMedBERT | `embeddings/{pdf_name}_embeddings.json` |\n",
    "\n",
    "### Output Structure\n",
    "```\n",
    "./output/\n",
    "â”œâ”€â”€ ingestion/         # Original PDF\n",
    "â”œâ”€â”€ bioc_xml/          # BioC XML (structured passages)\n",
    "â”œâ”€â”€ chunks/            # Chunk JSON (text only)\n",
    "â””â”€â”€ embeddings/        # Chunks + 768-dim vectors (for RAG)\n",
    "```\n",
    "\n",
    "### What's NOT included (requires external tools):\n",
    "- âŒ **Biomedical Entity Annotation** - Would add Gene, Disease, Chemical tags\n",
    "- âŒ **Summarization** - Would add article summaries to chunks\n",
    "\n",
    "### Using the Output for RAG:\n",
    "The `embeddings/{pdf_name}_embeddings.json` file contains everything needed:\n",
    "- `chunk_id` - Unique identifier\n",
    "- `text` - The chunk content\n",
    "- `embedding` - 768-dimensional PubMedBERT vector\n",
    "- `metadata` - Source info, section type, etc.\n",
    "\n",
    "Load this into a vector database (Pinecone, Weaviate, OpenSearch, etc.) for semantic search!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pdf_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
