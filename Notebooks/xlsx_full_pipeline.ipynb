{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fedc52b-2643-4414-87e9-2e3b4ab9a4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# XLSX Full Pipeline - Ingestion to RAG Chunks\n",
    "\n",
    "This notebook runs the **complete pipeline** for processing an XLSX (Excel) document locally, **using existing Apollo project functions**.\n",
    "\n",
    "**Key Functions Used (same as Apollo production pipeline):**\n",
    "- `pd.read_excel()` - Read XLSX sheets (from pandas)\n",
    "- `PandocProcessor` - CSV â†’ HTML conversion (from `pandoc_processor.py`)\n",
    "- `process_tables()` - Table extraction & processing (from `xlsx_table_processor.py`)\n",
    "- `load_embeddings_model()` - Model loading (from `embeddings_generator.py`)\n",
    "- `get_embeddings()` - Embedding generation (from `embeddings_generator.py`)\n",
    "- `calculate_similarity()` - Cosine similarity (from `embeddings_generator.py`)\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Read** - Load XLSX file and extract all sheets\n",
    "2. **Convert** - Convert each sheet to CSV â†’ HTML\n",
    "3. **Extract** - Process tables from HTML (structure, metadata, text)\n",
    "4. **Embed** - Generate vector embeddings for table content\n",
    "\n",
    "**No PostgreSQL** - **No S3** - **Fully local**\n",
    "\n",
    "**Note:** XLSX files are inherently tabular data, so they don't go through BioC conversion. Each sheet/table becomes a chunk for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71222520-5dfa-4217-b4a6-5154faf4e8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdec4f6-70e8-4e81-af19-a305f89245a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y pandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac01cb3-e2e1-43ca-a171-63963c20b8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "                \"chunks_path\": \"./output/chunks\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                },\n",
    "                \"embeddings_model\": {\n",
    "                    \"pubmedbert\": {\n",
    "                        \"model_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                        \"token_limit\": 512\n",
    "                    },\n",
    "                    \"chemberta\": {\n",
    "                        \"model_path\": \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "                        \"token_limit\": 512\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# Mock db.py to prevent database connection\n",
    "from types import ModuleType\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Mock FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    _handlers = {\"local\": LocalFileHandler, \"test\": LocalFileHandler, \"s3\": LocalFileHandler}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c80fd15-10a2-43b3-bf40-60c2423508f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES - Apollo XLSX Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# Apollo XLSX processing functions\n",
    "from src.data_ingestion.ingestion_utils.pandoc_processor import PandocProcessor\n",
    "from src.data_ingestion.ingest_apollo.ingest_xlsx.xlsx_table_processor import (\n",
    "    process_tables,\n",
    "    expand_table_to_matrix,\n",
    "    generate_clean_and_context_flat,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  Apollo XLSX pipeline modules loaded:\")\n",
    "print(\"  - PandocProcessor (CSV â†’ HTML)\")\n",
    "print(\"  - process_tables (table extraction)\")\n",
    "print(\"  - expand_table_to_matrix (table parsing)\")\n",
    "print(\"  - generate_clean_and_context_flat (text generation)\")\n",
    "print(\"  - Ready for embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bfc0dc5-3de4-425b-aa24-1154e22b2b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input XLSX and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502443af-77f9-467b-81c6-e90dc73a1e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input XLSX file path - CHANGE THIS to your XLSX file\n",
    "XLSX_INPUT_PATH = \"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/sample1.xlsx\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "INTERIM_PATH = OUTPUT_BASE_DIR / \"interim\"\n",
    "FAILED_PATH = OUTPUT_BASE_DIR / \"failed\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "\n",
    "# Get XLSX name without extension\n",
    "xlsx_name = Path(XLSX_INPUT_PATH).stem\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, INTERIM_PATH, FAILED_PATH, EMBEDDINGS_PATH, METADATA_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create document-specific interim directory (same structure as Apollo pipeline)\n",
    "XLSX_INTERIM_DIR = INTERIM_PATH / xlsx_name\n",
    "XLSX_INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ XLSX to process: {xlsx_name}\")\n",
    "print(f\"âœ“ Interim directory: {XLSX_INTERIM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cb16ff-7543-4435-9d6e-91b4337089d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read XLSX and Extract Sheets\n",
    "\n",
    "Using **pandas** to read the XLSX file and extract all sheets (same as Apollo pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16a8462-7882-400b-94d9-d649d87be8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify XLSX file exists and copy to ingestion directory\n",
    "xlsx_source_path = Path(XLSX_INPUT_PATH).resolve()\n",
    "\n",
    "if not xlsx_source_path.exists():\n",
    "    raise FileNotFoundError(f\"XLSX not found: {xlsx_source_path}\")\n",
    "\n",
    "xlsx_content = file_handler.read_file_bytes(str(xlsx_source_path))\n",
    "xlsx_dest_path = INGESTION_PATH / f\"{xlsx_name}.xlsx\"\n",
    "file_handler.write_file(str(xlsx_dest_path), xlsx_content)\n",
    "\n",
    "print(f\"âœ“ XLSX: {xlsx_source_path}\")\n",
    "print(f\"âœ“ Size: {len(xlsx_content):,} bytes\")\n",
    "print(f\"âœ“ Copied to: {xlsx_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7fbba0-3521-4715-b3ed-a9569137db0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all sheets from XLSX using pandas (same as Apollo pipeline)\n",
    "print(f\"Reading XLSX file: {xlsx_dest_path}\")\n",
    "\n",
    "try:\n",
    "    all_sheets = pd.read_excel(xlsx_dest_path, sheet_name=None, engine='openpyxl')\n",
    "    print(f\"âœ“ Successfully read XLSX with {len(all_sheets)} sheet(s)\")\n",
    "    \n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        print(f\"   - Sheet: '{sheet_name}' ({df.shape[0]} rows Ã— {df.shape[1]} columns)\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to read XLSX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d334a15-d8e1-490f-a4e7-63107c770345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Convert Sheets to CSV â†’ HTML\n",
    "\n",
    "Using **PandocProcessor** from the Apollo pipeline to convert each sheet to HTML (via CSV intermediate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fc7b80-66ba-4160-9a51-4eb63ace2478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to make safe filenames (from Apollo pipeline)\n",
    "def make_safe_filename(filename: str, max_len: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Produce a safe filename by replacing every non-alphanumeric character with '_'.\n",
    "    \"\"\"\n",
    "    safe_stem = re.sub(r\"[^A-Za-z0-9]\", \"_\", filename)\n",
    "    safe_stem = re.sub(r\"_+\", \"_\", safe_stem).strip(\"_\")\n",
    "    if max_len and len(safe_stem) > max_len:\n",
    "        safe_stem = safe_stem[:max_len]\n",
    "    if not safe_stem:\n",
    "        safe_stem = \"file\"\n",
    "    return safe_stem\n",
    "\n",
    "print(\"âœ“ Helper function make_safe_filename() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73560e87-0b81-45c4-abda-ff55bd2ac35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert each sheet to CSV then HTML (same as Apollo pipeline)\n",
    "pandoc_processor = PandocProcessor(pandoc_executable=\"pandoc\")\n",
    "\n",
    "sheet_html_paths = {}  # Store paths to generated HTML files\n",
    "\n",
    "print(f\"Converting {len(all_sheets)} sheet(s) to HTML...\")\n",
    "\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    safe_sheet_name = make_safe_filename(sheet_name)\n",
    "    \n",
    "    # Create sheet-specific directory\n",
    "    sheet_dir = XLSX_INTERIM_DIR / safe_sheet_name\n",
    "    sheet_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Save as CSV\n",
    "    csv_path = sheet_dir / f\"{safe_sheet_name}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"  â†’ Sheet '{sheet_name}' saved as CSV: {csv_path}\")\n",
    "    \n",
    "    # Step 2: Convert CSV to HTML using Pandoc\n",
    "    html_path = sheet_dir / f\"{safe_sheet_name}.html\"\n",
    "    \n",
    "    pandoc_processor.convert(\n",
    "        input_path=csv_path,\n",
    "        output_path=html_path,\n",
    "        input_format=\"csv\",\n",
    "        output_format=\"html\",\n",
    "        failed_ingestion_path=str(FAILED_PATH),\n",
    "        extract_media_dir=sheet_dir,\n",
    "    )\n",
    "    \n",
    "    if html_path.exists():\n",
    "        sheet_html_paths[sheet_name] = {\n",
    "            'safe_name': safe_sheet_name,\n",
    "            'html_path': html_path,\n",
    "            'csv_path': csv_path,\n",
    "            'sheet_dir': sheet_dir,\n",
    "            'dataframe': df,\n",
    "        }\n",
    "        print(f\"  âœ“ Converted to HTML: {html_path}\")\n",
    "    else:\n",
    "        print(f\"  âœ— Failed to convert: {sheet_name}\")\n",
    "\n",
    "print(f\"\\nâœ“ Converted {len(sheet_html_paths)} sheet(s) to HTML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128435e5-9e86-450e-9262-113c180e5c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Extract Tables from HTML\n",
    "\n",
    "Using **process_tables()** from the Apollo XLSX pipeline to extract structured table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8fb89a-2cef-4c0b-85c9-731681100d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract tables from each sheet's HTML using Apollo's process_tables()\n",
    "# This generates clean_flat_text and context_flat_text for each table\n",
    "\n",
    "all_table_details = []\n",
    "sheet_idx = 0\n",
    "\n",
    "# Create basic article metadata\n",
    "article_metadata = {\n",
    "    \"source\": \"local_xlsx\",\n",
    "    \"filename\": xlsx_name,\n",
    "    \"title\": xlsx_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(xlsx_source_path),\n",
    "}\n",
    "\n",
    "print(f\"Extracting tables from {len(sheet_html_paths)} sheet(s)...\")\n",
    "\n",
    "for sheet_name, sheet_info in sheet_html_paths.items():\n",
    "    sheet_idx += 1\n",
    "    html_path = sheet_info['html_path']\n",
    "    safe_name = sheet_info['safe_name']\n",
    "    sheet_dir = sheet_info['sheet_dir']\n",
    "    \n",
    "    print(f\"\\n  Processing sheet '{sheet_name}'...\")\n",
    "    \n",
    "    # Read HTML content\n",
    "    html_content = file_handler.read_file(str(html_path))\n",
    "    \n",
    "    # Process tables using Apollo's function\n",
    "    html_with_tables, table_details = process_tables(\n",
    "        html_str=html_content,\n",
    "        source_filename=f\"{safe_name}.html\",\n",
    "        sheet_idx=sheet_idx,\n",
    "        output_tables_path=sheet_dir,\n",
    "        article_metadata=article_metadata,\n",
    "        xlsx_filename=xlsx_name,\n",
    "    )\n",
    "    \n",
    "    # Add sheet name to each table detail\n",
    "    for table in table_details:\n",
    "        table['payload']['sheet_name'] = sheet_name\n",
    "        table['payload']['safe_sheet_name'] = safe_name\n",
    "    \n",
    "    all_table_details.extend(table_details)\n",
    "    print(f\"  âœ“ Extracted {len(table_details)} table(s) from sheet '{sheet_name}'\")\n",
    "\n",
    "print(f\"\\nâœ“ Total tables extracted: {len(all_table_details)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33414569-465c-4457-bbff-8d7c8cea3afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save extracted table details\n",
    "tables_json_path = EMBEDDINGS_PATH / f\"{xlsx_name}_tables.json\"\n",
    "file_handler.write_file_as_json(str(tables_json_path), all_table_details)\n",
    "\n",
    "print(f\"âœ“ Saved table details to: {tables_json_path}\")\n",
    "print(f\"âœ“ Total table chunks: {len(all_table_details)}\")\n",
    "\n",
    "# Preview first table\n",
    "if all_table_details:\n",
    "    first_table = all_table_details[0]['payload']\n",
    "    print(f\"\\nðŸ“„ First table preview:\")\n",
    "    print(f\"   Sheet: {first_table.get('sheet_name', 'N/A')}\")\n",
    "    print(f\"   Table ID: {first_table.get('article_table_id', 'N/A')}\")\n",
    "    print(f\"   Rows: {first_table.get('row_count', 0)}, Columns: {first_table.get('column_count', 0)}\")\n",
    "    clean_text = first_table.get('clean_flat_text', '')\n",
    "    print(f\"   Clean text: {clean_text[:200]}...\" if len(clean_text) > 200 else f\"   Clean text: {clean_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8721196e-6350-486a-99bf-e5233d136c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Generate Embeddings\n",
    "\n",
    "Using `get_embeddings()` from `embeddings_generator.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a66dae-04f8-4027-b7a6-700701a4cc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION - Download model from S3 and load locally\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from src.pubtator_utils.embeddings_handler.embeddings_generator import (\n",
    "    get_embeddings,\n",
    "    calculate_similarity,\n",
    ")\n",
    "\n",
    "local_dir = \"src/models/pubmedbert-base-embeddings\"\n",
    "\n",
    "def download_model_from_s3_uri(\n",
    "    s3_uri: str = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\",\n",
    "    local_dir: str = \"src/models/pubmedbert-base-embeddings\",\n",
    ") -> None:\n",
    "    parsed = urlparse(s3_uri)\n",
    "    if parsed.scheme != \"s3\":\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Ensure base folder exists\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading model from {s3_uri} â†’ {local_dir}\")\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    found_objects = False\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            found_objects = True\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            # Skip directory markers\n",
    "            if key.endswith(\"/\") or key == prefix:\n",
    "                continue\n",
    "\n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "            print(f\"Downloaded {key}\")\n",
    "\n",
    "    if not found_objects:\n",
    "        print(f\"No objects found at {s3_uri}\")\n",
    "\n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "download_model_from_s3_uri()\n",
    "\n",
    "def load_embeddings_model():\n",
    "    # Must match the download location exactly\n",
    "    model_dir = os.path.abspath(\"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "    print(f\"Loading embeddings model from {model_dir}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"Embeddings model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_embeddings_model()\n",
    "MODEL_NAME = \"pubmedbert\"  # Used by get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c10b3fa-cabb-46ec-b3d0-0e300198e4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all table chunks using project's get_embeddings()\n",
    "print(f\"Generating embeddings for {len(all_table_details)} table chunks using get_embeddings()...\")\n",
    "\n",
    "# Get texts from table chunks (use clean_flat_text for embedding)\n",
    "table_texts = []\n",
    "for table in all_table_details:\n",
    "    # Use clean_flat_text which is optimized for embedding\n",
    "    clean_text = table['payload'].get('clean_flat_text', '')\n",
    "    if not clean_text:\n",
    "        # Fallback to context_flat_text\n",
    "        clean_text = table['payload'].get('context_flat_text', '')\n",
    "    table_texts.append(clean_text)\n",
    "\n",
    "# Generate embeddings using project's function\n",
    "if table_texts:\n",
    "    embeddings = get_embeddings(\n",
    "        model_name=MODEL_NAME,\n",
    "        texts=table_texts,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(f\"âœ“ Generated embeddings for {len(table_texts)} table chunks\")\n",
    "    print(f\"âœ“ Embedding shape: {embeddings.shape}\")\n",
    "else:\n",
    "    print(\"âš  No table texts to embed\")\n",
    "    embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55d5b98-2fde-4d8b-a35a-f448a5049d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create table chunks with embeddings in the same format as production pipeline\n",
    "tables_with_embeddings = []\n",
    "\n",
    "for i, table in enumerate(all_table_details):\n",
    "    payload = table['payload']\n",
    "    \n",
    "    tables_with_embeddings.append({\n",
    "        'chunk_sequence': str(i + 1),\n",
    "        'merged_text': table_texts[i],  # Text used for embedding\n",
    "        'payload': {\n",
    "            **payload,\n",
    "            'chunk_id': payload.get('table_id', str(uuid.uuid4())),\n",
    "            'chunk_name': f\"{xlsx_name}_{payload.get('safe_sheet_name', 'sheet')}_{payload.get('article_table_id', f'table_{i}')}\",\n",
    "            'chunk_text': payload.get('clean_flat_text', ''),\n",
    "            'chunk_length': len(payload.get('clean_flat_text', '')),\n",
    "            'token_count': len(payload.get('clean_flat_text', '').split()),\n",
    "            'source': 'local_xlsx',\n",
    "            'chunk_type': 'table_chunk',\n",
    "        },\n",
    "        'embeddings': embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else list(embeddings[i]),\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{xlsx_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tables_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"âœ“ Format matches production pipeline output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37f5ef0-f92e-4014-8de6-9cfb99000439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Test Semantic Search (RAG Demo)\n",
    "\n",
    "Demonstrate semantic search over table data using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505711d1-d1a1-4556-825a-73808eeee726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def semantic_search(query: str, tables_with_embeddings: list, model_name: str, model, tokenizer, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using project's get_embeddings() and calculate_similarity().\n",
    "    \"\"\"\n",
    "    # Get query embedding using project's function\n",
    "    query_embedding = get_embeddings(\n",
    "        model_name=model_name,\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Get table embeddings\n",
    "    table_embeddings = [t['embeddings'] for t in tables_with_embeddings]\n",
    "    \n",
    "    # Calculate similarities using project's function\n",
    "    similarities = calculate_similarity(query_embedding[0].numpy(), table_embeddings)\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'table': tables_with_embeddings[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'rank': len(results) + 1,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Semantic search function defined (using project's calculate_similarity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b69fc13-2b83-4ee3-8fdb-e5948f8ef8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Semantic search over table data\n",
    "# Modify this query based on your XLSX content\n",
    "QUERY = \"patient demographics\"  # Change this to match your spreadsheet content\n",
    "\n",
    "print(f\"ðŸ” Query: '{QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if tables_with_embeddings:\n",
    "    results = semantic_search(\n",
    "        query=QUERY, \n",
    "        tables_with_embeddings=tables_with_embeddings, \n",
    "        model_name=MODEL_NAME,\n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        table = result['table']\n",
    "        similarity = result['similarity']\n",
    "        rank = result['rank']\n",
    "        payload = table['payload']\n",
    "        \n",
    "        sheet_name = payload.get('sheet_name', 'N/A')\n",
    "        table_id = payload.get('article_table_id', 'N/A')\n",
    "        rows = payload.get('row_count', 0)\n",
    "        cols = payload.get('column_count', 0)\n",
    "        clean_text = payload.get('clean_flat_text', '')\n",
    "        preview = clean_text[:300] + \"...\" if len(clean_text) > 300 else clean_text\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Result #{rank} (similarity: {similarity:.4f})\")\n",
    "        print(f\"   Sheet: {sheet_name}\")\n",
    "        print(f\"   Table: {table_id} ({rows} rows Ã— {cols} cols)\")\n",
    "        print(f\"   Text: {preview}\")\n",
    "else:\n",
    "    print(\"âš  No tables to search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bef2031-e973-4482-b942-7f6bd7c929db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418ac166-d2b7-405e-8895-ae84abdcb750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"Generated files:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_BASE_DIR):\n",
    "    level = root.replace(str(OUTPUT_BASE_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder = os.path.basename(root)\n",
    "    if files:\n",
    "        print(f\"{indent}{folder}/\")\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size = file_path.stat().st_size\n",
    "            size_str = f\"{size:,} bytes\" if size < 1024*1024 else f\"{size/1024/1024:.2f} MB\"\n",
    "            print(f\"{indent}  {file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6f6f94-44f8-484e-893b-bcac89b92e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "This notebook executed the **complete XLSX-to-RAG pipeline** using **Apollo project functions**:\n",
    "\n",
    "| Step | Project Function | Source File |\n",
    "|------|------------------|-------------|\n",
    "| 1. Read | `pd.read_excel()` | pandas |\n",
    "| 2. Convert | `PandocProcessor.convert()` | `pandoc_processor.py` |\n",
    "| 3. Extract | `process_tables()` | `xlsx_table_processor.py` |\n",
    "| 4. Flatten | `generate_clean_and_context_flat()` | `xlsx_table_processor.py` |\n",
    "| 5. Embed | `get_embeddings()` | `embeddings_generator.py` |\n",
    "| 6. Search | `calculate_similarity()` | `embeddings_generator.py` |\n",
    "\n",
    "### Output Structure\n",
    "```\n",
    "./output/\n",
    "â”œâ”€â”€ ingestion/         # Original XLSX\n",
    "â”œâ”€â”€ interim/           # CSV + HTML per sheet\n",
    "â”‚   â””â”€â”€ {xlsx_name}/\n",
    "â”‚       â””â”€â”€ {sheet_name}/\n",
    "â”‚           â”œâ”€â”€ {sheet_name}.csv\n",
    "â”‚           â”œâ”€â”€ {sheet_name}.html\n",
    "â”‚           â””â”€â”€ Extracted_Table_*.xlsx\n",
    "â”œâ”€â”€ embeddings/        # Table chunks + 768-dim vectors\n",
    "â”‚   â”œâ”€â”€ {xlsx_name}_tables.json\n",
    "â”‚   â””â”€â”€ {xlsx_name}_embeddings.json\n",
    "â””â”€â”€ metadata/          # Document metadata\n",
    "```\n",
    "\n",
    "### Key Differences from PDF/DOCX Pipelines:\n",
    "- **No BioC conversion** - XLSX data is already structured (tabular)\n",
    "- **Sheet-level processing** - Each sheet is processed independently\n",
    "- **Table-centric chunks** - Each table becomes a chunk for embedding\n",
    "- **clean_flat_text** - Optimized text representation for semantic search\n",
    "\n",
    "### Output Format (matches production pipeline):\n",
    "The `embeddings/{xlsx_name}_embeddings.json` file uses the **same format as `orchestrator.py`**:\n",
    "- `chunk_sequence` - Chunk order\n",
    "- `merged_text` - Text used for embedding (clean_flat_text)\n",
    "- `payload.chunk_id` - Unique identifier\n",
    "- `payload.chunk_text` - The clean table content\n",
    "- `payload.sheet_name` - Source sheet name\n",
    "- `payload.row_count` / `column_count` - Table dimensions\n",
    "- `embeddings` - 768-dimensional PubMedBERT vector\n",
    "\n",
    "Load this into a vector database (Pinecone, Weaviate, OpenSearch, etc.) for semantic search!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8075789507507212,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "xlsx_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
