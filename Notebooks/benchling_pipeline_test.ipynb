{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchling Pipeline Test Notebook\n",
    "\n",
    "This notebook tests the complete Benchling ingestion pipeline:\n",
    "\n",
    "1. **Extract** - Download files from S3 bucket\n",
    "2. **Ingest** - Process files (PDF, DOCX, XLSX, PPTX, TXT)\n",
    "3. **Chunk** - Create text chunks with sliding window\n",
    "4. **Embed** - Generate embeddings with PubMedBERT\n",
    "5. **Save** - Store to Delta tables (optional) and JSON files\n",
    "\n",
    "**Key Features:**\n",
    "- Uses inline configuration (no YAML files)\n",
    "- Uses Databricks instance profile for S3 access (no AWS secrets)\n",
    "- Saves to Delta tables instead of PostgreSQL\n",
    "- Skips NER annotation steps (not applicable for Benchling)\n",
    "\n",
    "**S3 Bucket:** `gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql`  \n",
    "**S3 Prefix:** `benchling_unstructured/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements if needed\n",
    "%pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these values as needed\n",
    "# ============================================================================\n",
    "\n",
    "# S3 Configuration\n",
    "S3_BUCKET = \"gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql\"\n",
    "S3_REGION = \"us-west-2\"\n",
    "S3_PREFIX = \"benchling_unstructured/\"  # Can be more specific, e.g., \"benchling_unstructured/RD_Biovia_ELNs/\"\n",
    "\n",
    "# Delta Table Configuration (optional - set to None to disable)\n",
    "DELTA_CATALOG = \"kite_rd_dev\"  # Set to None to disable Delta writes\n",
    "DELTA_SCHEMA = \"pubtator\"\n",
    "DOCUMENTS_TABLE = \"benchling_documents\"\n",
    "CHUNKS_TABLE = \"benchling_chunks\"\n",
    "\n",
    "# Processing Configuration\n",
    "WORKFLOW_ID = f\"benchling_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "SOURCE = \"benchling\"\n",
    "FILE_TYPES = [\"pdf\", \"docx\", \"xlsx\", \"pptx\", \"txt\"]  # File types to process\n",
    "MAX_FILES = 5  # Limit files for testing (set to None for all)\n",
    "\n",
    "# Local paths for processing\n",
    "BASE_PATH = \"/tmp/benchling_processing\"  # Change for Databricks: \"/dbfs/tmp/benchling_processing\"\n",
    "\n",
    "# Embedding model path in S3\n",
    "EMBEDDINGS_MODEL = \"pubmedbert\"\n",
    "EMBEDDINGS_MODEL_S3_PATH = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\"\n",
    "\n",
    "# Whether to write to Delta tables\n",
    "WRITE_TO_DELTA = False  # Set to True in Databricks with proper catalog access\n",
    "\n",
    "print(f\"Workflow ID: {WORKFLOW_ID}\")\n",
    "print(f\"S3 Source: s3://{S3_BUCKET}/{S3_PREFIX}\")\n",
    "print(f\"File types: {FILE_TYPES}\")\n",
    "print(f\"Max files: {MAX_FILES or 'all'}\")\n",
    "print(f\"Write to Delta: {WRITE_TO_DELTA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration object\n",
    "from src.data_ingestion.ingest_benchling.benchling_config import BenchlingConfig\n",
    "\n",
    "config = BenchlingConfig.from_dict({\n",
    "    \"s3_bucket\": S3_BUCKET,\n",
    "    \"s3_region\": S3_REGION,\n",
    "    \"s3_prefix\": S3_PREFIX,\n",
    "    \"delta_catalog\": DELTA_CATALOG,\n",
    "    \"delta_schema\": DELTA_SCHEMA,\n",
    "    \"documents_table\": DOCUMENTS_TABLE,\n",
    "    \"chunks_table\": CHUNKS_TABLE,\n",
    "    \"base_path\": BASE_PATH,\n",
    "    \"allowed_file_types\": FILE_TYPES,\n",
    "    \"embeddings_model\": EMBEDDINGS_MODEL,\n",
    "    \"embeddings_model_path\": EMBEDDINGS_MODEL_S3_PATH,\n",
    "})\n",
    "\n",
    "# Get paths for this workflow\n",
    "paths = config.paths.get_paths(WORKFLOW_ID, SOURCE)\n",
    "\n",
    "print(\"\\nConfiguration created successfully!\")\n",
    "print(f\"\\nPaths for workflow '{WORKFLOW_ID}':\")\n",
    "for name, path in paths.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore S3 Bucket\n",
    "\n",
    "First, let's explore what files are available in the Benchling S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_ingestion.ingest_benchling.benchling_s3_client import BenchlingS3Client\n",
    "\n",
    "# Initialize S3 client (uses instance profile in Databricks)\n",
    "s3_client = BenchlingS3Client(\n",
    "    bucket_name=config.s3.bucket_name,\n",
    "    bucket_region=config.s3.bucket_region,\n",
    ")\n",
    "\n",
    "print(f\"Connected to bucket: {s3_client.bucket_name}\")\n",
    "print(f\"Region: {s3_client.bucket_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the source prefix\n",
    "all_files = s3_client.list_files(\n",
    "    prefix=config.s3.source_prefix,\n",
    "    file_types=FILE_TYPES,\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(all_files)} files matching file types {FILE_TYPES}\")\n",
    "print(f\"\\nFirst 10 files:\")\n",
    "for f in all_files[:10]:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "if len(all_files) > 10:\n",
    "    print(f\"  ... and {len(all_files) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze file types distribution\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "extensions = [f.split('.')[-1].lower() for f in all_files if '.' in f]\n",
    "ext_counts = Counter(extensions)\n",
    "\n",
    "print(\"\\nFile type distribution:\")\n",
    "df_types = pd.DataFrame(ext_counts.most_common(), columns=['Extension', 'Count'])\n",
    "display(df_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Files from S3\n",
    "\n",
    "Download files from S3 to local staging directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_ingestion.ingest_benchling.benchling_articles_extractor import (\n",
    "    extract_benchling_articles,\n",
    "    stable_hash,\n",
    ")\n",
    "\n",
    "# Limit files for testing\n",
    "files_to_extract = all_files[:MAX_FILES] if MAX_FILES else all_files\n",
    "\n",
    "print(f\"\\nExtracting {len(files_to_extract)} files...\")\n",
    "print(f\"Files to extract:\")\n",
    "for f in files_to_extract:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified config with limited prefix if needed\n",
    "# For testing, we'll extract files one by one to have more control\n",
    "\n",
    "from src.data_ingestion.ingest_benchling.benchling_articles_extractor import extract_single_file\n",
    "import os\n",
    "\n",
    "# Create staging directory\n",
    "staging_path = paths[\"ingestion_path\"]\n",
    "os.makedirs(staging_path, exist_ok=True)\n",
    "\n",
    "extracted_files = {}\n",
    "\n",
    "for s3_path in files_to_extract:\n",
    "    print(f\"\\nExtracting: {s3_path}\")\n",
    "    \n",
    "    try:\n",
    "        doc_id = extract_single_file(\n",
    "            config=config,\n",
    "            s3_path=s3_path,\n",
    "            local_staging_path=staging_path,\n",
    "            workflow_id=WORKFLOW_ID,\n",
    "            source=SOURCE,\n",
    "            write_to_delta=WRITE_TO_DELTA,\n",
    "        )\n",
    "        \n",
    "        if doc_id:\n",
    "            extracted_files[s3_path] = doc_id\n",
    "            print(f\"  -> Document ID: {doc_id}\")\n",
    "        else:\n",
    "            print(f\"  -> Failed to extract\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error: {e}\")\n",
    "\n",
    "print(f\"\\n\\nExtracted {len(extracted_files)} files successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Load Benchling JSON Metadata and Update Documents\n",
    "\n",
    "Load Benchling metadata from S3 JSON files and update document records with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_ingestion.ingest_benchling.benchling_metadata_loader import (\n",
    "    load_benchling_metadata_from_s3,\n",
    "    get_metadata_for_entry,\n",
    ")\n",
    "from src.data_ingestion.ingest_benchling.databricks_delta_handler import DatabricksDeltaHandler\n",
    "from typing import Optional\n",
    "import re\n",
    "\n",
    "# Load Benchling metadata from S3\n",
    "print(\"Loading Benchling metadata from S3...\")\n",
    "metadata_by_entry = load_benchling_metadata_from_s3(\n",
    "    bucket_name=S3_BUCKET,\n",
    "    base_prefix=S3_PREFIX,\n",
    ")\n",
    "\n",
    "print(f\"Loaded metadata for {len(metadata_by_entry)} entries\")\n",
    "\n",
    "# Extract entry_id from S3 paths (format: benchling_unstructured/{project}/{entry_id}/{date}/{filename})\n",
    "DATE_PATTERN = re.compile(r\"/\\d{4}-\\d{2}-\\d{2}/\")\n",
    "\n",
    "def extract_entry_id_from_path(s3_path: str) -> Optional[str]:\n",
    "    \"\"\"Extract entry_id from S3 path.\"\"\"\n",
    "    if S3_PREFIX not in s3_path:\n",
    "        return None\n",
    "    \n",
    "    rel_path = s3_path[len(S3_PREFIX):]\n",
    "    parts = rel_path.split(\"/\")\n",
    "    \n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    \n",
    "    # entry_id is typically the first part after project\n",
    "    entry_id = parts[1] if len(parts) > 1 else None\n",
    "    \n",
    "    # Verify it looks like an entry_id (usually alphanumeric)\n",
    "    if entry_id and entry_id.replace(\"-\", \"\").replace(\"_\", \"\").isalnum():\n",
    "        return entry_id\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Update documents with metadata if writing to Delta\n",
    "if WRITE_TO_DELTA and metadata_by_entry:\n",
    "    print(\"\\nUpdating documents with Benchling metadata...\")\n",
    "    \n",
    "    delta_handler = DatabricksDeltaHandler(\n",
    "        catalog=DELTA_CATALOG,\n",
    "        schema=DELTA_SCHEMA,\n",
    "        documents_table=DOCUMENTS_TABLE,\n",
    "        chunks_table=CHUNKS_TABLE,\n",
    "    )\n",
    "    \n",
    "    updated_count = 0\n",
    "    for s3_path, doc_id in extracted_files.items():\n",
    "        entry_id = extract_entry_id_from_path(s3_path)\n",
    "        \n",
    "        if entry_id and entry_id in metadata_by_entry:\n",
    "            metadata = get_metadata_for_entry(entry_id, metadata_by_entry, s3_path)\n",
    "            \n",
    "            if metadata:\n",
    "                success = delta_handler.update_document_with_metadata(\n",
    "                    document_grsar_id=doc_id,\n",
    "                    **metadata\n",
    "                )\n",
    "                if success:\n",
    "                    updated_count += 1\n",
    "                    print(f\"  Updated {doc_id[:16]}... with metadata from entry {entry_id}\")\n",
    "    \n",
    "    print(f\"\\nUpdated {updated_count} documents with Benchling metadata\")\n",
    "else:\n",
    "    print(\"\\nSkipping metadata update (WRITE_TO_DELTA=False or no metadata found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List extracted files\n",
    "extracted_local_files = os.listdir(staging_path)\n",
    "\n",
    "print(f\"\\nFiles in staging directory ({staging_path}):\")\n",
    "for f in extracted_local_files:\n",
    "    file_path = os.path.join(staging_path, f)\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  {f} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Ingest Files\n",
    "\n",
    "Process each file type (convert to BioC XML, extract tables, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_ingestion.ingest_benchling.articles_ingestor import BenchlingIngestor\n",
    "\n",
    "# Initialize the ingestor\n",
    "ingestor = BenchlingIngestor(\n",
    "    workflow_id=WORKFLOW_ID,\n",
    "    config=config,\n",
    "    file_type=\"all\",\n",
    "    source=SOURCE,\n",
    "    write_to_delta=WRITE_TO_DELTA,\n",
    ")\n",
    "\n",
    "print(\"Ingestor initialized\")\n",
    "print(f\"\\nProcessing summary before ingestion:\")\n",
    "summary = ingestor.get_processing_summary()\n",
    "for key, value in summary.items():\n",
    "    if key != 'paths':\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for file_name in extracted_local_files:\n",
    "    print(f\"\\nProcessing: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        success = ingestor.process_file(file_name)\n",
    "        if success:\n",
    "            processed_count += 1\n",
    "            print(f\"  -> Success\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"  -> Skipped or failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "        print(f\"  -> Error: {e}\")\n",
    "\n",
    "print(f\"\\n\\nIngestion complete!\")\n",
    "print(f\"  Processed: {processed_count}\")\n",
    "print(f\"  Failed: {failed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processing results\n",
    "print(\"\\nProcessing summary after ingestion:\")\n",
    "summary = ingestor.get_processing_summary()\n",
    "for key, value in summary.items():\n",
    "    if key != 'paths':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# List BioC XML files\n",
    "bioc_path = paths[\"bioc_path\"]\n",
    "if os.path.exists(bioc_path):\n",
    "    bioc_files = os.listdir(bioc_path)\n",
    "    print(f\"\\nBioC XML files ({len(bioc_files)}):\")\n",
    "    for f in bioc_files:\n",
    "        print(f\"  {f}\")\n",
    "\n",
    "# List metadata files\n",
    "metadata_path = paths[\"metadata_path\"]\n",
    "if os.path.exists(metadata_path):\n",
    "    metadata_files = os.listdir(metadata_path)\n",
    "    print(f\"\\nMetadata files ({len(metadata_files)}):\")\n",
    "    for f in metadata_files:\n",
    "        print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Chunks and Embeddings\n",
    "\n",
    "Process BioC XML files to create chunks and generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, download the embeddings model from S3 (if not already available)\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def download_model_from_s3(\n",
    "    s3_uri: str,\n",
    "    local_dir: str,\n",
    "):\n",
    "    \"\"\"Download model files from S3.\"\"\"\n",
    "    parsed = urlparse(s3_uri)\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading model from {s3_uri}...\")\n",
    "    \n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\"):\n",
    "                continue\n",
    "            \n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(local_path):\n",
    "                s3.download_file(bucket, key, local_path)\n",
    "                print(f\"  Downloaded: {relpath}\")\n",
    "    \n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "# Download model\n",
    "MODEL_LOCAL_DIR = os.path.join(str(project_root), \"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "if not os.path.exists(MODEL_LOCAL_DIR) or not os.listdir(MODEL_LOCAL_DIR):\n",
    "    download_model_from_s3(EMBEDDINGS_MODEL_S3_PATH, MODEL_LOCAL_DIR)\n",
    "else:\n",
    "    print(f\"Model already exists at {MODEL_LOCAL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.orchestrator_benchling import BenchlingArticleProcessor\n",
    "\n",
    "# Initialize the processor\n",
    "processor = BenchlingArticleProcessor(\n",
    "    workflow_id=WORKFLOW_ID,\n",
    "    config=config,\n",
    "    source=SOURCE,\n",
    "    write_to_delta=WRITE_TO_DELTA,\n",
    "    embeddings_model=EMBEDDINGS_MODEL,\n",
    "    window_size=512,  # Chunk size in words\n",
    "    stride=256,       # 50% overlap\n",
    ")\n",
    "\n",
    "print(\"Processor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files and create chunks with embeddings\n",
    "results = processor.process_all(\n",
    "    save_to_json=True,\n",
    "    save_to_delta=WRITE_TO_DELTA,\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "chunks_path = paths[\"chunks_path\"]\n",
    "embeddings_path = paths[\"embeddings_path\"]\n",
    "\n",
    "print(f\"\\nChunks files:\")\n",
    "if os.path.exists(chunks_path):\n",
    "    for f in os.listdir(chunks_path):\n",
    "        file_path = os.path.join(chunks_path, f)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  {f} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nEmbeddings files:\")\n",
    "if os.path.exists(embeddings_path):\n",
    "    for f in os.listdir(embeddings_path):\n",
    "        file_path = os.path.join(embeddings_path, f)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  {f} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect Results\n",
    "\n",
    "Let's look at the generated chunks and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect chunks\n",
    "chunks_file = os.path.join(chunks_path, f\"{WORKFLOW_ID}_all_chunks.json\")\n",
    "\n",
    "if os.path.exists(chunks_file):\n",
    "    with open(chunks_file, \"r\") as f:\n",
    "        all_chunks = json.load(f)\n",
    "    \n",
    "    print(f\"Total chunks: {len(all_chunks)}\")\n",
    "    \n",
    "    if all_chunks:\n",
    "        print(f\"\\nFirst chunk:\")\n",
    "        first_chunk = all_chunks[0]\n",
    "        for key, value in first_chunk.items():\n",
    "            if key == 'embeddings':\n",
    "                print(f\"  {key}: [{len(value)} dimensions]\")\n",
    "            elif key == 'chunk_text' or key == 'merged_text':\n",
    "                print(f\"  {key}: {value[:200]}...\" if len(str(value)) > 200 else f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"Chunks file not found: {chunks_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "if all_chunks:\n",
    "    chunk_summary = []\n",
    "    for chunk in all_chunks:\n",
    "        chunk_summary.append({\n",
    "            'chunk_id': chunk['chunk_id'][:8] + '...',\n",
    "            'document_id': chunk['document_grsar_id'][:8] + '...',\n",
    "            'sequence': chunk['chunk_sequence'],\n",
    "            'type': chunk['chunk_type'],\n",
    "            'word_count': chunk['token_count'],\n",
    "            'has_embedding': len(chunk.get('embeddings', [])) > 0,\n",
    "        })\n",
    "    \n",
    "    df_chunks = pd.DataFrame(chunk_summary)\n",
    "    print(\"\\nChunks Summary:\")\n",
    "    display(df_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Semantic Search (Optional)\n",
    "\n",
    "Demonstrate semantic search using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query: str, chunks: list, model, tokenizer, top_k: int = 3):\n",
    "    \"\"\"Perform semantic search over chunks.\"\"\"\n",
    "    from src.pubtator_utils.embeddings_handler.embeddings_generator import get_embeddings\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_embeddings(\n",
    "        model_name=\"pubmedbert\",\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )[0]\n",
    "    \n",
    "    # Get chunk embeddings\n",
    "    chunk_embeddings = np.array([c['embeddings'] for c in chunks if c.get('embeddings')])\n",
    "    \n",
    "    if len(chunk_embeddings) == 0:\n",
    "        print(\"No chunks with embeddings found\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Semantic search function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for search (if we have chunks with embeddings)\n",
    "if all_chunks and any(c.get('embeddings') for c in all_chunks):\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    \n",
    "    print(\"Loading embeddings model for search...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_DIR, local_files_only=True)\n",
    "    model = AutoModel.from_pretrained(MODEL_LOCAL_DIR, local_files_only=True)\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    # Test query - modify based on your data\n",
    "    QUERY = \"experimental results and data analysis\"\n",
    "    \n",
    "    print(f\"\\nQuery: '{QUERY}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = semantic_search(QUERY, all_chunks, model, tokenizer, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        chunk = result['chunk']\n",
    "        similarity = result['similarity']\n",
    "        \n",
    "        print(f\"\\nResult #{i} (similarity: {similarity:.4f})\")\n",
    "        print(f\"  Document: {chunk['document_grsar_id'][:16]}...\")\n",
    "        print(f\"  Type: {chunk['chunk_type']}\")\n",
    "        print(f\"  Text: {chunk['chunk_text'][:300]}...\" if len(chunk['chunk_text']) > 300 else f\"  Text: {chunk['chunk_text']}\")\n",
    "else:\n",
    "    print(\"No chunks with embeddings available for search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete Benchling ingestion pipeline:\n",
    "\n",
    "| Step | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| 1. Extract | Download files from S3 | Local staging files |\n",
    "| 2. Ingest | Process files by type | BioC XML, metadata JSON |\n",
    "| 3. Chunk | Split into chunks | Chunks JSON |\n",
    "| 4. Embed | Generate embeddings | Embeddings JSON |\n",
    "| 5. Save | Store to Delta (optional) | Delta tables |\n",
    "\n",
    "### Key Differences from Apollo Pipeline:\n",
    "- **No YAML config** - Uses inline configuration\n",
    "- **No AWS secrets** - Uses Databricks instance profile\n",
    "- **No PostgreSQL** - Uses Delta tables\n",
    "- **No NER annotation** - Skips annotation steps\n",
    "\n",
    "### Output Files:\n",
    "```\n",
    "{base_path}/{workflow_id}/benchling/\n",
    "├── ingestion/      # Original files from S3\n",
    "├── interim/        # Intermediate processing files\n",
    "├── bioc_xml/       # BioC XML documents\n",
    "├── metadata/       # Document metadata JSON\n",
    "├── chunks/         # Chunk JSON files\n",
    "├── embeddings/     # Chunks with embeddings\n",
    "└── failed/         # Failed processing files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWorkflow ID: {WORKFLOW_ID}\")\n",
    "print(f\"Source: {SOURCE}\")\n",
    "print(f\"\\nFiles:\")\n",
    "print(f\"  Extracted: {len(extracted_files)}\")\n",
    "print(f\"  Processed: {processed_count}\")\n",
    "print(f\"  Failed: {failed_count}\")\n",
    "print(f\"\\nChunks:\")\n",
    "print(f\"  Total: {len(all_chunks) if 'all_chunks' in dir() else 0}\")\n",
    "print(f\"  With embeddings: {sum(1 for c in all_chunks if c.get('embeddings')) if 'all_chunks' in dir() else 0}\")\n",
    "print(f\"\\nOutput directory: {BASE_PATH}/{WORKFLOW_ID}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
