{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb401c7-53eb-4af0-93a4-19e1aa5267b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PPTX Full Pipeline - Ingestion to RAG Chunks\n",
    "\n",
    "This notebook runs the **complete pipeline** for processing a PowerPoint (PPTX) file locally, **using existing Apollo project functions**.\n",
    "\n",
    "**Key Functions Used (from Apollo PPTX modules):**\n",
    "- `PptxProcessor` - PPTX â†’ BioC XML conversion (from `apollo_pptx_to_bioc_converter.py`)\n",
    "- `merge_small_passages_in_collection()` - Passage merging (from `apollo_pptx_to_bioc_converter.py`)\n",
    "- `PptxTableExtractor` - Table extraction (from `pptx_table_processor.py`)\n",
    "- `chunk_annotated_articles()` - Chunking (from `chunks_handler.py`)\n",
    "- `merge_annotations()` - Annotation merging (from `merge_handler.py`)\n",
    "- `get_embeddings()` - Embedding generation (from `embeddings_generator.py`)\n",
    "- `calculate_similarity()` - Cosine similarity (from `embeddings_generator.py`)\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Read** - Load PPTX file using python-pptx\n",
    "2. **Convert** - Convert to BioC XML using `PptxProcessor`\n",
    "3. **Tables** - Extract tables using `PptxTableExtractor`\n",
    "4. **Chunk** - Split into chunks using `chunk_annotated_articles()`\n",
    "5. **Embed** - Generate vector embeddings using `get_embeddings()`\n",
    "\n",
    "**No PostgreSQL** - **No S3** - **Fully local**\n",
    "\n",
    "**Note:** PPTX processing extracts text from slides (excluding table text which is handled separately) and creates one BioC passage per slide, then merges small passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3f3e55-a2d7-4cc2-9d8f-686fc2de0b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26e0815-bd10-47e3-9bc1-6e928da69e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# Add project root to path so we can import from src\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# ============================================================================\n",
    "# MOCK CONFIG AND FILE HANDLER BEFORE ANY PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "MOCK_CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"storage\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"ingestion_path\": \"./output/ingestion\",\n",
    "                \"failed_ingestion_path\": \"./output/failed\",\n",
    "                \"ingestion_interim_path\": \"./output/interim\",\n",
    "                \"bioc_path\": \"./output/bioc_xml\",\n",
    "                \"metadata_path\": \"./output/metadata\",\n",
    "                \"embeddings_path\": \"./output/embeddings\",\n",
    "                \"chunks_path\": \"./output/chunks\",\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"test\",\n",
    "            \"test\": {\n",
    "                \"summarization_model\": {\n",
    "                    \"mistral_7b\": {\n",
    "                        \"model_path\": \"./models/mistral-7b\",\n",
    "                        \"token_limit\": 2048\n",
    "                    }\n",
    "                },\n",
    "                \"embeddings_model\": {\n",
    "                    \"pubmedbert\": {\n",
    "                        \"model_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "                        \"token_limit\": 512\n",
    "                    },\n",
    "                    \"chemberta\": {\n",
    "                        \"model_path\": \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "                        \"token_limit\": 512\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"aws\": {\n",
    "            \"platform_type\": \"HPC\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "class MockYAMLConfigLoader:\n",
    "    def get_config(self, config_name):\n",
    "        return MOCK_CONFIG.get(config_name, {})\n",
    "\n",
    "import src.pubtator_utils.config_handler.config_reader as config_reader\n",
    "config_reader.YAMLConfigLoader = MockYAMLConfigLoader\n",
    "\n",
    "# Mock db.py to prevent database connection\n",
    "from types import ModuleType\n",
    "mock_db = ModuleType(\"src.pubtator_utils.db_handler.db\")\n",
    "mock_db.get_db_url = lambda *args, **kwargs: \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.db_url = \"postgresql://mock:mock@localhost/mock\"\n",
    "mock_db.engine = None\n",
    "mock_db.Session = MagicMock()\n",
    "mock_db.session = MagicMock()\n",
    "sys.modules[\"src.pubtator_utils.db_handler.db\"] = mock_db\n",
    "\n",
    "# Mock FileHandlerFactory to always return LocalFileHandler\n",
    "from src.pubtator_utils.file_handler.local_handler import LocalFileHandler\n",
    "\n",
    "class MockFileHandlerFactory:\n",
    "    _handlers = {\"local\": LocalFileHandler, \"test\": LocalFileHandler, \"s3\": LocalFileHandler}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_handler(storage_type=None, platform_type=None):\n",
    "        return LocalFileHandler()\n",
    "\n",
    "import src.pubtator_utils.file_handler.file_handler_factory as file_handler_factory\n",
    "file_handler_factory.FileHandlerFactory = MockFileHandlerFactory\n",
    "\n",
    "print(\"âœ“ Mocked YAMLConfigLoader (no config file reads)\")\n",
    "print(\"âœ“ Mocked db.py (no PostgreSQL connection)\")\n",
    "print(\"âœ“ Mocked FileHandlerFactory (always returns LocalFileHandler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e54dfd7-fb7b-4a73-afd1-ce08d77b9af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORT PROJECT MODULES - PPTX Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "from src.pubtator_utils.logs_handler.logger import SingletonLogger\n",
    "\n",
    "# Apollo PPTX processing functions\n",
    "from src.data_ingestion.ingest_apollo.ingest_pptx.apollo_pptx_to_bioc_converter import (\n",
    "    PptxProcessor,\n",
    "    pptx_to_bioc_converter,\n",
    "    merge_small_passages_in_collection,\n",
    ")\n",
    "from src.data_ingestion.ingest_apollo.ingest_pptx.pptx_table_processor import (\n",
    "    PptxTableExtractor,\n",
    "    extract_pptx_tables,\n",
    ")\n",
    "\n",
    "# Chunking and merging\n",
    "from src.data_processing.chunking.chunks_handler import chunk_annotated_articles\n",
    "from src.data_processing.merging.merge_handler import merge_annotations\n",
    "\n",
    "from pptx import Presentation\n",
    "import bioc\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "logger = SingletonLogger().get_logger()\n",
    "file_handler = LocalFileHandler()\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"  PPTX pipeline modules loaded:\")\n",
    "print(\"  - PptxProcessor (PPTX â†’ BioC)\")\n",
    "print(\"  - merge_small_passages_in_collection (passage merging)\")\n",
    "print(\"  - PptxTableExtractor (table extraction)\")\n",
    "print(\"  - chunk_annotated_articles (chunking)\")\n",
    "print(\"  - merge_annotations (annotation merging)\")\n",
    "print(\"  - Ready for embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805dbcff-04ec-41e7-b1c6-26931d1c55c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure Paths\n",
    "\n",
    "Define input PPTX file and output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a224b10c-40f5-40f5-83bc-dce82307f157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE INPUT/OUTPUT PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Input PPTX file path - CHANGE THIS to your PPTX file\n",
    "PPTX_INPUT_PATH = \"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/sample3.pptx\"\n",
    "\n",
    "# Output directory structure\n",
    "OUTPUT_BASE_DIR = Path(\"/Workspace/Users/jesse.americogomesdelima@gilead.com/pubtator/GileadPubtator/sample_data/output\")\n",
    "INGESTION_PATH = OUTPUT_BASE_DIR / \"ingestion\"\n",
    "INTERIM_PATH = OUTPUT_BASE_DIR / \"interim\"\n",
    "FAILED_PATH = OUTPUT_BASE_DIR / \"failed\"\n",
    "BIOC_PATH = OUTPUT_BASE_DIR / \"bioc_xml\"\n",
    "CHUNKS_PATH = OUTPUT_BASE_DIR / \"chunks\"\n",
    "EMBEDDINGS_PATH = OUTPUT_BASE_DIR / \"embeddings\"\n",
    "METADATA_PATH = OUTPUT_BASE_DIR / \"metadata\"\n",
    "\n",
    "# Get PPTX name without extension\n",
    "pptx_name = Path(PPTX_INPUT_PATH).stem\n",
    "\n",
    "# Create all directories\n",
    "ALL_PATHS = [INGESTION_PATH, INTERIM_PATH, FAILED_PATH, BIOC_PATH, CHUNKS_PATH, EMBEDDINGS_PATH, METADATA_PATH]\n",
    "for dir_path in ALL_PATHS:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create document-specific interim directory\n",
    "PPTX_INTERIM_DIR = INTERIM_PATH / pptx_name\n",
    "PPTX_INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Output directories created in: {OUTPUT_BASE_DIR.resolve()}\")\n",
    "print(f\"âœ“ PPTX to process: {pptx_name}\")\n",
    "print(f\"âœ“ Interim directory: {PPTX_INTERIM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b133cbc-8ef4-4fe4-8ceb-ccd24476d239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read PPTX File\n",
    "\n",
    "Load the PPTX file using python-pptx and copy to ingestion directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48fcd6eb-820f-4954-be8d-99cf10b00244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify PPTX file exists and copy to ingestion directory\n",
    "pptx_source_path = Path(PPTX_INPUT_PATH).resolve()\n",
    "\n",
    "if not pptx_source_path.exists():\n",
    "    raise FileNotFoundError(f\"PPTX not found: {pptx_source_path}\")\n",
    "\n",
    "pptx_content = file_handler.read_file_bytes(str(pptx_source_path))\n",
    "pptx_dest_path = INGESTION_PATH / f\"{pptx_name}.pptx\"\n",
    "file_handler.write_file(str(pptx_dest_path), pptx_content)\n",
    "\n",
    "print(f\"âœ“ PPTX: {pptx_source_path}\")\n",
    "print(f\"âœ“ Size: {len(pptx_content):,} bytes\")\n",
    "print(f\"âœ“ Copied to: {pptx_dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244fdc23-0cc4-4559-bba0-219b6ac5a681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read PPTX using python-pptx\n",
    "print(f\"Reading PPTX file: {pptx_dest_path}\")\n",
    "\n",
    "try:\n",
    "    prs = Presentation(str(pptx_dest_path))\n",
    "    print(f\"âœ“ Successfully opened PPTX\")\n",
    "    print(f\"   Slides: {len(prs.slides)}\")\n",
    "    \n",
    "    # Count shapes and tables\n",
    "    total_shapes = 0\n",
    "    total_tables = 0\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            total_shapes += 1\n",
    "            if getattr(shape, \"has_table\", False) and shape.has_table:\n",
    "                total_tables += 1\n",
    "    \n",
    "    print(f\"   Total shapes: {total_shapes}\")\n",
    "    print(f\"   Total tables: {total_tables}\")\n",
    "    \n",
    "    # Preview first few slides\n",
    "    print(f\"\\nðŸ“Š Slide Preview:\")\n",
    "    for i, slide in enumerate(list(prs.slides)[:3], start=1):\n",
    "        # Collect text from slide\n",
    "        texts = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text:\n",
    "                texts.append(shape.text.strip()[:50])\n",
    "        preview = \" | \".join(texts[:3])[:100]\n",
    "        print(f\"   Slide {i}: {preview}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to read PPTX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c881b3b-3285-4c11-93c2-46e916ff6cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Convert PPTX to BioC XML\n",
    "\n",
    "Using Apollo's **PptxProcessor** to convert PPTX to BioC XML format.\n",
    "\n",
    "The processor:\n",
    "- Extracts text from each slide (excluding tables)\n",
    "- Creates one passage per slide\n",
    "- Merges small passages using `merge_small_passages_in_collection()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bddc04f-dded-49c5-812e-2f4001a1b541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create article metadata\n",
    "article_metadata = {\n",
    "    \"source\": \"local_pptx\",\n",
    "    \"filename\": pptx_name,\n",
    "    \"title\": pptx_name.replace(\"_\", \" \").title(),\n",
    "    \"full_path\": str(pptx_source_path),\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = METADATA_PATH / f\"{pptx_name}_metadata.json\"\n",
    "file_handler.write_file_as_json(str(metadata_file), article_metadata)\n",
    "print(f\"âœ“ Saved metadata to: {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e621e61c-3a58-4c72-9078-d2e414f9331e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert PPTX to BioC using Apollo's PptxProcessor\n",
    "print(f\"Converting PPTX to BioC XML using PptxProcessor...\")\n",
    "\n",
    "# Use the module-level function which wraps PptxProcessor\n",
    "bioc_xml_path = BIOC_PATH / f\"{pptx_name}.xml\"\n",
    "\n",
    "pptx_to_bioc_converter(\n",
    "    file_handler=file_handler,\n",
    "    internal_doc_name=f\"{pptx_name}.pptx\",\n",
    "    internal_docs_path=str(INGESTION_PATH),\n",
    "    bioc_path=str(BIOC_PATH),\n",
    "    metadata_fields=article_metadata,\n",
    "    write_to_s3=False,\n",
    "    s3_bioc_path=None,\n",
    "    s3_file_handler=None,\n",
    ")\n",
    "\n",
    "if bioc_xml_path.exists():\n",
    "    bioc_size = bioc_xml_path.stat().st_size\n",
    "    print(f\"âœ“ BioC XML created: {bioc_xml_path}\")\n",
    "    print(f\"âœ“ Size: {bioc_size:,} bytes\")\n",
    "else:\n",
    "    raise RuntimeError(f\"BioC conversion failed - file not created: {bioc_xml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4498ea08-b00f-429a-a1ee-5cf1fed3b089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and preview BioC XML\n",
    "with open(bioc_xml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bioc_collection = bioc.load(f)\n",
    "\n",
    "print(f\"BioC Collection loaded:\")\n",
    "print(f\"   Source: {bioc_collection.source}\")\n",
    "print(f\"   Date: {bioc_collection.date}\")\n",
    "print(f\"   Documents: {len(bioc_collection.documents)}\")\n",
    "\n",
    "for doc in bioc_collection.documents:\n",
    "    print(f\"\\nðŸ“„ Document: {doc.id}\")\n",
    "    print(f\"   Passages: {len(doc.passages)}\")\n",
    "    \n",
    "    # Preview passages\n",
    "    print(f\"\\n   Passage preview:\")\n",
    "    for i, passage in enumerate(doc.passages[:3]):\n",
    "        section = passage.infons.get('section_title', 'N/A')\n",
    "        text_preview = passage.text[:150] + \"...\" if len(passage.text) > 150 else passage.text\n",
    "        print(f\"   [{i+1}] Section: {section}\")\n",
    "        print(f\"       Text: {text_preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa86afcc-221d-408b-ac41-94a194bf8190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Extract Tables from PPTX\n",
    "\n",
    "Using Apollo's **PptxTableExtractor** to extract tables from slides.\n",
    "\n",
    "Tables are processed separately and saved as a JSON file in the embeddings directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ac118b-4496-4f65-bf4c-813f06f094ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract tables using Apollo's PptxTableExtractor\n",
    "print(f\"Extracting tables from PPTX using PptxTableExtractor...\")\n",
    "\n",
    "# Use the module-level function\n",
    "extract_pptx_tables(\n",
    "    file_handler=file_handler,\n",
    "    pptx_path=str(pptx_dest_path),\n",
    "    interim_dir=str(INTERIM_PATH),\n",
    "    embeddings_dir=str(EMBEDDINGS_PATH),\n",
    "    bioc_metadata_fields=article_metadata,\n",
    "    write_to_s3=False,\n",
    "    s3_embeddings_path=None,\n",
    "    s3_interim_path=None,\n",
    "    s3_file_handler=None,\n",
    ")\n",
    "\n",
    "# Check if tables JSON was created\n",
    "tables_json_path = EMBEDDINGS_PATH / f\"{pptx_name}_tables.json\"\n",
    "if tables_json_path.exists():\n",
    "    tables_data = file_handler.read_json_file(str(tables_json_path))\n",
    "    print(f\"âœ“ Extracted {len(tables_data)} table(s)\")\n",
    "    print(f\"âœ“ Tables saved to: {tables_json_path}\")\n",
    "    \n",
    "    # Preview tables\n",
    "    for i, table in enumerate(tables_data[:3]):\n",
    "        payload = table.get('payload', table)\n",
    "        print(f\"\\nðŸ“Š Table {i+1}:\")\n",
    "        print(f\"   Slide: {payload.get('slide_index', 'N/A')}\")\n",
    "        print(f\"   Name: {payload.get('table_name', 'N/A')}\")\n",
    "        print(f\"   Rows: {payload.get('row_count', 'N/A')}\")\n",
    "        print(f\"   Columns: {payload.get('column_count', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"âœ“ No tables found in PPTX (or file not created)\")\n",
    "    tables_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a3415c-954c-48df-aa54-c12a87f705f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Chunk the Document\n",
    "\n",
    "Using `chunk_annotated_articles()` from `chunks_handler.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8e367f-8271-4e22-9235-efe3dcdfbeae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chunk the BioC XML using project's chunk_annotated_articles()\n",
    "# Chunker types: 'sliding_window', 'passage', 'annotation_aware', 'grouped_annotation_aware_sliding_window'\n",
    "\n",
    "CHUNKER_TYPE = \"sliding_window\"  # Same default as production pipeline\n",
    "WINDOW_SIZE = 512  # Window size in words\n",
    "\n",
    "print(f\"Chunking {bioc_xml_path} using chunk_annotated_articles()...\")\n",
    "print(f\"  Chunker type: {CHUNKER_TYPE}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE}\")\n",
    "\n",
    "chunks = chunk_annotated_articles(\n",
    "    file_handler=file_handler,\n",
    "    input_file_path=str(bioc_xml_path),\n",
    "    chunker_type=CHUNKER_TYPE,\n",
    "    window_size=WINDOW_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created {len(chunks)} chunks\")\n",
    "\n",
    "# Save chunks\n",
    "chunks_output_path = CHUNKS_PATH / f\"{pptx_name}_chunks.json\"\n",
    "with open(chunks_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ“ Saved chunks to: {chunks_output_path}\")\n",
    "\n",
    "# Show statistics\n",
    "if chunks:\n",
    "    word_counts = [len(c['text'].split()) for c in chunks]\n",
    "    print(f\"\\nðŸ“Š Chunk Statistics:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print(f\"   Avg words/chunk: {sum(word_counts) // len(word_counts)}\")\n",
    "    print(f\"   Min/Max words: {min(word_counts)} / {max(word_counts)}\")\n",
    "    \n",
    "    # Preview first chunk\n",
    "    print(f\"\\nðŸ“„ First chunk preview:\")\n",
    "    print(f\"   Text: {chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f375507e-171a-4c3d-918d-1e75811d233b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Generate Embeddings\n",
    "\n",
    "Using `get_embeddings()` from `embeddings_generator.py` - the **same function used by the production pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2ba8c3-4907-45a6-bea3-6fc86803430f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION - Download model from S3 and load locally\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from src.pubtator_utils.embeddings_handler.embeddings_generator import (\n",
    "    get_embeddings,\n",
    "    calculate_similarity,\n",
    ")\n",
    "\n",
    "local_dir = \"src/models/pubmedbert-base-embeddings\"\n",
    "\n",
    "def download_model_from_s3_uri(\n",
    "    s3_uri: str = \"s3://gilead-edp-kite-rd-dev-us-west-2-kite-benchling-text-sql/models/pubmedbert-base-embeddings/\",\n",
    "    local_dir: str = \"src/models/pubmedbert-base-embeddings\",\n",
    ") -> None:\n",
    "    parsed = urlparse(s3_uri)\n",
    "    if parsed.scheme != \"s3\":\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Ensure base folder exists\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading model from {s3_uri} â†’ {local_dir}\")\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    found_objects = False\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            found_objects = True\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            # Skip directory markers\n",
    "            if key.endswith(\"/\") or key == prefix:\n",
    "                continue\n",
    "\n",
    "            relpath = os.path.relpath(key, prefix)\n",
    "            local_path = os.path.join(local_dir, relpath)\n",
    "\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            s3.download_file(bucket, key, local_path)\n",
    "\n",
    "            print(f\"Downloaded {key}\")\n",
    "\n",
    "    if not found_objects:\n",
    "        print(f\"No objects found at {s3_uri}\")\n",
    "\n",
    "    print(\"Model download complete.\")\n",
    "\n",
    "download_model_from_s3_uri()\n",
    "\n",
    "def load_embeddings_model():\n",
    "    # Must match the download location exactly\n",
    "    model_dir = os.path.abspath(\"src/models/pubmedbert-base-embeddings\")\n",
    "\n",
    "    print(f\"Loading embeddings model from {model_dir}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"Embeddings model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_embeddings_model()\n",
    "MODEL_NAME = \"pubmedbert\"  # Used by get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a167549-8710-4d0a-a207-39fdc46eb140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks using project's get_embeddings()\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks using get_embeddings()...\")\n",
    "\n",
    "# Get texts from chunks (merge_annotations if annotations exist)\n",
    "chunk_texts = []\n",
    "for chunk in chunks:\n",
    "    text = chunk['text']\n",
    "    annotations = chunk.get('annotations', [])\n",
    "    if annotations:\n",
    "        # Use project's merge_annotations to combine text with annotations\n",
    "        merged_text = merge_annotations(text=text, annotations=annotations, merger_type=\"prepend\")\n",
    "        chunk_texts.append(merged_text)\n",
    "    else:\n",
    "        chunk_texts.append(text)\n",
    "\n",
    "# Generate embeddings using project's function\n",
    "embeddings = get_embeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    texts=chunk_texts,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Generated embeddings for {len(chunk_texts)} chunks\")\n",
    "print(f\"âœ“ Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95720b0-4427-4c66-be8a-e492cae136de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create chunks with embeddings in the same format as production pipeline\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    chunk_sequence = i + 1\n",
    "    \n",
    "    # Get slide_ids from chunk if available (PPTX specific)\n",
    "    slide_ids = chunk.get('slide_ids', None)\n",
    "    section_title = chunk.get('section_title', chunk.get('infons', {}).get('section_title', []))\n",
    "    \n",
    "    chunks_with_embeddings.append({\n",
    "        'chunk_sequence': str(chunk_sequence),\n",
    "        'merged_text': chunk_texts[i],  # Text used for embedding (may include annotations)\n",
    "        'payload': {\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_processing_date': datetime.now().date().isoformat(),\n",
    "            'chunk_name': f\"{pptx_name}_chunk_{chunk_sequence}\",\n",
    "            'chunk_text': chunk['text'],\n",
    "            'chunk_length': len(chunk['text']),\n",
    "            'token_count': len(chunk['text'].split()),\n",
    "            'chunk_annotations_count': len(chunk.get('annotations', [])),\n",
    "            'article_id': pptx_name,\n",
    "            'source': 'local_pptx',\n",
    "            'chunk_type': 'article_chunk',\n",
    "            'processing_ts': datetime.now().isoformat(),\n",
    "            'section_title': section_title,\n",
    "            'slide_ids': slide_ids,\n",
    "            'offset': chunk.get('offset', 0),\n",
    "        },\n",
    "        'embeddings': embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else list(embeddings[i]),\n",
    "    })\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_output_path = EMBEDDINGS_PATH / f\"{pptx_name}_embeddings.json\"\n",
    "with open(embeddings_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_with_embeddings, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "file_size = embeddings_output_path.stat().st_size\n",
    "print(f\"âœ“ Saved embeddings to: {embeddings_output_path}\")\n",
    "print(f\"âœ“ File size: {file_size:,} bytes ({file_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"âœ“ Format matches production pipeline output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd42cdd7-b5d9-40c1-a367-83729643f82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Test Semantic Search (RAG Demo)\n",
    "\n",
    "Demonstrate semantic search using the generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbfec92-6de8-4631-aa69-2208fdd989da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def semantic_search(query: str, chunks_with_embeddings: list, model_name: str, model, tokenizer, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using project's get_embeddings() and calculate_similarity().\n",
    "    \"\"\"\n",
    "    # Get query embedding using project's function\n",
    "    query_embedding = get_embeddings(\n",
    "        model_name=model_name,\n",
    "        texts=[query],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Get chunk embeddings\n",
    "    chunk_embeddings = [c['embeddings'] for c in chunks_with_embeddings]\n",
    "    \n",
    "    # Calculate similarities using project's function\n",
    "    similarities = calculate_similarity(query_embedding[0].numpy(), chunk_embeddings)\n",
    "    \n",
    "    # Get top k results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'chunk': chunks_with_embeddings[idx],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'rank': len(results) + 1,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"âœ“ Semantic search function defined (using project's calculate_similarity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab9765a-9655-4628-845e-65488cebf36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Demo: Semantic search\n",
    "# Modify this query based on your PPTX content\n",
    "QUERY = \"clinical trial results\"  # Change this to match your presentation content\n",
    "\n",
    "print(f\"ðŸ” Query: '{QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = semantic_search(\n",
    "    query=QUERY, \n",
    "    chunks_with_embeddings=chunks_with_embeddings, \n",
    "    model_name=MODEL_NAME,\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    chunk = result['chunk']\n",
    "    similarity = result['similarity']\n",
    "    rank = result['rank']\n",
    "    chunk_text = chunk['payload']['chunk_text']\n",
    "    preview = chunk_text[:300] + \"...\" if len(chunk_text) > 300 else chunk_text\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Result #{rank} (similarity: {similarity:.4f})\")\n",
    "    print(f\"   Chunk: {chunk['payload']['chunk_name']}\")\n",
    "    print(f\"   Words: {chunk['payload']['token_count']}\")\n",
    "    slide_ids = chunk['payload'].get('slide_ids')\n",
    "    if slide_ids:\n",
    "        print(f\"   Slides: {slide_ids}\")\n",
    "    print(f\"   Text: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db08fd0-c287-40e8-a55c-96cb528590c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary: Generated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a68608-73ab-4b20-b81a-a7183b474be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"Generated files:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_BASE_DIR):\n",
    "    level = root.replace(str(OUTPUT_BASE_DIR), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder = os.path.basename(root)\n",
    "    if files:\n",
    "        print(f\"{indent}{folder}/\")\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            size = file_path.stat().st_size\n",
    "            size_str = f\"{size:,} bytes\" if size < 1024*1024 else f\"{size/1024/1024:.2f} MB\"\n",
    "            print(f\"{indent}  {file} ({size_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58447556-ffea-41e5-a5de-dcaf07a49f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "This notebook executed the **complete PPTX-to-RAG pipeline** using **Apollo project functions**:\n",
    "\n",
    "| Step | Function | Source File |\n",
    "|------|----------|-------------|\n",
    "| 1. Read | `Presentation()` | python-pptx |\n",
    "| 2. Convert | `pptx_to_bioc_converter()` | `apollo_pptx_to_bioc_converter.py` |\n",
    "| 2a. Merge | `merge_small_passages_in_collection()` | `apollo_pptx_to_bioc_converter.py` |\n",
    "| 3. Tables | `extract_pptx_tables()` | `pptx_table_processor.py` |\n",
    "| 4. Chunk | `chunk_annotated_articles()` | `chunks_handler.py` |\n",
    "| 5. Embed | `get_embeddings()` | `embeddings_generator.py` |\n",
    "| 6. Search | `calculate_similarity()` | `embeddings_generator.py` |\n",
    "\n",
    "### Output Structure\n",
    "```\n",
    "./output/\n",
    "â”œâ”€â”€ ingestion/         # Original PPTX\n",
    "â”œâ”€â”€ interim/           # Extracted table Excel files\n",
    "â”œâ”€â”€ bioc_xml/          # BioC XML representation\n",
    "â”œâ”€â”€ chunks/            # Raw chunks JSON\n",
    "â”œâ”€â”€ embeddings/        # Chunks + 768-dim vectors + table embeddings\n",
    "â””â”€â”€ metadata/          # Document metadata\n",
    "```\n",
    "\n",
    "### PPTX-Specific Features:\n",
    "- **Slide-based passages**: Each slide becomes a BioC passage\n",
    "- **Table extraction**: Tables extracted separately with slide context\n",
    "- **Passage merging**: Small passages merged using `merge_small_passages_in_collection()`\n",
    "- **Slide IDs**: Chunk metadata includes `slide_ids` for traceability\n",
    "\n",
    "### Output Format (matches production pipeline):\n",
    "The `embeddings/{pptx_name}_embeddings.json` file uses the **same format as production**:\n",
    "- `chunk_sequence` - Chunk order\n",
    "- `merged_text` - Text used for embedding\n",
    "- `payload.chunk_id` - Unique identifier\n",
    "- `payload.chunk_text` - The slide text content\n",
    "- `payload.slide_ids` - Source slide numbers\n",
    "- `embeddings` - 768-dimensional PubMedBERT vector\n",
    "\n",
    "Load this into a vector database (Pinecone, Weaviate, OpenSearch, etc.) for semantic search!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pptx_full_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
